


  




  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.48 with theme Tranquilpeak 0.4.3-BETA">
    <title>Posts</title>
    <meta name="author" content="Fukuball">
    <meta name="keywords" content="">

    <link rel="icon" href="images/favicon.ico">
    
      <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.fukuball.com/post/index.xml">
    

    
    <meta name="description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Posts">
    <meta property="og:url" content="/post/">
    <meta property="og:site_name" content="I am Fukuball">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="I am Fukuball">
    <meta name="twitter:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    
    

    
    

    
      <meta property="og:image" content="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=640">
    

    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://blog.fukuball.com/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="1">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://blog.fukuball.com/">I am Fukuball</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://blog.fukuball.com/#about">
    
    
    
      
        <img class="header-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=90" alt="" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="1">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://blog.fukuball.com/#about">
          <img class="sidebar-profile-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
        </a>
        <h4 class="sidebar-profile-name">Fukuball</h4>
        
          <h5 class="sidebar-profile-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://facebook.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-facebook"></i>
      
      <span class="sidebar-button-desc">Facebook</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">Blog</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="1"
        class="
               hasCoverMetaIn
               ">
        <section class="postShorten-group main-content-wrap">
          
          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 8 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-11-08T10:26:42&#43;08:00">
        
  November 8, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/">第 7 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講我們介紹了如何使用 Blending 及 Bagging 的技巧來做到 Aggregation Model，可以使用 Uniform 及 Linear 的方式融合不同的 Model。至於以 Non-linear 的方式融合 Model 就需要依據想展現的特性去調整演算法來做到，這一講將介紹 Adaptive Boosting 這種特別的演算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-1.png">
</p>

<h3 id="幼稚園學生學認識蘋果的故事-一">幼稚園學生學認識蘋果的故事（一）</h3>

<p>我們用一個幼稚園學生在課堂上學認識蘋果的故事來作為開頭說明，在課堂上老師問 Michael 說「上面的圖片哪些是蘋果呢？」，Michael 回答「蘋果是圓的」，的確蘋果很多是圓的，但是有些水果是圓的但不是蘋果，有些蘋果也不一定是圓的，因此 Michael 的回答在藍色的這些圖片犯了錯誤。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-2.png">
</p>

<h3 id="幼稚園學生學認識蘋果的故事-二">幼稚園學生學認識蘋果的故事（二）</h3>

<p>於是老師為了讓學生可以更精確地回答，將 Michael 犯錯的圖片放大了，答對的圖片則縮小了，讓學生的可針對這些錯誤再修正答案。於是 Tina 回答「蘋果是紅的」，這的確是一個很好的觀察，但一樣在底下藍色標示的這是個圖片犯了錯，番茄跟草莓也是紅的、青蘋果的話就是綠的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-3.png">
</p>

<h3 id="幼稚園學生學認識蘋果的故事-三">幼稚園學生學認識蘋果的故事（三）</h3>

<p>於是老師又將 Tina 犯錯的圖片放大了，答對的圖片縮小，讓學生繼續精確的回覆蘋果的特徵。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-4.png">
</p>

<h3 id="動機">動機</h3>

<p>這樣的教學過程也是一種可以用來教機器如何學習的過程，每個學生都只會一些簡單的假設 gt（蘋果是紅的），綜合所有學生的假設就可以好好地認識出蘋果的特徵形成 G，而老師則像是一個演算法一樣指導學生方向，讓錯誤越來越少。</p>

<p>接下來我們就要介紹如何用演算法來模擬這樣的學習過程。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-5.png">
</p>

<h3 id="有權重的-ein">有權重的 Ein</h3>

<p>老師調整圖片放大縮小的教學方式，在數學上我們可以為每個點犯錯時加上一個權重來表示。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-6.png">
</p>

<h3 id="每一回合調整權重">每一回合調整權重</h3>

<p>那我們如何調整權重呢？我們每次調整權重，是希望每個學生能學出不一樣的觀點，這樣才能配合所有學生的觀點做出對蘋果完整的認識，因此挑整權重時應該要讓第一個學習到的 gt 在 u(t+1) 時這樣的權重下表現很差。</p>

<p>所謂的表現差就是表現跟丟銅板沒兩樣，也就是猜錯的情況為 1/2。（全猜錯其實算是一個好的表現）</p>

<p>我們讓演算法根據新的權重 u(t+1)（會使 gt 表現很差）再去學習出新的 g(t+1)，這個 g(t+1) 就是一個觀點與 gt 不同，但表現卻也不錯的新假設了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-8.png">
</p>

<h3 id="調整權重的數學式">調整權重的數學式</h3>

<p>調整權重的數學式如下，我們想讓 gt 在 u(t+1) 的情況下猜對跟猜錯的情況為 1/2，那其實就是將原本的 ut 在猜對時乘上錯誤率(incorrect rate)，在猜錯時乘上答對率（correct rate），這樣 gt 在 u(t+1) 的 Ein 就會是 <sup>1</sup>&frasl;<sub>2</sub> 了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-9.png">
</p>

<h3 id="縮放因數">縮放因數</h3>

<p>演算法的作者在上述的概念上選擇了一個特別的縮放因數，但基本上就是剛剛上述所說的概念，這個縮放因數我們用 方塊t 來表示（如下圖），這在物理意義上有特別的意義，當 錯誤率 &lt;= <sup>1</sup>&frasl;<sub>2</sub> 時，縮放因數 方塊t 才會 &gt;= 1，然後在計算下一輪的 ut 時，答錯的點會乘上 方塊t，答對的點會除上 方塊t，這就像老師在小朋友答錯的圖片放大、答對的圖片縮小那樣的教學過程。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-10.png">
</p>

<h3 id="初步的演算法">初步的演算法</h3>

<p>初步的演算法如下，首先第一輪的 u 對每個點都是相同的權重，這時我們學出第一個假設 gt，接下來使用 方塊t 調整權重得出 u(t+1)，然後繼續學出第二個假設 g(t+1)，我們可以繼續這樣的過程學出更多假設，現在只剩下一個問題，如何融合所有的假設呢？</p>

<p>我們可以用之前學過的 blending 使用 uniform 或是 linear 來融合，但這邊作者選擇了一個特別的演算法在計算過程中融合所有的假設。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-11.png">
</p>

<h3 id="演算過程中就算出融合假設的權重">演算過程中就算出融合假設的權重</h3>

<p>這邊作者想了一個方法可以在演算過程中就算出融合假設的權重，這邊權重 alpha t = ln(方塊t)，這在物理意義上也有特別的意義。當錯誤率是 <sup>1</sup>&frasl;<sub>2</sub> 跟丟銅板沒兩樣時，那 方塊t 會等於 1，這時 ln(方塊t) 就會是 0，代表這個假設的權重是 0，一點用都沒有。當錯誤率是 0 的時候，那 方塊t 會等於無限大，這時 ln(方塊t) 就會是無限大，我們可以只看這個超強的假設 gt 就好。</p>

<p>綜合以上，這個演算法就是 Adptive Boosting。我們有 weak learning 就上課堂上的幼稚園學生；我們有一個演算法調整權重就像老師會調整圖片大小引領學生學習；我們有一個融合假設的方法就像我們將課堂上所有學生的觀點融合起來一樣。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-12.png">
</p>

<h3 id="adaptive-boosting-adaboost-演算法">Adaptive Boosting(AdaBoost) 演算法</h3>

<p>詳細 Adaptive Boosting 演算法如下所示，中文又稱這個演算法就皮匠法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-13.png">
</p>

<h3 id="理論上的保證">理論上的保證</h3>

<p>跟去理論上的保證只要 weak learner 比亂猜還好，那 AdaBoost 可以很容易地將 Ein 降到 0，而 dvc 的成長也相對慢，因此當資料量夠多時，Eout 理論保證會很小。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-14.png">
</p>

<h3 id="weak-learner-decision-stump">Weak Learner：Decision Stump</h3>

<p>AdaBoost 使用的 Weak Learner 是 Decision Stump，是一個只能在平面上切一條水平線或垂直線的 Weak Learner，詳細演算法如下，每個假設就是要學習出是一條水平線或垂直線（direction s），切在哪個 feature（feature i），切在那個值（threshold theta）。如果想要看程式碼可以參考：<a href="https://github.com/fukuball/fuku-ml/blob/master/FukuML/DecisionStump.py">https://github.com/fukuball/fuku-ml/blob/master/FukuML/DecisionStump.py</a></p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-15.png">
</p>

<h3 id="我們用一個簡單的例子說明-adaboost">我們用一個簡單的例子說明 AdaBoost</h3>

<p>我們用一個簡單的例子來說明 AdaBoost 的演算過程。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-16.png">
</p>

<h3 id="第一輪">第一輪</h3>

<p>第一輪先學出一個 weak learner 切了一個垂直線，這時犯錯的點會放大、答對的點縮小。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-17.png">
</p>

<h3 id="第二輪">第二輪</h3>

<p>根據犯錯放大的權重，再去學出一個不同觀點的 weak learner 再切了一條垂直線，根據答案再對點的權重調整。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-18.png">
</p>

<h3 id="第三輪">第三輪</h3>

<p>第三輪又學出了一個不同觀點的 weak learner 切了一條水平線，現在已經可以看出分界線慢慢變得複雜了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-19.png">
</p>

<h3 id="第五輪">第五輪</h3>

<p>持續這樣的過程，AdaBoost 不斷地得出不同的 weak learner，綜合 weak learner 的答案便可以回答一些較複雜的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-21.png">
</p>

<h3 id="總結">總結</h3>

<p>這一講介紹了 AdaBoost 這個特殊的機器學習演算法，能夠將表現只比丟銅板好一些的 Weak Learner 融合起來去得到更好的學習效果，算是從 Blending 技巧中衍伸出來的一個特殊演算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-22.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 7 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-09-14T09:05:58&#43;08:00">
        
  September 14, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji/">第 6 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>前 6 講我們對 SVM 做了完整的介紹，從基本的 SVM 分類器到使用 Support Vector 性質發展出來的 regression 演算法 SVR，在機器學習基石中學過的各種問題，SVM 都有對應的演算法可以解。</p>

<p>第 7 講我們要介紹 Aggregation Models，顧名思義就是要講多種模型結合起來，看能不能在機器學習上有更好的效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-1.png">
</p>

<h3 id="aggregation-的故事">Aggregation 的故事</h3>

<p>我們用一個簡單的故事來說明 Aggregation，假設現在你有很多個朋友可以預測股票會漲還是會跌，那你要選擇相信誰的說法呢？這就像我們有很多個機器學習預測模型，我們要選擇哪一個來做預測。</p>

<p>一個方式是選擇裡面最準的那一個人的說法，在機器學習就是使用 Validation 來做選擇。</p>

<p>一個方式是綜合所有人的意見，每個人代表一票，然後選擇票數最多的預測。在機器學習也可以用這樣的方法綜合所有模型的預測。</p>

<p>另一個方式也是綜合所有人的意見，只是每個人的票數不一樣，比較準的人票數較多，比較沒那麼準的人票數較少。在機器學習上，我們也可以為每個模型放上不同的權重來做到這樣的效果。</p>

<p>最後一個方式就是會依據條件來選擇相信誰的說法，因為每個人擅長的預測可能不多，有的人擅長科技類股，有的人擅長傳統類股，所以我們需要依據條件來做調整。在機器學習上也會有類似的演算法來整合各個預測模型。</p>

<p>Aggregation 大致就是依照上述方式來整合各個模型。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-2.png">
</p>

<h3 id="用-validation-選擇預測模型">用 Validation 選擇預測模型</h3>

<p>我們已經學過如何使用 Validation 來選擇預測模型，這個方式有一個問題就是，需要其中有一個強的預設模型才會有用，如果所有的預設模型都不準確，那也只是從廢渣裡面選一個比較不廢的而已。</p>

<p>所以 Validation 在機器學習上還是有一些限制的，那我們有辦法透過 Aggregation 來讓所有的廢渣整合起來，然後變強，讓預測變得更準確嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-3.png">
</p>

<h3 id="為何-aggregation-會有用">為何 Aggregation 會有用？</h3>

<p>首先我們看左圖，如果現在預設模型只能切垂直線或水平線，其實預測效果可想而知是不會好到哪裡去的。但是如果我們將多個垂直線或水平線的預測模型整合起來，就有辦法做好一些更複雜的分類。這某種程度像是做了特徵轉換到高維度，讓預測模型變得更強、更準確。</p>

<p>再來我們看右圖，我們知道 PLA 因為隨機的性質，每次算出來的分類線可能會不太一樣，如果我們將所有的分類線平均整合起來，那就會得到比較中間的線，得到的線會跟 SVM 的 large margin 分類線有點類似。這某種程度就像是做了正規化，能夠避免 Overfitting。</p>

<p>這給了我們一個啟示，以往的學習模型，只要越強大，那就越容易發生 Overfitting，但如果越正規畫，有時可能會有 Underfitting。而 Aggregatin 卻可以一邊變強又一邊有正規化的效果。</p>

<p>所以適當的使用 Aggregation 的技巧是可以讓機器學習的效果更好的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-4.png">
</p>

<h3 id="uniform-blending-用在分類上">Uniform Blending 用在分類上</h3>

<p>現在我們介紹一個最簡單的 Aggregation 方法 - Uniform Blending，在分類問題上，我們只要很單純地將所有的預測模型的預測結果加起來，然後看正負號就可以做到分類的 Uniform Blending。</p>

<p>這個方法要注意整合的預測模型需要有差異性，不能每個模型預測結果都是一致的，這樣就會沒有效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-5.png">
</p>

<h3 id="uniform-blending-用在迴歸上">Uniform Blending 用在迴歸上</h3>

<p>在迴歸問題使用 Uniform Blending 就是將所有預設模型的預測結果加起來，然後除上模型的個數，也就是做平均的意思，這樣就可以做到 Aggregation 了。</p>

<p>但同樣也要注意整合的預設模型需要有差異性才會有效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-6.png">
</p>

<h3 id="linear-blending">Linear Blending</h3>

<p>Uniform Bleding 每個預測模型的是一視同仁，可能不夠強大，我們可以使用 Linear Blending 來給每個預設模型不一樣的權重，數學式如下圖所示，其中權重是大於等於 0 的。</p>

<p>演算法的調整方法，其實就是先將資料透過每個模型做預測當成是一種特徵轉換，然後將轉換過後的資料當成是新的資料來做訓練，再使用 Linear Regressiong 算出每個預測模型的權重。未來預測時也是用每個模型轉換過後的資料再依權重做計算。</p>

<p>不過數學式上的條件還有權重都要大於等於 0，這在我們目前的演算法並沒有考慮進去。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-7.png">
</p>

<h3 id="權重大於等於-0-可以忽略">權重大於等於 0 可以忽略</h3>

<p>在 Linear Regression 算權重時，權重可能會有小於 0 的情況，在物理意義上就代表這個模型猜得很不準，所以物理意義上就像是我們把它用來當成反指標，所以它也是對預設有幫助的。因此權重大於等於 0 這個條件我們可以忽略，</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-8.png">
</p>

<h3 id="差異化的模型">差異化的模型</h3>

<p>目前我們已經大致學會了幾種 Aggregation 的方法，都需要整合的模型之間有差異化。那我們怎麼得到有差異化的模型呢？</p>

<p>一個就是本來就是演算法哲學不同的模型；一個是從參數來調出差異化；而有隨機性的模型其實每次得到的預測模型也會有差異化；另外一個方式我們可以從資料面（每次訓練餵不一樣的模型）來做出模型的差異化。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-9.png">
</p>

<h3 id="bootstrap-aggregation-資料差異化的整合">Bootstrap Aggregation 資料差異化的整合</h3>

<p>我們如何做到資料差異化的整合呢？如果我們能夠一直不斷的得到不同的 N 筆資料來做訓練，那就可以很容易地做到了。但我們手上只有原本的 N 筆訓練資料，不可能再拿到其他訓練資料，實務上我們的做法就會是從這 N 筆資料中做取後放回的抽樣抽出 N 筆資料，這樣我們每一輪都會得到不同訓練資料（由於是取後放回，N 筆資料中會有重複的資料），這樣就可以用來訓練有差異化的預測模型。</p>

<p>這種 Bootstrap Aggregation 也叫做 Bagging。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-10.png">
</p>

<h3 id="pocket-使用-bagging-aggregation-的效果">Pocket 使用 Bagging Aggregation 的效果</h3>

<p>下圖是 Pocket 使用 Bagging Aggregation 的實例效果，可以看出每個分類線是有差異的，而整合起來又有非線性分類的效果，Bagging 這個方法在有隨機性質的算法上理論上都是有用的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-11.png">
</p>

<h3 id="總結">總結</h3>

<p>這一講就是介紹如何整合很多個預測模型的預測結果，理論上是可以帶來更好的效果的，所以如果訓練出來的模型都是廢渣的話，也許用 Aggregation 的技巧就會讓效果變好。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-12.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji-2/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 6 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-08-06T06:53:29&#43;08:00">
        
  August 6, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/">第 5 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM  Kernel Trick 所啟發的 Kernel Logistic Rregression。這一講我們將繼續介紹如何延伸到解 Regression 的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-1.png">
</p>

<h3 id="利用-representer-theorem-延伸">利用 Representer Theorem 延伸</h3>

<p>從數學模型上，我們發現 L2-regularized 線性模型都可以轉換成 Kernel 形式，而 Linear/Ridge Regression 都有公式解，那麼 Kernel Ridge Regession 也可以推導出公式解嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-2.png">
</p>

<h3 id="kernel-ridge-regression-數學式">Kernel Ridge Regression 數學式</h3>

<p>我們使用 Representer Theorem 將 Kernel 應用至 Ridge Regression 的數學式上，得到以下 Kernel Ridge Regression 數學式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-3.png">
</p>

<h3 id="解-kernel-ridge-regression-最佳化">解 Kernel Ridge Regression 最佳化</h3>

<p>接下來我們利用偏微分來計算 Kernel Ridge Regreesion 數學式的最佳化，對 beta 進行偏微分為 0 時即可求得 beta 最佳解。我們可以很簡易的用這個式子做到非線性的 Regression。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-4.png">
</p>

<h3 id="linear-及-kernel-ridge-regression-的比較">Linear 及 Kernel Ridge Regression 的比較</h3>

<p>Linear 及 Kernel Ridge Regression 比較起來，當然 Linear 模型較簡易，因此計算效能較好，而 Kernel Ridge Regression 由於可以做非線性轉換，因此有更強的彈性。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-5.png">
</p>

<h3 id="soft-margin-svm-與-least-squares-svm-的比較">Soft-Margin SVM 與 Least-Squares SVM 的比較</h3>

<p>當我們使用 Kernel Ridge Regression 做分類時，這就是 Least-Squares SVM，Least-Squares SVM 與 Soft-Margin SVM 比較起來，他們的邊界會很接近，但會有更多的 Support Vector，如此在做預測時會慢一些。Suppport Vector 的數量跟 beta 有關，我們可以讓 beta 變得跟標準的 SVM 一樣稀疏嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-6.png">
</p>

<h3 id="tube-regression-模型">Tube Regression 模型</h3>

<p>我們重新思考一個新模型 Tube Regression，讓錯誤在一定範圍內為 0，當錯誤超過界線時，我們再以錯誤的點與邊界的距離當作錯誤值。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-7.png">
</p>

<h3 id="l2-regularized-tube-regression">L2-Regularized Tube Regression</h3>

<p>將 Tube Regression 的性質帶入 L2-Regularized，與 SVM 對照一下，目前的 L2-Regularized Tube Regression 並不能微分，雖然可以 Kernel 化，但卻不知有沒有跟 SVM 一樣有稀疏 Support Vector 的性質。</p>

<p>我們將 L2-Regularized Tube Regression 改成跟 SVM 幾乎一樣的形式來求解看看，這就是 Support Vector Regression。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-8.png">
</p>

<h3 id="support-vector-regression-primal-形式">Support Vector Regression Primal 形式</h3>

<p>首先我們來推導一下 Support Vector Regression 的 Primal 形式，由於目前的數學式有絕對值，所以我們使用上界的錯誤及下界的錯誤做展開，如下數學式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-9.png">
</p>

<h3 id="帶入-lagrange-multiplier">帶入 Lagrange Multiplier</h3>

<p>如同解 SVM 的對偶問題，我們使用了 Lagrange Multiplier 的技巧，這邊也是一樣，但由於有上界的錯誤及下界的錯誤，我們也需要有上界的 Lagrange Multiplier 及 下界的 Lagrange Multiplier。</p>

<p>如此就可以仿造解 Dual SVM 一樣，去解出最佳解時 SVR 的 KKT Conditions。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-11.png">
</p>

<h3 id="比較一下-svm-dual-及-svr-dual">比較一下 SVM Dual 及 SVR Dual</h3>

<p>SVM Dual 及 SVR Dual 數學式比較如下圖所示，因此如同之前使用 QP Solver 解 SVM Dual，我們可以將 SVR Dual 對應的變數帶入 QP Solver 來解 SVR Dual。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-12.png">
</p>

<h3 id="svr-的稀疏性質">SVR 的稀疏性質</h3>

<p>從 SVR 最佳解的條件中，當錯誤值在 tube 範圍內時，我們會訂為 0，然後因為點在 Tube 裡面，所以 y 跟分數的差值是不等於 0 的，依照 complementary slackness，如此 alpha 上界跟 alpha 下界就都是 0，alpha 上界與 alpha 下界相減是 beta，這樣就代表 beta 也是 0。而 Support Vetor 是 beta 不等於 0 的點，所以這就代表 SVR 有稀疏 Support Vetor 的性質。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-13.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講中我介紹了 Kernel Ridge Regression 及 Support Vetor Regression，有關 SVM 的相關模型已經都介紹完畢了，之後的課程將介紹如何像雞尾酒那樣結合各種學習模型。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-16.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji-2/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 5 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-06-28T07:41:37&#43;08:00">
        
  June 28, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/">第 4 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中，我們介紹了 Soft Margin SVM，讓 SVM 可以容忍一些小錯以避免 Overfitting，由於強度與容忍度兼具，Soft Margin SVM 比較通用，其實大家平常口中所說的 SVM 就是指 Soft Margin SVM。</p>

<p>前面四講我們都在討論 SVM 這個分類演算法，那我們有可能用 SVM 來做 Logistic Regression 或是 Regression 嗎？在這一講中我們將介紹如何使用 SVM 的方法來做 Logistic Regression。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-1.png">
</p>

<h3 id="觀察-svm-的容錯項">觀察 SVM 的容錯項</h3>

<p>我觀察一下 Soft Margin SVM 的容錯項，我們可以把原本的限制式整合到要最小化的式子裡來看看，如下圖所示，如此就沒有限制式了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-2.png">
</p>

<h3 id="觀察沒有限制式的-svm-數學式">觀察沒有限制式的 SVM 數學式</h3>

<p>我們再仔細觀察一下沒有限制式的 SVM 數學式，發現形式跟 L2 regularized Logistic Regression 有點像，只是沒有限制式的 SVM 數學式有個 max 的函數在裡面，這樣的數學式不再是一個 QP 問題了，然後也不是一個可以微分的式子，因此很難最佳化。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-3.png">
</p>

<h3 id="觀察-svm-錯誤衡量-function">觀察 SVM 錯誤衡量 function</h3>

<p>我們再仔細觀察一下 SVM 錯誤衡量 function，其實 err_svm 跟 err_0/1 在數線圖上 err_svm 會是 err_0/1 的上界，且邊界也很接近，所以我們可以說 SVM 與 L2-regularized logistic regression 是很接近的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-6.png">
</p>

<h3 id="第一個方法-two-level-learning">第一個方法 Two-Level Learning</h3>

<p>怎麼讓 SVM 做 Logistic Regression 呢？一個做法是使用 Two-Level Learning，也就是先做 SVM，然後將原來的 X 計算分數（轉換到 SVM 的空間）之後，再對新的 X 以及 Y 做 Logistic Regression 學習 A 與 B。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-8.png">
</p>

<h3 id="probabilistic-svm">Probabilistic SVM</h3>

<p>這就是 Probabilistic SVM，具體演算法如下，但仔細研究這個算法的背後意涵，這樣的做法並不是讓 Logistic Regression 在 z 空間做最佳解，有其它方法可以讓 Logistic 真正在 z 空間算最佳解嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-9.png">
</p>

<h3 id="kernel-trick-背後的關鍵">Kernel Trick 背後的關鍵</h3>

<p>我們了解一下 SVM 使用的 Kernel Trick，SVM 其實有在 z 空間算最佳解，只是用了 Kernel Trick 來省下計算時間，然後算出的 w 其實就是某種 z 空間的資料線性組合。SVM 是取 support vector 的線性組合、PLA 是取錯誤資料的線性組合、Logistic Regression 是取梯度下降的線性組合，所以只要 w 是一種 z 空間的線性組合的形式，那就可以使用 Kernel Trick。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-10.png">
</p>

<h3 id="kernel-logistic-regression">Kernel Logistic Regression</h3>

<p>我們將原本的 L2-Regularized Logistic Regression 數學式使用 w 是一種 z 空間線性組合的形式帶進去，得到如下圖數學式，而這數學式是可以最佳化的，所以我們可以使用之前的梯度下降法、隨機梯度下降法來求得最佳的 beta。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-11.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM  Kernel Trick 所啟發的 Kernel Logistic Rregression。下一講我們將繼續介紹如何延伸到解 Regression 的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-13.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 4 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-06-13T13:06:59&#43;08:00">
        
  June 13, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/">第 3 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中，我們介紹了使用 kernel 這樣的方法來處理高維度特徵的轉換，如此我們就能省下在高維度空間進行的運算，也因此無限多維的轉換也能輕易做到，讓 SVM 可能有更強的效果。</p>

<p>截至目前為止所學的 SVM 模型都是 Hard Margin SVM，這樣的 SVM 就是會將資料完美的分好，也因此在越強的學習模型中越可能會有 Overfiting 的情況發生（雖然 fat margin 有避免一些，但可能還是會發生）。所以這一講我們希望能允許 SVM 能容忍一些小錯誤（雜訊），這樣的 SVM 就是 Soft Margin SVM。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-1.png">
</p>

<h3 id="hard-margin-svm-的缺點">Hard-Margin SVM 的缺點</h3>

<p>由於 Hard Margin SVM 堅持分好資料，所以在高維 Polynomial 及 Gaussian SVM 的學習模型可能會有 Overfitting 的現象，即使 SVM 的 Fat Margin 性質可以避掉一些，但 Overfitting 還是有可能發生，如下圖。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-2.png">
</p>

<h3 id="放棄一些小錯">放棄一些小錯</h3>

<p>回想之前學過的 Pocket PLA，我們是否也可以讓 SVM 也可以放棄一些小錯？在這邊我們將 SVM 要最佳化的式子加上了容錯項，在做錯的點上面允許 y_n(W^TZ_n + b) &gt;= 負無限大，也就是說錯了也沒關係，如此最佳化時就能允許錯誤了。其中 C 可以調整 large margin 跟容錯項，C 越大代表容錯越小，C 越小代表容錯越大。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-3.png">
</p>

<h3 id="soft-margin-svm-數學式-1">Soft Margin SVM 數學式（1）</h3>

<p>整理一下 Soft Margin SVM 的數學式，兩個限制式可以再合起來如下圖。但這個數學式不再是原來的 QP 問題了，而且目前的數學式是無法記錄犯了小錯或是大錯。</p>

<p>所以我們想辦法講原來的數學式轉換成可以記錄犯了多少錯，將犯了多少錯記錄在 xi 裡，如此就將原來的數學式轉換成線性的形式了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-4.png">
</p>

<h3 id="soft-margin-svm-數學式-2">Soft Margin SVM 數學式（2）</h3>

<p>xi 記錄的錯誤如下圖所示，表示 xi 違反了 margin 多少量，離 margin 越遠的，xi 值會越大，所以在 Soft-Margin SVM 我們除了最佳化 w, b，也要最佳化 xi。其中 C 可以調整 large margin 跟容錯項，C 越大代表容錯越小，C 越小代表容錯越大，margin 也就越大。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-5.png">
</p>

<h3 id="largrange-dual-解-soft-margin-svm">Largrange Dual 解 Soft Margin SVM</h3>

<p>我們可以仿造上一講的 Dual 方法解 Soft Margin SVM。由於我們多了 xi 這 N 個變數，所以我們必須多出 N 個 b_n Largrange Multiplier。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-6.png">
</p>

<h3 id="簡化-xi-及-b-n">簡化 xi 及 b_n</h3>

<p>然後對 xi 偏微分，得到在最佳解時 0 = C - a_n - b_n。將最佳解時的條件帶回原式，我們會得到更簡化的式子如下圖所示。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-7.png">
</p>

<h3 id="再簡化">再簡化</h3>

<p>我們繼續對數學式簡化，對 b 進行偏微分、對 w 進行偏微分，都可以得到在最佳解時的限制式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-8.png">
</p>

<h3 id="soft-margin-svm-的標準對偶數學式">Soft Margin SVM 的標準對偶數學式</h3>

<p>經過上述處理之後，Soft Margin SVM 的標準對偶數學式如下圖所示，藍色的部分是跟 Hard Margin 不一樣的地方，這是一個 QP 問題，只要帶入 QP Solver 就可以解出來。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-9.png">
</p>

<h3 id="kernel-soft-margin-svm">Kernel Soft Margin SVM</h3>

<p>仿造上一講介紹的 Kernel 方法，我們可以輕易做到 Kernel Soft Margin SVM，如此就可以進行無限多維轉換，alpah 可以使用 QP 算出來，整個算法幾乎跟 Hard Margin SVM 一摸一樣，只是多了一個上界的條件。現在問題只剩下 b 如何求得？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-10.png">
</p>

<h3 id="求得-b">求得 b</h3>

<p>如何求得 b 的方法跟 Hard Margin 一樣都是使用 complementary slackness 的性質。在 alpha 大於 0 的這些點可以用來求得 b。但這個式子還要先求得 xi。xi 則是可以在 alpah &lt; C 的這些點求得，因此 b 就可以求得了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-11.png">
</p>

<h3 id="觀察-soft-margin-gaussian-svm">觀察 Soft Margin Gaussian SVM</h3>

<p>我們來觀察一下 Soft Margin Gaussian SVM，其實如果使用不當還是會有 Overfitting 的現象產生。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-12.png">
</p>

<h3 id="alpha-的物理意義">alpha 的物理意義</h3>

<p>我們使用特出的 alpha 來計算出 w 及 b，這些 alpha 隱含著什麼物理意義呢？其中那些 alpha=0 的，就是對於 margin 沒有意義的點。alpha 大於 0 小於 C 的就是在邊界上的點。alpha = C 的，就是代表 xi 有值的點，就代表有違反邊界的點。我們可以利用 alpha 的性質來做一些資料分析。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-13.png">
</p>

<h3 id="選擇-model">選擇 Model</h3>

<p>我們可以使用 C 及 gamma 來挑整 Soft Gaussian SVM，那怎麼挑選 C 及 gamma 參數呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-14.png">
</p>

<h3 id="使用-cross-validation-來選">使用 Cross Validation 來選</h3>

<p>當然還是可以用之前學過的 cross validation 來挑選。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-15.png">
</p>

<h3 id="leave-one-out-cv-error-與-svm-的關係">Leave-One-Out CV Error 與 SVM 的關係</h3>

<p>這邊有一個 SVM 的特殊性質，SVM 的 Support Vector 數量除以 dataset 數量會是 Leave-One-Out Cross Validation Error 的上界。（non-SV 的 leave-one-out error 是 0，SV 的 leave-one out error 是 0 或 1，所以整體的 error 就是 #SV/N）</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-16.png">
</p>

<h3 id="用-sv-來選擇">用 #SV 來選擇</h3>

<p>了解了上述的性質之後，我們也可以利用這個性質來選擇模型，如果 Cross Validation 會計算很久，我們可以簡單的利用 Support Vector 的數量來選擇模型，理論上可以選到 Leave-One-Out CV Error 的上限。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-17.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講我們了解了 Soft-Margin SVM 的演算法，並且了解了各種 Support Vector 所代表的物理意義，Support Vector 跟 Leave-One-Out Cross Validation 之間的關係也可以讓我們用來選擇模型。</p>

<p>Soft-Margin SVM 也是大家一般口中所說的 SVM，一般都使用在分類問題上，之後的課程我們將介紹如何運用 SVM 相關的技巧來解不是二元分類的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-18.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 3 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-05-23T18:58:10&#43;08:00">
        
  May 23, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/">第 2 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，能夠將演算法跟高維度的計算脫鉤，但上一講中的 Q 矩陣實際上還是計算了高維度特徵矩陣內積，所以並沒有真的解決問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-1.png">
</p>

<h3 id="dual-svm-仍與高維度-d-有依賴關係">Dual SVM 仍與高維度 d 有依賴關係</h3>

<p>目前推導出來的 Dual SVM 仍與高維度 d 有依賴關係，我們能不能簡化 Q 矩陣的高維度特徵矩陣內積計算呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-2.png">
</p>

<h3 id="觀察矩陣內積的每一個運算">觀察矩陣內積的每一個運算</h3>

<p>我們用二次轉換拆開來觀察，發現原本將矩陣進行特徵轉換之後在做矩陣內積，可以分成 0 次項、1 次項、 2 次項分開來計算，結果會是一樣的。而更高維度的轉換也會有相同的性質。如此我們就可以限制計算量只在原本的特徵維度 O(d)。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-3.png">
</p>

<h3 id="kernel-的概念">Kernel 的概念</h3>

<p>有了上述的性質，我們可以引進 Kernel 的概念，之前都是將矩陣進行特徵轉換之後再去計算內積，現在我們可以改成使用 kernel function 來做計算，znTzm 可以改成對應的 kernel function K(xn, xm)。</p>

<p>由於我們都不去 z 空間做計算了，因此也無法得到 z 空間的 w，所以 b 與 w 的式子都要改成使用 kernel function K(xn, xm) 來計算。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-4.png">
</p>

<h3 id="用-qp-解-kernel-svm">用 QP 解 Kernel SVM</h3>

<p>導出 Kernel function 之後，我們一樣可以將 kernel function 計算出來的 Q 矩陣丟進去 QP Solver 來解 SVM。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-5.png">
</p>

<h3 id="polynomial-kernel">Polynomial Kernel</h3>

<p>我們可以將 Kernel Function 整理成更一般化的形式，這樣就可以推導出各式各樣的 Polynomail Kernel。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-6.png">
</p>

<h3 id="poly-2-kernel-圖示">Poly-2 Kernel 圖示</h3>

<p>我們使用一個例子來看一下 Poly-2 Kernel 的效果，我們可以調整 gamma 參數得出不一樣的分類曲線。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-7.png">
</p>

<h3 id="polynomial-kernel-一般化">Polynomial Kernel 一般化</h3>

<p>由 Poly-2 Kernel，我們可以再做更多變化，常數項用 zeta 當參數、特徵空間轉換用 Q 當參數，加上原本的 gamma 參數，Polynomial SVM 可以很自由地調整 Kernel 參數來得到更好的分類效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-8.png">
</p>

<h3 id="無限多維轉換的-kernel">無限多維轉換的 Kernel</h3>

<p>由於我們的演算法已經跟高維度的空間脫鉤了，所以我們有了這樣的想法，我們是不是可以使用無限多維轉換的 kernel 呢？</p>

<p>我們觀察一下指數函數 exp(-(x-x&rsquo;)^2)，結果發現就是一個對 X 的無限多維轉換，由此我們可以推導出下式的無限多維轉換 Kernel，也稱為 Gaussian kernel。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-9.png">
</p>

<h3 id="gaussian-svm">Gaussian SVM</h3>

<p>推導出 Gaussian Kernel 之後，使用 Gaussian Kernel 的 SVM 就是 Gaussian SVM。Gaussian SVM 演算法會與 Polynomial SVM 一樣，只是 Kernel 不一樣，由於是無限多維轉換，我們也不用再去煩腦要用幾次的轉換。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-10.png">
</p>

<h3 id="觀察-gaussian-svm-的效果">觀察 Gaussian SVM 的效果</h3>

<p>Gaussian SVM 是無限多維的轉換，因此可以預期它有很強的 power 可以做好分類，同時又保證 mergin 最大可以避免 overfit。但下圖中實驗調整 Gaussian Kernel 的 gamma 參數，其實還是有可能會產生 overfit，所以 Gaussian SVM 也不是萬能的，還是要謹慎驗證計算出來的結果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-12.png">
</p>

<h3 id="回顧-linear-kernel-優缺點">回顧 Linear Kernel 優缺點</h3>

<p>我們來回顧一下目前學過的 Kernel。首先是 Linear Kernel，優點是模型較為簡單，也因此比較安全，不容易 overfit；可以算出確切的 W 及 Support Vectors，解釋性較好。缺點就是，限制會較多，如果資料點非線性可分就沒用。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-13.png">
</p>

<h3 id="回顧-polynomial-kernel-優缺點">回顧 Polynomial Kernel 優缺點</h3>

<p>再來是 Polynomial Kernel，由於可以進行 Q 次轉換，分類能力會比 Linear Kernel 好。缺點就是高次轉換可能會有一些數字問題產生，造成計算結果怪異。然後太多參數要選，比較難使用。也因此 Polynomial Kernel 可能會用在比較低次轉換的 SVM 問題上，但這樣也許就可以用 Linear SVM 取代。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-14.png">
</p>

<h3 id="回顧-gaussian-kernel-優缺點">回顧 Gaussian Kernel 優缺點</h3>

<p>最後是 Gaussian Kernel，優點就是無限多維的轉換，分類能力當然更好，而且需要選擇的參數的較少。但缺點就是無法計算出確切的 w 及 support vectors，預測時都要透過 kernel function 來計算，也因此比較沒有解釋性，而且也是會發生 overfit。比起 Polynomail SVM，Gaussian SVM 比較常用。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-15.png">
</p>

<h3 id="其他-kernel">其他 Kernel</h3>

<p>除了目前介紹的 Kernel 之外是否還有其他 Kernel 呢？當然有，你也可以自己定義自己的 Kernel，只要符合 Mercer&rsquo;s condition 就是一個合法的 Kernel。不過要定義自己的 Kernel 並不是件容易的事。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-16.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講中，我們終於脫離了高維度空間的計算依賴，使用 Kernel Funciton 來解 Dual SVM，因此引進了 Polynomial Kernel SVM 的概念，最後甚至推導出了無限多維轉換的 Gaussian Kernel SVM。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-17.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
            
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 2 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-05-07T14:24:53&#43;08:00">
        
  May 7, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-1-jiang-xue-xi-bi-ji/">第 1 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-1.png">
</p>

<h3 id="非線性-svm">非線性 SVM</h3>

<p>學會了 Hard Margin Linear SVM 之後，如果我們想要訓練非線性模型要怎麼做呢？跟之前的學習模型一樣，我們只要將資料點經過非線性轉換之後，在高維空間做訓練就可以了。</p>

<p>非線性的轉換其實可以依我們的需求轉換到非常高維，甚至可能到無限多維，如果是無限多維的話，我們怎麼使用 QP Solver 來解 SVM 呢？如果 SVM 模型可以轉換到與 feature 維度無關，那我們就可以使用無限多維的轉換了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-2.png">
</p>

<h3 id="與特徵維度無關的-svm">與特徵維度無關的 SVM</h3>

<p>為了可以做到無限多維特徵轉換，我們需要將 SVM 轉為另外一個問題，在數學上已證明這兩個問題其實是一樣的，所以又稱為是 SVM 的對偶問題，Dual SVM，由於背後的數學證明很複雜，這門課程只會解釋一些必要的原理來讓我們理解。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-3.png">
</p>

<h3 id="使用-lagrange-multipliers-當工具">使用 Lagrange Multipliers 當工具</h3>

<p>我們在正規化那一講中曾經使用過 Lagrange Multipliers 來推導正規化的數學式，在推導 Dual SVM 也會使用到 Lagrange Multiplier。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-4.png">
</p>

<h3 id="將-svm-的限制條件轉換成無限制條件">將 SVM 的限制條件轉換成無限制條件</h3>

<p>在以往的課程中，我們已經了解有限制條件時，會造成我們找最佳解的困難，所以第一步我們先想辦法把 SVM 的限制條件轉換成無限制條件看看。</p>

<p>有了這樣的想法，我們把原本 SVM 的數學式改寫成一個 Lagrange Function，如下圖所示，原本的 N 的限制式改成了 1-yn(wTZn + b)，並用 N 個 Lagrange Multiplier 來做調整（N 個 alpha）。</p>

<p>數學需要證明 SVM 會等於 min (max Largrange Function, all alpha &gt;= 0)，我們可以先看一下 Largrange Function 的意涵。我們希望 SVM 可以完美的分好資料，所以 1-yn(wTZn + b) 應該都是 &lt;= 0，假設現在 1-yn(wTZn + b) 有一些正值的話，那 max Largrange Function 就會趨向無限大，所以如果我們找到正確的 b, w 分好資料，那 max Largrange 就會趨向 1/2wTw，這樣加上前面的 min，就可以知道 Lagrange Function 的轉換解出來的答案會跟原本的 SVM 一樣。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-5.png">
</p>

<h3 id="max-min-做交換">Max Min 做交換</h3>

<p>由於 Lagrange 對偶性質，Max Min 可以透過下圖的關係式做調換，原本的 min(max Lagrange Function) 有大於等於 max(min Lagrange Function) 的關係，在 QP 的性質上，其實又說兩邊解出來的答案會一模一樣。（這邊用單純的說明，沒有數學推導證明）</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-7.png">
</p>

<h3 id="解-lagrange-dual-1">解 Lagrange Dual (1)</h3>

<p>導出目前的 Lagrange Dual 式子 max(min Lagrange Function)之後，我們要來解看看最佳解了。由於 min Lagrange Function 是沒有限制條件的，所以我們可以用偏微分來求極值。</p>

<p>首先我們對 b 做偏微分，會得到 - sigma(anyn) ＝0，負號可以不用管，所以寫成 sigma(anyn) = 0。</p>

<p>將這個限制式代入原本的式子，就可以把原本的式子做一些簡化，如下圖所示。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-8.png">
</p>

<h3 id="解-lagrange-dual-2">解 Lagrange Dual (2)</h3>

<p>接下來我們對 wi 做偏微分，可以得到 w = sigma(anynzn)，一樣將這個限制式代入原本的式子，原本式子中的 w 就都可以換掉。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-9.png">
</p>

<h3 id="kkt-optimality-conditions">KKT Optimality Conditions</h3>

<p>將過這些最佳換轉換的式子，導出了一些限制式：</p>

<ol>
<li>yn(wTZn + b)&gt;= 1，這是原本要將資料分好的限制式</li>
<li>an &gt;= 0，對偶問題 Lagrange Multiplier 的條件</li>
<li>sigma(ynan) = 0，w = sigma(anynzn)，這是最佳解時會有的條件</li>
<li>在最佳解時，an(1-yn(wTZn + b)) = 0</li>
</ol>

<p>這就是著名的 KKT Optimality Conditions，目前這些 b,w 最佳解時的限制式，其中的變數就只剩下 an，所以實務上我們就要去找出最佳解時的 an 會是什麼，再利用上述的關係解出 b, w。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-10.png">
</p>

<h3 id="dual-svm">Dual SVM</h3>

<p>導出最佳解時的所有限制式之後，原來的式子可以改成下圖中的式子，這個式子其實也是一個 QP 問題，我們可以用 QP Solver 來解出最佳解時的 an。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-11.png">
</p>

<h3 id="用-qp-solver-解-dual-svm">用 QP Solver 解 Dual SVM</h3>

<p>我們可以用 QP Solver 解 Dual SVM，造上一講的做法去將 QP Solver 所需要的參數找出來，會有下圖中的參數。相等關係的限制式可以改成一個 &gt;= 及 一個 &lt;= 的關係，如果你所使用的 QP Solver 有提供相等關係的參數，那就不用這樣做。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-12.png">
</p>

<h3 id="特殊的-qp-solver">特殊的 QP Solver</h3>

<p>找出 Dual SVM 的所有 QP Solver 所需參數之後，我們就可以將參數丟進去 QP Solver 讓它幫忙解出最佳解。由於其中的 Q 參數可能會很大，因此使用有對 SVM 問題做特殊處理的 QP Solver 會比較沒問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-13.png">
</p>

<h3 id="找出最佳的-w-b">找出最佳的 w , b</h3>

<p>機器學習演算法最終就是要找出最佳的 w, b 來做未來的預測，不過現在 QP Solver 解出來的只有最佳解時的 an，我們要怎麼求出最佳解時的 w, b 呢？從 KKT Optimality Conditions 我們可以找出 w, b，w ＝sigma(anynzn) 這個條件可以算出 w，an(1-yn(wTZn + b)) = 0 這個條件可以算出 b，因為 an 通常會有大於 0 的情況，所以 1-yn(wTZn + b) 等於 0 才能符合條件，所以 b ＝yn - wTZn。</p>

<p>由於 an 大於 0 的點才能算出 b，這些大於 0 的 an 的資料點其實就是落在胖胖的邊界上。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-14.png">
</p>

<h3 id="support-vector-的性質">Support Vector 的性質</h3>

<p>由於 w = sigma(anynzn)，所以其實也只有 an 大於 0 的點會影響到 w 的計算，b 也是只有 an 大於 0 時才有辦法計算，所以 an 大於 0 的資料點其實就是 Support Vector。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-15.png">
</p>

<h3 id="support-vector-可以呈現胖胖的超平面">Support Vector 可以呈現胖胖的超平面</h3>

<p>我們來看看 w ＝ sigma(anynzn) 的含義，其實他的意思就是 w 可以被 Support Vecotr 線性組合呈現出來。這跟 PLA 也有點像，PLA 的 w 含義是被它犯錯的點的線性組合呈現出來。其實在 Logistic Regression 及 Linear Regression 也可以找到類似的性質，簡而言之，我們最後來出來做預測的 w 其實都可以被我們的訓練資料線性組合呈現出來。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-16.png">
</p>

<h3 id="比較一下-primal-及-dual-svm">比較一下 Primal 及 Dual SVM</h3>

<p>我們將 Primal 及 Dual SVM 的式子放在一起比較，其實可以發現 Dual SVM 與資料點的特徵維度 d 已經沒有關係了，因此可以做很高維度的特徵轉換。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-17.png">
</p>

<h3 id="但其實-dual-svm-只是把特徵維度藏起來">但其實 Dual SVM 只是把特徵維度藏起來</h3>

<p>但其實仔細一看 Dual SVM 只是把特徵維度藏起來，在計算 Q 時，就會與特徵維度牽扯上關係，這樣就還是無法做無限維度的轉換，我們如何真正不需要計算到高維度特徵呢？這是下一講的課程了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-18.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，算是把高維度的計算藏了一半，另一半就是下次的課程了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-19.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

          
          
  <div class="pagination-bar">
    <ul class="pagination">
      
        
          <li class="pagination-prev">
            <a class="btn btn--default btn--small" href="https://blog.fukuball.com/post/page/3/">
              <i class="fa fa-angle-left text-base icon-mr"></i>
              <span></span>
            </a>
          </li>
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="https://blog.fukuball.com/post/page/5/">
              <span></span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
      
      <li class="pagination-number"> </li>
    </ul>
  </div>


        </section>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Fukuball. 
  </span>
</footer>

      </div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
    
    <h4 id="about-card-name">Fukuball</h4>
    
      <div id="about-card-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Co-Founder / Head of Engineering at OurSong
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Taipei, Taiwan
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center"></div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-23smart-contract-%E5%88%9D%E6%8E%A2%E5%BE%9E-bytecode-%E5%88%B0-solidity/">
                <h3 class="media-heading">Ethereum 開發筆記 2–3：Smart Contract 初探，從 Bytecode 到 Solidity</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Ethereum 上的 EVM（Ethereum Virtual Machine）可以執行程式，而 EVM 上的可執行程式基本上是 Bytecode 的形式，所以所謂的 Smart Contract 就是存放在 Ethereum 上的 Bytecode，然後可由 EVM 來執行。
Bytecode Smart Contract 直接用 Bytecode 寫 Smart Contract 我們來嘗試一下直接用 Bytecode 來寫 Smart Contract，以下這段程式碼主要內容是執行運算後，將運算結果存放在 0 這個位置：
PUSH1 0x03 PUSH1 0x05 ADD // 3 + 5 -&gt; 8 PUSH1 0x02 MUL // 8 * 2 -&gt; 16 PUSH1 0x00 SSTORE // 將 16 存到 0 這個位置  這段程式轉成 Bytecode 就是：
0x60 0x03 0x60 0x05 0x01 0x60 0x02 0x02 0x60 0x00 0x55  也就是：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-22geth-%E5%9F%BA%E7%A4%8E%E7%94%A8%E6%B3%95%E5%8F%8A%E6%9E%B6%E8%A8%AD-muti-nodes-%E7%A7%81%E6%9C%89%E9%8F%88/">
                <h3 class="media-heading">Ethereum 開發筆記 2–2：Geth 基礎用法及架設 Muti-Nodes 私有鏈</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">要連上 Ethereum 就需要安裝 Ethereum Node，在這邊我們選擇使用 Geth 來安裝 Ethereum Node，接下來就來一步一步的學學怎麼使用 Geth，甚至如何使用 Geth 來架設自己的 Ethereum 私有鏈。
安裝環境 首先我們在 AWS 上開啟兩台 Ubuntu 虛擬機器，記得開 t2.medium（2 vCPU, 4 GB RAM）這個規格以上才跑得動，硬碟可以開 100 G，Security Group 將 TCP 30303 打開，Ethereum Node 之間是用 30303 這個 port 來溝通的。
接下來使用以下指令安裝 Geth：
$ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:ethereum/ethereum $ sudo apt-get update $ sudo apt-get install -y ethereum  兩台虛擬機器都要安裝，應該幾分鐘就可以裝好了。
使用 Main Net 安裝完 Geth 之後，我們就可以透過 Geth 連上 Ethereum Network 了，我們就來連上 Main Net 看看：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-21ethereum-%E9%96%8B%E7%99%BC%E6%95%B4%E9%AB%94%E8%84%88%E7%B5%A1/">
                <h3 class="media-heading">Ethereum 開發筆記 2–1：Ethereum 開發整體脈絡</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在第一次接觸 Ethereum 應用程式開發時，會發現有各式各樣工具，不知要從何下手，我們用一個圖來說明一下與 Ethereum 互動時的整體脈絡及這之間的工具主要做了什麼事，了解之後自己就可以挑選開發時、甚至使用在產品上時要用什麼適合的工具了。
要在自己的機器接上 Ethereum 首先需要安裝 Ethereum Node，我們之前安裝的 Mist 其實就會在我們的機器上安裝 Ethereum Node 並同步帳本，而像這樣安裝 Node 並同步帳本甚至進行挖礦的軟體有很多，大家可以去選擇適合自己使用的。Mist 其實是將一個叫 geth 的軟體用 GUI 包裝起來，如果是開發者的話，可以選擇直接安裝 geth。
geth 提供了許多 API 指令可以讓我們跟 Ethereum 做互動，但有時下指令並不是那麼親和，所以 geth 提供了 RPC(Remote Procedure Calls) 與 IPC(Inter-process Communications) 兩種方式來與 geth 互動，如果你要在 local 機器連上 geth，那就可以使用 IPC；如果要讓遠端連上 geth，那就使用 RPC，可以開 HTTP 或 Web Socket 兩種方式來讓遠端使用。
以上就是 Ethereum 應用程式開發的基礎環境，接下來跟開發網頁應用程式一樣，Ethereum 應用程式也分成後端與前端，後端程式就是 Smart Contract，前端程式就是 Dapp。Smart Contract 可使用 Solidity 撰寫，目前也有許多其他語言可以撰寫 Smart Contract。Smart Contract 要在 Ethereum 上的 EVM 執行要先 Compile 成 Byte Code 之後，再透過 IPC 或 RPC 發佈到 Ethereum 上。前端程式的 Dapp 可用 Web3 JavaScript 透過 RPC 接上 Ethereum，以及使用網頁應用常用到的 HTML、CSS、JavaScript 製作成使用者互動介面，如此就能執行發佈在 Ethereum 上 Smart Contract 所提供的一些程式功能了。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-18ethereum-%E7%9A%84%E7%8D%8E%E5%8B%B5%E6%A9%9F%E5%88%B6/">
                <h3 class="media-heading">Ethereum 開發筆記 1–8：Ethereum 的獎勵機制</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Bitcoin 的獎勵機制基本上是挖到新區塊的節點獲得記帳權及獎勵，Ethereum 大體也是遵循這樣的概念，但做了一些調整與變化，讓我們整個脈絡了解一下。
由於 Blockchain 是一種去中心化的系統，所有的礦工（節點）可以同時挖礦（計算合法 hash），彼此獨立運作，所以極有可能出現兩的礦工同時發現不同的滿足條件的區塊，如此就會產生我們之前有提過的分叉（Fork）。
那我們該採用誰的區塊當主鏈呢？我們會先依工作量最大的區塊為主鏈，如果工作量一樣，就看誰先接了子區塊，一般來說只有成了主鏈的區塊才能獲得獎勵。但這樣沒有變成主鏈的區塊之前的算力就都白費了，所以 Ethereum 創造了 Uncle Block（叔塊）這樣的概念，不能成為主鏈的區塊如果後來被收留成為 Uncle Block，那這些沒有成為主鏈的區塊也有機會可以做為 Uncle Block 而獲得獎勵。
這就是 Ethereum 共識機制中的 GHOST（Greedy Heaviest Observed Subtree）協議，Ethereum 會這樣設計的原因，是由於 Ethereum 產生區塊的速度較快，也因此較容易產生分叉，也會使得新區塊較難以在整個網絡傳播，這對於傳播速度較慢的區塊並不公平。且分叉後的區塊可能在幾個區塊之後整併起來，我們會發現裡面的交易可能會與主鏈一致（雖然單獨查看分塊交易內容不同，不過數個區塊整體一起看交易內容就一致了），符合這種條件的分叉區塊我們就會納入主鏈參考，這些區塊就成了所謂的 Uncle Block，這某種角度也是更確認了 Blockchain 上的交易內容一致，因此 Uncle Block 也有貢獻，應該給予獎勵。
以上我們已經了解了 Ethereum 上的區塊大致分成兩種，普通區塊和 Uncle Block，Ethereum 對這兩種區塊的獎勵方式是不同的。我們分別來看一下。
普通區塊獎勵  固定獎勵 5 ETH 區塊內所有的 Gas Fee 如果區塊納入了 Uncle Block，那每包含一個 Uncle Block 可以得到固定獎勵 5 ETH * 1/32，也就是 0.15625 ETH，一個區塊最多隻能包含 2 個 Uncle Block，也因此不會無限延伸，同時又可鼓勵區塊納入 Uncle Block，增加交易內容的一致性。  Uncle Block 獎勵  用公式計算：（Uncle Block 高度 + 8 - 包含此 Uncle Block 的區塊的高度）* 普通區塊固定獎勵 / 8  我們用個實例來看一下獎勵怎麼算。首先我們來看一個普通區塊：https://etherscan.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-17blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E6%80%A7%E8%B3%AA/">
                <h3 class="media-heading">Ethereum 開發筆記 1–7：Blockchain 的一些重要性質</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">我們這邊再次總結一下 Blockchain 中幾點較重要的性質，包含共識機制、不可竄改、經濟激勵三項。
共識機制（Consensus） 在分散式系統中，我們需要有一套用於協同合作的共識機制來組織行動，但有時候系統中的成員可能會出錯或是故意傳送出錯誤的資訊，而使得網路中不同成員對於全體協作的策略得出不同的結論，進而破壞系統的一致性，這就是所謂的拜占庭將軍問題。
拜占庭將軍問題（Byzantine Generals Problem） 拜占庭將軍問題這個故事是這樣的：
 一組拜占庭將軍分別各率領一支軍隊共同圍困一座城市，這個敵人雖不比拜占庭帝國，但也足以抵禦 5 支拜占庭軍隊的同時襲擊。這 10 支軍隊在分開的包圍狀態下，他們任 1 支軍隊單獨進攻都毫無勝算，除非有至少 6 支軍隊（一半以上）同時襲擊才能攻下敵國。他們分散在敵國的四周，依靠通信兵騎馬相互通信來協商進攻意向及進攻時間。困擾這些將軍的問題是，他們不確定他們中是否有叛徒，叛徒可能擅自變更進攻意向或者進攻時間。在這種狀態下，拜占庭將軍們才能保證有多於 6 支軍隊在同一時間一起發起進攻，從而贏取戰鬥？
 上述的故事對映到電腦系統裡，將軍便成了電腦，而通信兵就是通訊系統。叛徒發送前後不一致的進攻提議，被稱為「拜占庭錯誤」，而能夠處理拜占庭錯誤的這種容錯性稱為「Byzantine Fault Tolerance」。Blockchain 上的共識機制通常具有容錯的設計來達成一致性，主要比較常見的共識機制方法有兩個，「工作量證明」以及「股權證明」兩種方法。
工作量證明演算法（Proof of Work, PoW） 中本聰在 Bitcoin 中創造性的引入了「工作量證明」（俗稱挖礦）來解決拜占庭將軍問題，顧名思義，工作量證明就是用來證明你做了一定量的工作，可用工作成果來證明完成相應的工作量。其中的工作技術原理可以看之前這篇文章：Ethereum 開發筆記 1–4：Blockchain 技術原理簡介
由於工作量證明具相當高的計算成本，因此無誘因去偽造，只有遵守協議約定，才能夠回收成本並獲得收益，也因此減少了叛徒的產生，減少拜占庭錯誤。
股權證明演算法（Proof of Stake, PoS） 股權證明的出現，主要是希望取代工作量證明，進而減少「挖礦」的大量運算。它與工作量證明不同地方在於：工作量證明中，大家比的是「算力」（運算能力），透過大量運算得出符合難度的 Hash 值，進而得到獎勵；而在股權證明，大家比拼的是「股權」，「股權」越大的人（節點）越大機會負責產生新區塊，進而得到獎勵。
舉例來說，在股權證明系統中所有擁有股權（此 Blockchain 的數位貨幣）的人都有機會被挑選為產生新區塊（也就是記帳）的人，擁有更多股權的人被選中的機率越大。假這這個系統中共有三個人：Alice 持有 50 股、Bob 持有 30 股、Cathy 持有 20 股，那每次 Alice 被選為記帳人的機率會是 Cathy 的兩倍。所以股權證明會驅使人們購買更多的股權，進而增加獲選為記帳人的機率，以買股權來代替挖礦，同樣需要付出高成本，也因此可以減少叛徒的產生，減少拜占庭錯誤。
不可竄改（Immutability） Blockchain 不可竄改的性質主要來自資料結構及 hash 方式的設計，讓資料的順序緊密鏈結，若從中竄改了某些資料，那之後的鏈結 hash 都會發生錯誤，形成了 Blockchain 不可竄改的特性。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-16blockchain-%E7%9B%B8%E9%97%9C%E7%9A%84%E5%8A%A0%E5%AF%86%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98/">
                <h3 class="media-heading">Ethereum 開發筆記 1–6：Blockchain 相關的加密基礎知識</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Blockchain 裡應用了一些加密技術來保證及驗證交易訊息的正確性，這也更加強了 Blockchain 資料不可竄改的特性。我們來介紹其中比較重要的「公私鑰加密」以及「Merkle Tree」加密樹。
公私鑰加密 公私鑰加密算法是目前資訊通訊安全的基石，它保證了加密訊息不可被破解，相關的加解密原理大家可以參考這兩篇文章：
 RSA算法原理（一）http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html RSA算法原理（二）http://www.ruanyifeng.com/blog/2013/07/rsa_algorithm_part_two.html  加密與解密 公私鑰加密方法是一種非對稱式加密，透過公鑰加密過後的訊息只有私鑰可以解密，也因此只要保護好私鑰就能保證資訊的安全。
現在假設 Alice 要傳一個訊息給 Bob，希望訊息加密過後只有 Bob 可以解密，大概會經過如下步驟：
 Bob 傳他的公鑰給 Alice Alice 使用 Bob 的公鑰加密訊息 Alice 將加密過後的訊息傳給 Bob Bob 用他的私鑰解密訊息  我們這邊使用 openssl 來練習一下加密與解密，首先我們來產生一對公私鑰：
// Create RSA private key $ openssl genrsa -des3 -out rsa-key.pem 2048 // Create public key $ openssl rsa -in rsa-key.pem -outform PEM -pubout -out rsa-key-pub.pem  其中 rsa-key.pem 就是私鑰，rsa-key-pub.pem 為公鑰，私鑰會要求設置密碼，請妥善記下密碼。
我們先用 rsa-key-pub.pem 加密資料：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-15blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%BE%A9%E8%88%87%E5%90%8D%E8%A9%9E/">
                <h3 class="media-heading">Ethereum 開發筆記 1–5：Blockchain 的一些定義與名詞</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在 Ethereum 開發筆記 1–4 應該已經將 Blockchain 的技術原理說明得很清楚了，不過如果要向一般大眾簡單說明 Blockchain 是什麼，要怎麼說呢？我會說：Blockchain 就是一個分散式帳本，大家都有一樣的帳本，大家都可一起參與記帳，且記完帳大家的帳本就會自動更新到最新版本，而帳裡的紀錄都會分塊並用密碼按順序鏈結起來，用以驗證帳的正確性，如果中間有人改了資料，那後面的鏈結密碼都會發生錯誤，因此沒有人可以亂改帳，這就是 Blockchain。
但 Blockchain 這個名詞還包含了許多概念與內涵，我們之前說過，Blockchain 是因為分散式去中心化帳本的發展而慢慢產生出來的，這樣慢慢被統稱出來的名詞裡底下也就會包含了許多內涵，很難用三言兩語來說明，所以有一些 Blockchain 相關的定義與名詞我們都可以了解一下，這樣就能更了解 Blockchain。
交易（Transaction） 交易是 Blockchain 帳本中的原子單位，如果將交易再往下拆分就會變得沒有意義，比如下列就是一個交易：
 A 減少了 $10 B 增加了 $9 C 增加了 $1  如果只看 1，我們就會想那減少的 $10 到哪裡去了？所以 1、2、3 一起看才算是一個交易。
Blockchain 是一個分散式帳本（Distributed Ledger） 不像銀行依靠自己的帳本來記帳，Blockchain 提供了可靠的分散式帳本，當銀行之間要進行交易時，會需要一個受信任的第三方來進行銀行之間的交易，這也是為何你在做跨國轉帳時，需要付出高昂的手續費以及等待數天處理交易，Blockchain 可靠的分散式帳本讓跨國交易可以在幾分鐘甚至幾秒之內完成，這也是為何銀行想要應用 Blockchain 在金融交易上以降低交易成本。
Blockchain 是一個資料結構（Data Structure） 通常 Blockchain 的資料結構如下組成：
 交易是原子單位 區塊是由一系列的交易組成 區塊鏈由排序良好的區塊所組成  Blockchain 會有分叉（Fork） 當有兩名礦工 A 及 B 幾乎在相同時間內算出了合法的 hash，這兩個區塊傳播到鄰近節點時，有些節點收到了 A 的區塊，有些節點收到了 B 的區塊，這兩個區塊都可以是主鏈的延伸，這時就會產生區塊鏈分叉。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-14blockchain-%E6%8A%80%E8%A1%93%E5%8E%9F%E7%90%86%E7%B0%A1%E4%BB%8B/">
                <h3 class="media-heading">Ethereum 開發筆記 1–4：Blockchain 技術原理簡介</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前我們簡單地介紹過 Blockchain 了，但我們還是對 Blockchain 背後的技術原理不是那麼了解，我們知道 Blockchain 是因為一個數位貨幣帳本這樣的概念被創造出來的，而數位貨幣最擔心的是什麼問題呢？其實就是雙重支付（Double-Spending）這樣的問題。
數位貨幣不像實體貨幣，數位資產比起實體資產容易複製，也因此如果花用數位貨幣的行為如果沒有處理好，就會產生憑空多出其他交易，這就像是偽鈔一樣，會造成通貨膨脹而導致貨幣貶值，讓人不再信任並願意持與流通。因此數位貨幣的支付通常需要一個受信任的第三方來做驗證，這樣的做法雖然簡單，卻存在單點脆弱性，只要這第三方受到攻擊或是監守自盜也一樣會讓這個數位貨幣變成一個失敗的貨幣。
分散式去中心化帳本能解決單點脆弱性的問題，但在驗證正確性這點難度卻很高，所有的節點都有記帳的權利，要如何確定由誰來記帳、記的帳對不對？如果無法確定帳是對的，那就存在雙重支付的風險。
為了改善單點脆弱性及雙重支付這樣的問題，許多分散式的雙重支付防範方法慢慢被提出來，中本聰提出了去中心化（以受信任第三方為中心）的方法來展示解決雙重支付問題，並實作出了 Bitcoin，使用共識機制來解決記帳及驗證的問題，這帶來去中心化數位貨幣帳本的成功。
Bitcoin 的共識協議主要由「工作量證明」（Proof-of-Work, PoW）和「最長鏈機制」兩部分組成，Bitcoin 上的各個節點就是透過共識機制中的工作量證明來決定誰有記帳權，然後取得記帳權的節點就能將新的區塊記帳加到最長鏈上並給予該節點獎勵（新區塊獎勵及交易費收益）。
Bitcoin 的 工作量證明大概會做以下的事情：
 收集還未記到帳上的交易 檢查每個交易中付款地址有沒有足夠的餘額 驗證交易是否有正確的簽名 把驗證通過的交易信息進行打包（組成 Merkle Tree） 為自己增加一個交易紀錄獲得 Bitcoin 獎勵金 計算合法的 hash 爭奪記帳權  計算合法 hash 的方式請見下方影片說明，個人覺得這個影片是目前將 Blockchain 加密機制說明得最清楚的影片。我這邊簡略說明一下，合法的 hash 公式大致看起來像這樣：hash(交易內容+交易簽名+nonce+上一個區塊的 hash)，我們要取得記帳權，就需要找出前面開頭有 N 個 0 的 hash，由於交易內容、交易簽名及上一個區塊的 hash 都是不可變的，所以每個節點就是不斷的調整 nonce 來計算得出不同的 hash，直到找到開頭 N 個 0 的 hash 為止，第一個找的節點就能獲得記帳權，而其他的節點只要計算 hash 對不對就能驗證這個帳對不對。其中 N 個 0 開頭的 hash 就代表了計算的難度，越多 0 代表越難找到這樣的 hash，也因此可以調整計算難度。就是這樣的設計解決了去中心化分散式系統驗證資料及決定記帳順序的難題，也就改善了數位貨幣單點脆弱性及雙重支付的問題。
  以上的內容看完應該就能大體了解 Blockchain 的原理了，甚至要自己做一個 Blockchain 都沒問題！了解了 Blockchain 的技術原理之後，應該能更信任去中心化的數位貨幣的安全性，或許有天大家都信任了去中心化的數位貨幣我們就真的能廣泛使用數位貨幣，為經濟活動帶來更有效率的流通。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98%E7%B7%B4%E7%BF%92-1%E4%BD%BF%E7%94%A8-mist-%E7%99%BC%E8%A1%8C%E8%87%AA%E5%B7%B1%E7%9A%84-token/">
                <h3 class="media-heading">Ethereum 開發筆記練習 1：使用 Mist 發行自己的 Token</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前說過，Blockchain 基本上是因為金流帳本這樣的問題而被創造出來的，也就是說區塊鏈非常適合運用在金流的應用上，我們也可以建立自己的 Blockchain 來搭建自己的金流系統，不過在 Ethereum 上 Smart Contract 這種設計讓我們擁有可以在 Ethereum 區塊鏈上創造自己金流系統的能力，如此我們就不需要自己建一條鏈了。
我們使用 Smart Contract 仿造貨幣性質創造了數位資產（說穿了其實就是在 Smart Contract 上紀錄的變數而已），而這種具貨幣性質的數位資產又被稱作 Token，如此我們就可以在應用程式中使用這個去中心化的金流系統，由於 Token 的應用很普遍，大部分的功能都已經標準化了，我們只要仿造標準來實作就可以發行自己的數位貨幣了。
在這邊我們就練習一下怎麼使用 Mist 發佈 Token Smart Contract 來發行自己的數位貨幣。（目前我們還沒有學習過如何撰寫 Smart Contract，因此這邊會先直接提供範例程式碼，實作的部分我們之後再慢慢學習）
以下是我們的範例程式碼：
 請打開 Mist，如下圖點擊 Contract，然後點擊 Deploy New Contract。
你會看到如下圖的頁面，請在 Solidity Contract Source Code 中貼上我們上面提供的範例程式碼。
貼上範例程式碼之後，Mist 會自動編譯程式，檢查是否有語法上的錯誤，如果沒問題，右方的 Select Contract to Deploy 就會出現選項，在這邊我們選擇 Token ERC 20。
選擇 Token ERC 20 之後，右方會出現要初始化 Contract 的參數表單，有 Initial supply、Token name、Token symbol 需要填寫。Initial supply 代表 Token 的總發行量是多少，我這邊設定成 7777777777，你可以設成你想要的數字。Token name 就是這個 Token 要叫什麼名字，這邊我設定成 7 Token，你想要取 Dog Coin 或是 Cat Coin 也都可以。Token symbol 就是這個 Token 要用什麼代號，像是美金就是用 $、Ether 是用 ETH，這邊我設定成 7token，你可以取自己覺得帥的代號。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-13%E4%BD%BF%E7%94%A8-mist/">
                <h3 class="media-heading">Ethereum 開發筆記 1–3：使用 Mist</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Mist 跟前回介紹的 MetaMask 一樣是可以與 Ethereum 進行互動的工具，除了可以管理 Ethereum 相關密鑰之外，Mist 還包含了 Ethereum 節點以及網頁瀏覽器，方便大家瀏覽 Dapp 網頁。
首先請到這邊安裝 Mist，請選擇適於自己的作業系統安裝。
由於 Mist 會安裝節點在你的電腦裡，也因此會同步整個帳本下來，所以會花上不少時間同時也會佔用許多硬碟空間。我們目前僅是要使用測試鏈，所以請切換到 Ropsten 測試鏈（如下圖），這樣就不用花這麼多時間與空間了。
在 Mist 的左下角可以觀察目前已同步到你的電腦的區塊數（如下圖），如果這個數字跟 Etherscan（Etherscan 是一個可以查看 Ethereum 區塊鏈所有交易的網站） 上的最新區塊數一致的話，那就代表已經同步完成了。
接下來讓我們用 Mist 開一個 Ethereum 帳戶，請點擊 Add Account，並依指示輸入密碼後創建帳號，密碼請務必要記下來，將來交易時都會需要輸入你的密碼。
學會創建 Ethereum 帳戶之後，我們要來看一下 Mist 要怎麼備份帳號，請點擊 Mist 上方選單的 File -&gt; Backup -&gt;Accounts（如下圖），這樣就會打開帳號存放的資料夾，所有的帳號都會加密存在這邊，所以只要備份這些檔案及當時設定的密碼，你就可以在別台電腦復原你的帳號。
現在你這個 Ethereum 帳戶還沒有任何 Ether，我們仿造之前用 MetaMask 來跟水龍頭要 Ether 的步驟來取得 Ether 看看。
我個人提供了一個水龍頭 Dapp，請前往這個網址來取得 Ether：https://blog.fukuball.com/dapp/faucet/
由於 Mist 也是一個 Dapp 網頁瀏覽器，請在 Mist 上方的網址列輸入：https://blog.fukuball.com/dapp/faucet/
Mist 在揭露你的 Ethereum 帳戶資訊給 Dapp 網頁時都會詢問你的同意，請先選擇要瀏覽這個 Dapp 網頁的帳號（你可能在 Mist 有多個帳號，所以就需要選擇目前要用哪個帳號瀏覽這個網頁）。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero=""
         data-message-one=""
         data-message-other="">
         76 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://blog.fukuball.com/images/ok.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://blog.fukuball.com/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>






<script async src="https://www.googletagmanager.com/gtag/js?id=UA-41911929-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-41911929-4');
</script>

    
  </body>
</html>

