


  




  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.48 with theme Tranquilpeak 0.4.3-BETA">
    <title>Machine Learning</title>
    <meta name="author" content="Fukuball">
    <meta name="keywords" content="">

    <link rel="icon" href="images/favicon.ico">
    
      <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.fukuball.com/tags/machine-learning/index.xml">
    

    
    <meta name="description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Machine Learning">
    <meta property="og:url" content="/tags/machine-learning/">
    <meta property="og:site_name" content="I am Fukuball">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="I am Fukuball">
    <meta name="twitter:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    
    

    
    

    
      <meta property="og:image" content="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=640">
    

    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://blog.fukuball.com/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="1">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://blog.fukuball.com/">I am Fukuball</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://blog.fukuball.com/#about">
    
    
    
      
        <img class="header-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=90" alt="" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="1">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://blog.fukuball.com/#about">
          <img class="sidebar-profile-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
        </a>
        <h4 class="sidebar-profile-name">Fukuball</h4>
        
          <h5 class="sidebar-profile-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://facebook.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-facebook"></i>
      
      <span class="sidebar-button-desc">Facebook</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">Blog</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      
        

      
      <div id="main" data-behavior="1"
        class="
               hasCoverMetaIn
               ">
        
          <section class="postShorten-group main-content-wrap">
            
            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 5 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-10-15T13:08:14&#43;08:00">
        
  October 15, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/">第四講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<p>在第四講中我們了解了在有限假設集合的情況下機器學習是可能的，而第五講就是想要將有限假設集合可以推廣出去，讓我們在無限的假設集合裡也可以透過一些理論慢慢收斂到一個多項式集合，如此我們就可以放心的利用機器學習來解決我們所面對的一些問題。</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>我們先來回顧一下上一講的內容，在上一講我們知道了機器學習在足夠的資料及有限的假設集合這種情況下是可行的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-1.png">
</p>

<h3 id="更新機器學習流程圖">更新機器學習流程圖</h3>

<p>如果假設集合 H 是有限 M 個，然後資料量夠多 N，不管我們的演算法 A 是什麼，我們由定理可以知道 Eout 跟 Ein 是很接近的。所以如果 A 找到了一個假設 g 讓 Ein 近似於 0，那我們就可以說 Eout 大概就會是 0，因此機器學習在這樣的情況下是可行的。</p>

<p>有了這樣的概念，我們擴充了我們的機器學習流程圖。從輸入資料中去訓練機器學習，然後得到 Ein 近似於 0，之後再從同一個資料分佈中去測試機器學習的結果，如此可以證明機器有學習到技能。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-2.png">
</p>

<h3 id="機器學習的兩個核心問題">機器學習的兩個核心問題</h3>

<p>機器學習有兩個核心問題，我們希望 Eout 跟 Ein 是很接近的，這個意思就是說，我們希望後來測試學習的結果，會跟訓練時得到的結果很接近，這樣我們才能說機器有學習到技能。</p>

<p>另一個就是，我們希望 Ein 可以很小，也就是訓練的過程中，我們希望機器就可以得到很好的效果，也就是誤差很小。</p>

<p>那之前我們所說的有限集合 M 在這邊扮演什麼角色呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-3.png">
</p>

<h3 id="ｍ-的兩難">Ｍ 的兩難</h3>

<p>如果假設集合 M 很小，我們可以保證 Eout 可以接近 Ein，但是因為假設集合小，可以挑選的選擇就少，也因此 Ein 可能會是一個不小的值，也就是誤差會大。</p>

<p>如果假設集合 M 很大，我們可以會得到一個比較好的 Ein，也就是誤差比較小，但是 M 太大我們就無法保證 Eout 會跟 Ein 接近，也就是我們無法保證學習的結果，那機器就白學了。</p>

<p>所以 M 是在哪個值剛好能同時解決這個兩個問題就是一個重點。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-4.png">
</p>

<h3 id="ｍ-從哪裡來">Ｍ 從哪裡來？</h3>

<p>從上一講的定理證明中，我們知道 M 是從當較大的誤差發生的情況累積出來的，所以如果假設集合越大，那累積出來的就一個越大的數字，如果是無限集合，那就沒有上限，但如果是有限集合的話，那就會有個上限。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-6.png">
</p>

<h3 id="大誤差發生的情況有很多時候是重疊的">大誤差發生的情況有很多時候是重疊的</h3>

<p>但其實那個上限值我們是高估的，因為許多種大誤差發生的情況都可能是很相似的情況，所以其實這些大誤差發生的情況有很大一部份是重疊的。</p>

<p>那我們可以轉一個想法，我們可以把無限的假設集合依照它們的類型來分類嗎？這樣就可以轉換成一個有限的集合。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-7.png">
</p>

<h3 id="一個點的時候有幾種線">一個點的時候有幾種線</h3>

<p>之前的 PLA 演算法其實有無限多的假設集合 H，所以在平面上分類一個點的線有無限多條，但是如果說有幾種線，那就只有兩種，一種線是將 x1 分成 o 的，一種線是將 x1 分成 x 的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-8.png">
</p>

<h3 id="兩個點的時候有幾種線">兩個點的時候有幾種線</h3>

<p>以此類推，兩個點的時候，平面上會多多少種分類這兩個點的線呢？答案是四種線。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-9.png">
</p>

<h3 id="四個點的時候有幾種線">四個點的時候有幾種線</h3>

<p>再以此類推，四個點的時候，平面上會多多少種分類這四個點的線呢？我們會發現下圖中有兩個組合已經無法找到直線可以做好分類，我們最多只能找到 14 條可以分類四個點的線。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-10.png">
</p>

<h3 id="effective-number-of-lines">Effective Number of Lines</h3>

<p>這個故事告訴我們，隨著輸入變多，線的種類會慢慢變成不是指數型成長。依這樣的概念，我們將原本無限多的線轉變為有限多的不同種類的線，我們就可以用這個有限多的不同種類的線的數字來取代 M，這個數字會小於 2 的 N 次方。</p>

<p>這樣我們就可以再套到霍夫丁的定理，知道這樣的過程機器學習的解決會是有效可行的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-11.png">
</p>

<h3 id="dichotomies">Dichotomies</h3>

<p>這樣的過程就是將原本無限多的線，轉換成分種類的線 Dichotomies，一個 Dichotomy 就是一種分類組合，在二元分類裡這樣組合的上界就是 2 的 N 次方，我們可以用這個數字來取代無限大的 M。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-12.png">
</p>

<h3 id="找出問題的成長函數">找出問題的成長函數</h3>

<p>了解這樣的特性之後，我們只要找出我們要解的問題的成長函數，而這個成長函數不會無限增加，那機器學習就可以證明是可行的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-13.png">
</p>

<h3 id="四種成長函數">四種成長函數</h3>

<p>我們這邊列出了四種成長函數，分屬不同的問題，PLA 是屬於 2D perceptrons，mH 會小於 2 的 N 次方，代進霍夫丁不等式，由於是指數型成長，並無法保証式子會成立。所以我們希望能證明 PLA 的 mH 會是一個多項式，這樣才能保證霍夫丁不等式成立。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-14.png">
</p>

<h3 id="break-point-的概念">Break Point 的概念</h3>

<p>Break Point 指的是，當下一個輸入出現時，Dichotomies 組合不再是指數型成長的的那個點，在 2D perceptrons 這邊從我們剛剛的例子可以知道 break point 出現在 4 個輸入時。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-15.png">
</p>

<h3 id="四種成長函數的-break-points">四種成長函數的 Break Points</h3>

<p>我們知道 positive rays 的 break point 出現在 2，他的成長函數是 big O N，positive intervals 的 break point 出現在 3，他的成長函數是 big O N 平方，那麼在 2D perceptrons 時，我們知道 break point 出現在 4，那他的成長函數是 big O N 三次方嗎？我們可以推廣成一個通式嗎？這就是下次課程的內容了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-16.png">
</p>

<h3 id="總結">總結</h3>

<p>機器學習可能的兩個核心問題是 Ein 近似於 0，且 Eout 跟 Ein 要接近。PLA 無限多的線我們可以轉換成 Effective Number of Lines 也就是轉換成 Effective Numbers of Hypotheses，M 無限集合也就轉為 mH 的有限集合，然後觀察 break point 的出現，讓我們可以知道假設集合不會是一個指數型成長的情況，如此依照霍夫丁不等式所說的，我們就可以讓機器學習的理論有充分的證明為可行了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-17.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 4 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-09-27T10:49:15&#43;08:00">
        
  September 27, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/">第三講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<p>第四講的內容主要是讓我們知道機器學習是否真的可能，並利用數學上的定理來說明機器學習在某些情境之下是可能的，有數學上定理的支持，我們就可以放心的利用機器學習來解決我們所面對的一些問題。</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>我們先來回顧一下上一講的內容，在上一講我們知道了各式各樣的機器學習方法及名詞，而我們未來會專注於二元分類及迴歸這樣的問題，然後使用大量監督式標示好的資料且定義明確的特徵來進行機器學習。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-1.png">
</p>

<h3 id="看看這個問題-想想如何使用學習">看看這個問題，想想如何使用學習</h3>

<p>有人會問，說了這麼多，如何知道機器學習是不是真的可能？說不定根本無法做到。比如這個問題，g(x)可以回答 +1 還是 -1。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-2.png">
</p>

<h3 id="見仁見智的問題無法解">見仁見智的問題無法解</h3>

<p>像這樣的問題，你可以回答 +1，因為 +1 的圖都是對稱的，而這個圖是對稱的，所以是 +1。你可以回答 -1，因為 -1 的圖都是左上方黑色的，而這個圖是左上方黑色的，所以是 -1。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-3.png">
</p>

<h3 id="套到二元分類的問題">套到二元分類的問題</h3>

<p>現在我們套到二元分類的問題，給了 Xn 及 Yn，然後機器學習出了 g，我們可以說 g 近似於 f 嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-4.png">
</p>

<h3 id="天下沒有白吃的午餐">天下沒有白吃的午餐</h3>

<p>在驗證 g 的時候，如果是用原本的 D，那我們很容易的說明 g 近似於 f，但是如果資料是用 D 以外的資料來驗證，那我們無法很明確的說明 g 近似於 f，但我們要的其實就是希望 g 在 D 以外的資料也近似於 f，這有可能嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-5.png">
</p>

<h3 id="利用罐子取彈珠的例子來說明是否可能">利用罐子取彈珠的例子來說明是否可能</h3>

<p>現在想像我們有一個裡面有很多橘色和綠色彈珠的罐子，我們可能無法知道橘色彈珠的真實比例，但我們可以推估出橘色彈珠出現的機率嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-6.png">
</p>

<h3 id="取樣看看">取樣看看</h3>

<p>我們取樣看看，比如從罐子中取出十顆彈珠，假設現在取出的是三顆橘色七顆綠色，那我們可以說罐子中的比例是 30% 橘色 70% 綠色嗎？可能不能這樣說，有可能不是這樣比例，但很可能 30% 橘色 70% 綠色是一個很接近的比例數字。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-7.png">
</p>

<h3 id="hoeffding-不等式-1">Hoeffding 不等式 1</h3>

<p>我們可以說取樣出來的結果會很接近真實情況，是因為 Hoeffding&rsquo;s Inequality 這個定理。這個定理的數字如下圖，這說明了當 N 很大，也就是我們取樣的數字很大，那們 v - u 就會是一個很小的數字，也就是我們預估的 g 跟真實的 f 的差距很小。而當 ε 很大，也就是我們可以容忍的誤差很大的話，那當然我們就可以很容易地說 g 跟真實的 f 的差距很小。只要符合了這個不等式，那就叫做 probably approximately correct（PAC）。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-8.png">
</p>

<h3 id="hoeffding-不等式-2">Hoeffding 不等式 2</h3>

<p>通常我們不會希望容忍誤差很大，所以通常我們會取 N 很大來推估 g 是否接近 f 的真實情況。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-9.png">
</p>

<h3 id="把-hoeffding-不等式連結到機器學習">把 Hoeffding 不等式連結到機器學習</h3>

<p>當我們現在有一個固定的 h(x)（假設集合 H 的其中一個，他有可能是 g），我們想要知道是否接近 f(x)，x 從 X 中取出，如果 h(x)  ̸= f(x)，就是 h 答錯，就像是罐子中的橘色彈珠，如果 h(x) = f(x)，就是 h 答對，就像是罐子中的綠色彈珠，如果取樣的數字很大的話，那我們就可以用部分的已知資料來大概推估 h(x) 在未知資料的表現情況。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-10.png">
</p>

<h3 id="把這個誤差推估的概念加進我們的機器學習圖表">把這個誤差推估的概念加進我們的機器學習圖表</h3>

<p>把這個誤差推估的概念加進我們的機器學習圖表，任何一個 h 都可以用已知的 Ein 來推估未知的 Eout，如果 N 夠大。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-11.png">
</p>

<h3 id="套進-hoeffding-不等式">套進 Hoeffding 不等式</h3>

<p>我們套進 Hoeffding 不等式，Ein 代表在已知取樣的中 h(x) 跟 f(x) 的誤差，Eout 代表其他未知的資料中 h(x) 跟 f(x) 的誤差，這在 Hoeffding 不等式的定理下，我們知道 N 很大時，Ein 與 Eout 的誤差很接近，所以我們可以說 Ein 與 Eout 是 PAC。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-12.png">
</p>

<h3 id="驗證-h-x-好不好">驗證 h(x) 好不好</h3>

<p>所以這常常會被機器學習用在驗證得出來的 h(x) 好不好，可以用這個圖來簡單表示，只要 N 夠大，取樣的分佈一致，那他的表現結果好與不好，是可以很明確地用部分已知資料來推估它在其他未知資料表現得好不好。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-13.png">
</p>

<h3 id="好死不死取到壞資料的情況">好死不死取到壞資料的情況</h3>

<p>我們還是會有好死不死取到壞資料的情況，但一樣可以用 Hoeffding 不等式說明這個機率很小，但是還是會發生。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-14.png">
</p>

<h3 id="如果有很多個-h">如果有很多個 h</h3>

<p>我們剛剛都是用一個 h 來推估是否機器學習是可能的，的確單一個 h 的時候，我們可以用取樣的資料來說明 h(x) 是否跟 f(x) 接近。那如果有很多個 h 呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-15.png">
</p>

<h3 id="一樣套進-hoeffding-不等式">一樣套進 Hoeffding 不等式</h3>

<p>如果是有限個 h，如 M 個 h，那一樣我們可以用取樣的資料來說明 h(x) 是否跟 f(x) 接近。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-16.png">
</p>

<h3 id="將這樣的概念加進去機器學習圖表">將這樣的概念加進去機器學習圖表</h3>

<p>如果 H 這個集合只有 M 個，然後取樣的 N 夠大，利用 A 取出 g，如果 Ein(g) 接近 0，那麼 g 就是一個 PAC 的答案了，我可由此可知機器學習是可能的。那如果 M 是無限大呢？這個就要下一講來解答了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-17.png">
</p>

<h3 id="總結">總結</h3>

<p>這一張說明了，我們無法直觀地說明得到的 h 是否能就是 f，因為天下沒有白吃的午餐。但我們用定理說明的 h 可以在未知的資料中 probably approximate correct。我們將定理連結到機器學習，就可以驗證 h。然後學習的過程也可以套進這樣的概念，當 H 中的 h 數量非無限的時候，機器學習是可能的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Learning is Impossible-4-18.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 3 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-09-12T11:32:31&#43;08:00">
        
  September 12, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/">第二講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<p>第三講的內容偏向介紹各種機器學習方法，以前念論文的時候看到這些名詞都會覺得高深莫測，但其實這各式各樣的機器學習方法其實都是從最基礎的核心變化而來，所以不要被嚇到。了解各種機器學習方法的輸入輸出對於日後面對一些問題的時候，我們才能夠知道要挑選什麼機器學習方法來解決問題。</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>我們先來回顧一下上一講的內容，在上一講我們知道了如何使用 PLA 讓機器學會回答是非題這樣的兩類問題（Binary Classificaction），套到機器學習的那句名句，我們可以清楚的了解，PLA 這個演算法 A 觀察了線性可分（linear separable）的 D 及感知假設集合 H 去得到一個最好的假設 g，這一句話就可以概括到上一講的內容了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-0.png">
</p>

<h3 id="從輸出-y-的角度看機器學習-y-只有兩個答案選一個-就叫-binary-classification">從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification</h3>

<p>接下來我們來了解一下各式各樣的學習方法，從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification，像是之前的是否發信用卡的例子就是 Binary Classification。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-1.png">
</p>

<h3 id="從輸出-y-的角度看機器學習-y-有多個答案選一個-就叫-multiclass-classification">從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification</h3>

<p>從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification，像是使用投飲機辨識錢幣的問題就是一個 Multiclass Classification 的問題，所以我們可以將分類問題推廣到分成 K 類，這樣 Binary Classificatin 就是一個 K=2 的分類問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-2.png">
</p>

<h3 id="從輸出-y-的角度看機器學習-y-為一個實數-就叫-regression">從輸出 y 的角度看機器學習，y 為一個實數，就叫 Regression</h3>

<p>從輸出 y 的角度看機器學習，y 為一個實數，就叫 Regression，像是要預估病人再過幾天病會好，這就需要用到 Regression，這也會用到許多統計學相關的工具。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-3.png">
</p>

<h3 id="從輸出-y-的角度看機器學習-y-為一個結構序列-就叫-structured-learning">從輸出 y 的角度看機器學習，y 為一個結構序列，就叫 Structured Learning</h3>

<p>從輸出 y 的角度看機器學習，y 為一個結構序列，就叫 Structured Learning，比如一個句子的詞性分析，會需要考慮到句子中的前後文，而句子的組合可能有無限多種，因此不能單純用 Multiclass Classification 來做到，這就需要用到 Structured Learning 相關的機器學習方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-4.png">
</p>

<h3 id="從輸出-y-的角度看機器學習-做個小結">從輸出 y 的角度看機器學習，做個小結</h3>

<p>從輸出 y 的角度看機器學習，如果 y 是兩類，那就是 Binary Classification；如果 y 是 k 類，那就是 Multiclass Classification；如果 y 是一個實數，那就是 Regression；如果 y 是一種結構關係，那就是 Structured Learning。當然還有其他變化，不過基礎上就是 Binary Classification 及 Regression，我們可以透過這兩個基礎核心來延伸出其他機器學習方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-5.png">
</p>

<h3 id="從輸入的資料-yn-的角度看機器學習-如果每個-xn-都有明確對應的-yn-這就叫監督式學習-supervised-learning">從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都有明確對應的 Yn，這就叫監督式學習（Supervised Learning）</h3>

<p>從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都有明確對應的 Yn，這就叫監督式學習（Supervised Learning），比如在訓練投飲機辨識錢幣的時候，我們很完整個告訴他什麼大小、什麼重量就是什麼幣值的錢幣，這樣就是一種監督式學習方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-6.png">
</p>

<h3 id="從輸入的資料-yn-的角度看機器學習-如果每個-xn-都沒有標示-yn-這就叫非監督式學習-unsupervised-learning">從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都沒有標示 Yn，這就叫非監督式學習（Unsupervised Learning）</h3>

<p>從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都沒有標示 Yn，這就叫非監督式學習（Unsupervised Learning）。比如在訓練投飲機辨識錢幣的時候，我們只告訴投飲機錢幣的大小及重量，但不告訴他什麼大小及重量個錢幣是哪個幣值的錢幣，讓機器自己去觀察特徵將這些錢幣分成一群一群，這又叫做分群，這就是一種非監督式學習方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-7.png">
</p>

<h3 id="從輸入的資料-yn-的角度看機器學習-如果-xn-只有部分有標示-yn-這就叫半監督式學習-semi-supervised-learning">從輸入的資料 Yn 的角度看機器學習，如果 Xn 只有部分有標示 Yn，這就叫半監督式學習（Semi-supervised Learning）</h3>

<p>從輸入的資料 Yn 的角度看機器學習，如果 Xn 只有部分有標示 Yn，這就叫半監督式學習（Semi-supervised Learning），有些資料較難取得的狀況下，我們會使用到半監度式學習，比如在預測藥物是否對病人有效時，由於做人體實驗成本高且可能要等一段時間來看藥效，這樣的情況下標示藥物有效或沒效的成本很高，所以就可能需要用到半監度式學習。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-8.png">
</p>

<h3 id="從輸入的資料-yn-的角度看機器學習-如果-yn-是很難確知描述的-只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯-這就叫增強式學習-reinforcement-learning">從輸入的資料 Yn 的角度看機器學習，如果 Yn 是很難確知描述的，只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯，這就叫增強式學習（Reinforcement Learning）</h3>

<p>從輸入的資料 Yn 的角度看機器學習，如果 Yn 是很難確知描述的，只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯，這就叫增強式學習（Reinforcement Learning）。這樣的機器學習方式，比較像自然界生物的學習方式，就像你要教一隻狗坐下，你很難直接告訴他怎麼做，而是用獎勵或處罰的方式讓狗狗漸漸知道坐下是什麼。增強式學習也就是這樣的機器學習方法，透過一次一次經驗的累積讓機器能夠學習到一個技能。比如像是教機器學習下棋，我們也可以透過勝負讓機器漸漸學習到如何下棋會下得更好。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-9.png">
</p>

<h3 id="從輸入的資料-yn-的角度看機器學習-做個小結">從輸入的資料 Yn 的角度看機器學習，做個小結</h3>

<p>從輸入的資料 Yn 的角度看機器學習，如果明確告知每個 Yn，那就是監督式學習；如果沒有告知任何 Yn，那就是非監督式學習；如果只有部份 Yn 的資料，那就是半監督式學習；如果是用獎勵、處罰的方式來告知 Yn，那就是增強式學習。當然還有其他種機器學習方法，其中最重要的核心就是監督式學習方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-10.png">
</p>

<h3 id="從餵資料給機器的角度看機器學習-一次餵進全部資料-這就叫-batch-learning">從餵資料給機器的角度看機器學習，一次餵進全部資料，這就叫 Batch Learning</h3>

<p>從餵資料給機器的角度看機器學習，一次餵進全部資料，這就叫 Batch Learning。監督式學習方法，可能也會常使用 Batch Learning 的方式為資料。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-11.png">
</p>

<h3 id="從餵資料給機器的角度看機器學習-可以再慢慢餵進新資料-這就叫-online-learning">從餵資料給機器的角度看機器學習，可以再慢慢餵進新資料，這就叫 Online Learning</h3>

<p>從餵資料給機器的角度看機器學習，可以再慢慢餵進新資料，這就叫 Online Learning。Batch Learging 訓練好的機器，就無法調整他的技巧，可能會有越來越不準的情況，所以 Online Learning 可以再慢慢調整、增進技巧。PLA 算法可以很容易應用在 Online Learning 上，增強式學習方法也常常是使用 Online Learning 的方式餵資料。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-12.png">
</p>

<h3 id="從餵資料給機器的角度看機器學習-機器可以問問題-然後從問題的答案再餵進資料-這就叫-active-learning">從餵資料給機器的角度看機器學習，機器可以問問題，然後從問題的答案再餵進資料，這就叫 Active Learning</h3>

<p>從餵資料給機器的角度看機器學習，機器可以問問題，然後從問題的答案再餵進資料，這就叫 Active Learning。這樣的學習方法是要希望讓機器可以用一些策略問問題，然後慢慢學習、改善技巧。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-13.png">
</p>

<h3 id="從餵資料給機器的角度看機器學習-做個小結">從餵資料給機器的角度看機器學習，做個小結</h3>

<p>從餵資料給機器的角度看機器學習，如果一次餵進所有資料，就叫 Batch Learning；如果後續可以再慢慢餵進資料，就叫 Online Learning；如果機器可以問問題來餵進資料，就叫 Active Learning。當然還有其他種機器學習方法，其中最重要的核心就是 Batch Learning。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-14.png">
</p>

<h3 id="從輸入-x-的角度看機器學習-如果-x-的特徵很明確定義-這就叫-concrete-feature">從輸入 X 的角度看機器學習，如果 X 的特徵很明確定義，這就叫 Concrete Feature</h3>

<p>從輸入 X 的角度看機器學習，如果 X 的特徵很明確定義，這就叫 Concrete Feature。Concrete Featrue 的取得通常需要人去介入，比如為何發不發信用卡要看申請者的年收入，這就是因為人們覺得年收入對於付不得付出卡費有關係。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-15.png">
</p>

<h3 id="從輸入-x-的角度看機器學習-如果-x-的特徵是用最基礎未人為整理過的-這就叫-raw-feature">從輸入 X 的角度看機器學習，如果 X 的特徵是用最基礎未人為整理過的，這就叫 Raw Feature</h3>

<p>從輸入 X 的角度看機器學習，如果 X 的特徵是用最基礎未人為整理過的，這就叫 Raw Feature。比如聲音訊號的頻率，圖片的像素，這都是 Raw Feature。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-16.png">
</p>

<h3 id="從輸入-x-的角度看機器學習-如果-x-的特徵是抽象的像是編號這樣的資料-這就叫-abstract-feature">從輸入 X 的角度看機器學習，如果 X 的特徵是抽象的像是編號這樣的資料，這就叫 Abstract Feature</h3>

<p>從輸入 X 的角度看機器學習，如果 X 的特徵是抽象的像是編號這樣的資料，這就叫 Abstract Feature。這通常就需要有人去抽取出更具象的特徵資料出來，這些特徵可能包含 Concrete Feature 或 Raw Feature。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-17.png">
</p>

<h3 id="從輸入-x-的角度看機器學習-做個小結">從輸入 X 的角度看機器學習，做個小結</h3>

<p>從輸入 X 的角度看機器學習，如果 X 是明確定義的，那就是 Concrete Feature；如果 X 是未經人為定義過的，那就是 Raw Feature；如果 X 是抽象的編號，那就是 Abstract Feature。當然還有其他種特徵，其中最重要的核心就是 Concrete Feature。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-18.png">
</p>

<h3 id="總結">總結</h3>

<p>從輸出 y 的角度看機器學習、從輸入的資料 Yn 的角度看機器學習、從餵資料給機器的角度看機器學習、從輸入 X 的角度看機器學習都會有許多不同的機器學習方法，但重要的是了解哪些是核心，其他機器學習方法也都是從這些核心發展而來。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-19.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 2 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-08-28T09:05:06&#43;08:00">
        
  August 28, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/">第一講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在前一章我們基本上可以了解機器學習的架構大致上就是 *A takes D and H to get g*，也就是說我們會使用演算法來基於資料與假設集合計算出一個符合資料呈現結果的方程式 g，在這邊我們就會看到 H 會長什麼樣子，然後介紹 Perceptron Learning Algorithm（PLA）來讓機器學習如何回答是非題，比如讓機器回答銀行是否要發信用卡給申請人這樣的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-0.png">
</p>

<h3 id="再看一次是否要發信用卡這個問題">再看一次是否要發信用卡這個問題</h3>

<p>是否要發信用卡這個問題我們可以想成它是一個方程式 f，而申請者的資料集合 X 丟進去就可以得到 Y 這些是否核發信用卡的記錄，我們現在不知道 f，將歷史資料 D 拿來當成訓練資料，其中每個 xi 就是申請者的資料，它會一個多維相向，比如第一個維度是年齡，第二個維度是性別&hellip;等等，然後我們會將這些資料 D 及假設集合 H 丟到機器學習演算法 Ａ，最後算出一個最像 f 的 g，這個 g 其實就是從假設集合 H 挑出一個最好的假設的結果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-1.png">
</p>

<h3 id="簡單的假設集合-感知器">簡單的假設集合：感知器</h3>

<p>要回答是否核發信用卡，可以用這樣簡單的想法來實現，現在我們知道申請者有很多基本資料，這些資料可以關係到是否核發信用卡，學術上就稱為是「特徵值」，這些特徵值有的重要、有的不重要，我們可以為這些特徵值依照重要性配上一個權重分數 wi，所以當這些分數加總起來之後，如果超過一個界線 threshold 時，我們就可以就可以決定核發信用卡，否則就不核發。這些 wi 及 threshold 就是所謂的假設集合，可以表示成如投影片中的線性方程式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-2.png">
</p>

<h3 id="將假設集合的線性方程式整理一下">將假設集合的線性方程式整理一下</h3>

<p>Threshold 我們也可以視為是一種加權分數，所以就可以將假設集合的線性方程式整理得更簡潔，整個線性方程式就變成了很簡易的兩個向量內積而已。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-3.png">
</p>

<h3 id="用二維空間來看看這個例子">用二維空間來看看這個例子</h3>

<p>假如現在申請者的只有兩個特徵值，那就可以用一個二維空間來標出每個申請者的位置，而是否核發信用卡，則用藍色圈圈來代表核發，紅色叉叉代表不核發，而假設 h 就是在這空間的一條線的法向量，可以將藍色圈圈跟紅色叉叉完美的分開來，這在機器學習上就是所謂的「分類」，Perceptrons 就是一種線性分類器。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-4.png">
</p>

<h3 id="怎麼從所有的假設中得到最好的假設-g">怎麼從所有的假設中得到最好的假設 g</h3>

<p>我們希望 g 可以跟 f 一樣完美的分類信用卡的核發與否，只要從 H 這個假設集合中挑到可以完美分類信用卡核發與否的線，我們就可以得到 g 了，但這很難，因為平面中可以有無限多條線，這樣算不完。所以我們改變想法，我們先隨便切一條線，然後如果有錯的地方，就修正這條線。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-5.png">
</p>

<h3 id="perceptron-learning-algorithm-感知學習模型">Perceptron Learning Algorithm 感知學習模型</h3>

<p>剛剛這樣有錯就去修正的想法，就是感知學習模型（Perceptron Learning Algorithm）的核心思想，實際上我們怎麼修正呢？我們來仔細看一下。假設現在有一個點 x 分錯了，它實際是核發的點，但卻被分在不核發的那一邊，這就代表 wt 向量與 x 之間的夾角太大，那就要讓它們之間的夾角變小，我們可以很簡單的用向量相加的方式來做到。如果現在 x 分錯了，它實際是不核發，那就代表 wt 向量與 x 向量之間的夾角太小，那就要讓他們之間的夾角變大，我們可以用 wt 向量減掉 x 向量來做到。這樣的計算很容易可以用程式做到。PLA 也是一個最簡易的神經網路算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-6.png">
</p>

<h3 id="看看-pla-演算法修正-h-的過程">看看 PLA 演算法修正 h 的過程</h3>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-8.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-9.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-10.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-11.png">
</p>

<h3 id="pla-的一些問題">PLA 的一些問題</h3>

<p>PLA 這個演算法會一直修正 h 直到對所有的 D 都沒有錯誤時，就會停止。但真實世界的資料不會這麼完美，PLA 可能會有不會停止的情況發生，所以我們只能修正 PLA，只算出夠好的 h 就可以了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-12.png">
</p>

<h3 id="什麼時候-pla-不會停止">什麼時候 PLA 不會停止</h3>

<p>什麼時候 PLA 會停止，什麼時候不會停止？當資料集合 D 為線性可分的時候，PLA 就會停止，但如果不是線性可分的時候就不會停止。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-13.png">
</p>

<h3 id="處理雜訊">處理雜訊</h3>

<p>其實真實世界的資料就是這樣，充滿了雜訊，這些雜訊也有可能本身就是錯誤的資料，比如銀行一開始核發就是錯的，這也就是為何我們只要得到一個接近 f 的 g 就可以了，而不一定要得到完美的 f。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-14.png">
</p>

<h3 id="找出犯最少錯的線">找出犯最少錯的線</h3>

<p>既然真實世界的資料有雜訊，那我們就用程式找出犯最少錯的線吧！說起來簡單，做起來很難，這個問題其實是個 NP-hard 的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-15.png">
</p>

<h3 id="pocket-algorithm">Pocket Algorithm</h3>

<p>所以折衷的方式就是找到夠好的線就好，我們修改一下 PLA，讓他每次計算時，如果得到更好的線，就先暫時存下來，然後算個幾百輪，我們就可以假設目前得到的線就是一個不錯的 h 了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-16.png">
</p>

<h3 id="演算法原始碼">演算法原始碼</h3>

<p>以上就是第二講的內容，這邊我找到了有人實作這兩個演算法的<a href="http://wizmann.tk/ml-foundations-pla.html">原始碼</a>，讓大家可以參考一下。</p>

<h4 id="naive-pla">Naive PLA</h4>

<pre><code class="language-python">from numpy import *

def naive_pla(datas):
    w = datas[0][0]
    iteration = 0
    while True:
        iteration += 1
        false_data = 0

        for data in datas:
            t = dot(w, data[0])
            if sign(data[1]) != sign(t):
                error = data[1]
                false_data += 1
                w += error * data[0]
        print 'iter%d (%d / %d)' % (iteration, false_data, len(datas))
        if not false_data:
            break
    return w
</code></pre>

<h4 id="pocket-pla">Pocket PLA</h4>

<pre><code class="language-python">import numpy as np

def pocket_pla(datas, limit):
    ###############
    def _calc_false(vec):
        res = 0
        for data in datas:
            t = np.dot(vec, data[0])
            if np.sign(data[1]) != np.sign(t):
                res += 1
        return res
    ###############
    w = np.random.rand(5)
    least_false = _calc_false(w)
    res = w

    for i in xrange(limit):
        data = random.choice(datas)
        t = np.dot(w, data[0])
        if np.sign(data[1]) != np.sign(t):
            t = w + data[1] * data[0]
            t_false = _calc_false(t)

            w = t

            if t_false &lt;= least_false:
                least_false = t_false
                res = t
    return res, least_false
</code></pre>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 1 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-08-17T13:04:46&#43;08:00">
        
  August 17, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>機器學習（Machine Learning）是一門很深的課程，要直接跳進來學習其實並不容易，因此系統性由淺而深的學習過程還是必須的。這一系列部落格文章我將分享我在 Coursera 上臺灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明，希望對有心學習 Machine Learning 的碼農們有些幫助。</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="如何有效學習機器學習">如何有效學習機器學習</h3>

<p>從基礎來由淺入深，包含理論及實作技術用說故事的方式包裝，比如何時可以使用機器學習、為何機器可以學習、機器怎麼學習、如何讓機器學得更好，讓我們可以記得並加以應用。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-1.png?1">
</p>

<h3 id="從人的學習轉換到機器學習">從人的學習轉換到機器學習</h3>

<p>人學習是為了習得一種技能，比如學習辨認男生或女生，而我們可以從觀察中累積經驗而學會辨認男生或女生，這就是人學習的過程，觀察 -&gt; 累積經驗、學習 -&gt; 習得技能；而機器怎麼學習呢？其實有點相似，機器為了學習一種技能，比如一樣是學習辨認男生或女生，電腦可以從<strong>觀察資料</strong>及<strong>計算</strong>累積<strong>模型</strong>而學會<strong>辨認</strong>男生或女生，這就是機器學習的過程，資料 -&gt; 計算、學習出模型 -&gt; 習得技能。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-2.png">
</p>

<h3 id="再定義一下什麼是技能">再定義一下什麼是技能</h3>

<p>「師爺，翻譯翻譯什麼是他媽的技能」「技能不就是技能嗎」在機器學習上，技能就是透過計算所搜集到的資料來提升一些可量測的性能，比如預測得更準確，實例上像是我們可以搜集股票的交易資料，然後透過機器學習的計算及預測後，是否可以得到更多的投資報酬。如果可以增加預測的準確度，那麼我們就可以說電腦透過機器學習得到了預測股票買賣的技能了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-3.png">
</p>

<h3 id="舉個例子">舉個例子</h3>

<p>各位勞苦功高的碼農們，現在老闆心血來潮要你寫一個可以辨識樹的圖片的程式，你會怎麼寫呢？你可能寫一個程式檢查圖片中有沒有綠綠的或是有沒有像葉子的形狀的部份等等，然後寫了幾百條規則來完成辨識樹的圖片的功能，現在功能上線了，好死不死現在來了一張樹的圖片上面剛好都沒有葉子，你寫的幾百條規則都沒用了，辨識樹的圖片的功能只能以失敗收場。機器學習可以解決這樣的問題，透過觀察資料的方式來讓電腦自己辨識樹的圖片，可能會比寫幾百條判斷規則更有效。這有點像是教電腦學會釣魚（透過觀察資料學習），而不是直接給電腦魚吃（直接寫規則給電腦）。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-4.png">
</p>

<h3 id="那麼什麼時候可以使用機器學習呢">那麼什麼時候可以使用機器學習呢</h3>

<p>從上個例子我們可以大概了解使用機器學習的使用時機，大致上如果觀察到現在你想要解決的問題有以下三個現象，應該就是機器學習上場的時刻了：</p>

<ol>
<li>存在某種潛在規則</li>
<li>但沒有很辦法很簡單地用程式直接定義來作邏輯判斷（if else 就可以做到，就不用機器學習）</li>
<li>這些潛在規則有很多資料可以作為觀察、學習的來源</li>
</ol>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-5.png">
</p>

<h3 id="舉個實際的機器學習例子-1">舉個實際的機器學習例子 1</h3>

<p>Netflix 現在出了一個問題，如果你能讓使用者對電影喜好程度星級預測準確率提升 10%，那就可以獲得 100 萬美金，馬上讓你從碼農無產階級晉升到天龍人資產階級，而這個問題是這樣的：他們給了你大量使用者對一些電影的星級評分資料，你必須要讓電腦學到一個技能，這個技能可以預測到使用者對他還沒看過的電影評分會是多少星級，如果電腦能準確預測的話，那某種程度它就有了可以知道使用者會不會喜歡這些電影的技能，進而可以推薦使用者他們會喜歡的電影，讓他們從口袋裡拿錢過來～</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-6.png">
</p>

<h3 id="舉個實際的機器學習例子-2">舉個實際的機器學習例子 2</h3>

<p>這邊偷偷告訴大家一個很常見的機器學習方法的模型，我們再來整理一下，其實這個問題可以轉化成這樣，使用者有很多個會喜歡這部電影的因素，比如電影中有沒有爆破場景、有沒有養眼畫面、有沒有外星人等等，這個我們就稱之為使用者的特徵值（feature），而電影本身也有很多因素，比如電影中有出現炸彈、是很有魅力的史嘉蕾·喬韓森所主演、片名是 ET 第二集等等，這個我們就稱之為電影的特徵值，我們把這兩個特徵值表示成向量（vector），如此如果使用者與電影特徵值有對應的特徵越多，那就代表使用者很有可能喜歡這部電影，而這可以很快地用向量內積的方式計算出來。也就是說，機器學習在這個問題上，只要能學習出這些會影響使用者喜好的<strong>因素</strong>也就是機器學習所說的<strong>特徵值</strong>會是什麼，那這樣當一部新的電影出來，我們只要叫電腦對一下這部新電影與使用者的特徵值的對應起來的向量內積值高不高就可以知道使用者會不會喜歡這部電影了～</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-7.png">
</p>

<h3 id="將剛剛的問題用數學式來描述">將剛剛的問題用數學式來描述</h3>

<p>我們在用銀行核發信用卡的例子來描述機器學習，我們可以把信用卡申請者的資料想成是 x，而 y 是銀行是否核發信用卡。所以這就是一個函式，它有一個潛在規則，可以讓 x 對應到 y，機器學習就是要算出這個 f 函式是什麼。現在我們有大量的信用卡對申請者核發信用卡的資料，就是 D，我們可以從資料觀察中得到一些假設，然後讓電腦去學習這些假設是對的還是錯的，慢慢習得技能，最後電腦可能會算出一個 g 函式，雖然不是完全跟 f 一樣，但跟 f 很像，所以能夠做出還蠻精確的預測。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-8.png">
</p>

<h3 id="機器學習流程">機器學習流程</h3>

<p>所以機器學習銀行是否核發信用卡的流程就像這樣，我們想要找到 target function，可以完整預測銀行對申請者是否要核發信用卡才會賺錢，這時我們會餵給電腦大量的資料，然後透過學習演算法找出重要的特徵值，這些重要的特徵值就可以組成函式 g，雖然跟 f 不一樣，但很接近。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-9.png">
</p>

<h3 id="機器學習的-model">機器學習的 Model</h3>

<p>從上面的學習流程，我們可以知道最後電腦會學習出 g 可以辨認資料中較重要的特徵值，這些特徵值可能是我們一開始觀察資料所整理出來的假設，所以我們餵資料給電腦做學習演算法做計算時，也會餵一些假設進去，以銀行核發信用卡的例子就是這個申請者年薪是否有 80 萬、負債是否有 10 萬、工作是否小於兩年等等假設，這些假設就是 H，學習演算法再去計算實際資料與假設是否吻合，這個演算法就是 A，最後演算法會挑出最好的假設集合是哪些。 H 與 A 我們就稱為是機器學習 Model</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-10.png">
</p>

<h3 id="機器學習的基本定義">機器學習的基本定義</h3>

<p>機器學習的基本定義可以用這個圖來概括，用一句話來說的話就是「use data to compute hypothesis g that approximates target f」，你如果問我為何要用英文寫下這句話，其實只是因為這樣看起來比較像是一個偉人大學者寫的啊！拿來唬人用的！</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-11.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-jieba-%E7%B5%90%E5%B7%B4%E4%B8%AD%E6%96%87%E5%88%86%E8%A9%9E%E7%A8%8B%E5%BC%8F/">
          如何使用 jieba 結巴中文分詞程式
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2014-08-06T13:49:55&#43;08:00">
        
  August 6, 2014

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>自然語言處理的其中一個重要環節就是中文斷詞的處理，比起英文斷詞，中文斷詞在先天上就比較難處理，比如電腦要怎麼知道「全台大停電」要斷詞成「全台 / 大 / 停電」呢？如果是英文「Power outage all over Taiwan」，就可以直接用空白斷成「Power / outage / all / over /  Taiwan」，可見中文斷詞真的是一個大問題啊～</p>

<p>這樣的問題其實已經有很多解法，比如中研院也有提供「<a href="http://ckipsvr.iis.sinica.edu.tw/">中文斷詞系統</a>」，但就是很難用，不僅 API Call 的次數有限制，還很難串，Server 也常常掛掉，真不曉得為何中研院不將核心開源出來，讓大家可以一起來改善這種現象，總之我要棄中研院的斷詞系統而去了。</p>

<p>近來玩了一下 <a href="https://github.com/fxsjy/jieba">jieba</a> 結巴這個 Python Based 的開源中文斷詞程式，感覺大好，順手發了一些 pull request，今天早上就成為 <a href="http://bit.ly/1pWOuzp">contributor</a> 了！ 感覺真爽！每次發 pull request 總是有種莫名的爽感，既期待被 merge 又怕被 reject，就跟告白的感覺類似啊～</p>

<p>這麼好用的開源中文斷詞系統，當然要介紹給大家用啊！</p>

<h3 id="背後演算法">背後演算法</h3>

<p>jieba 中文斷詞所使用的演算法是基於 Trie Tree 結構去生成句子中中文字所有可能成詞的情況，然後使用動態規劃（Dynamic programming）算法來找出最大機率的路徑，這個路徑就是基於詞頻的最大斷詞結果。對於辨識新詞（字典詞庫中不存在的詞）則使用了 HMM 模型（Hidden Markov Model）及 Viterbi 算法來辨識出來。基本上這樣就可以完成具有斷詞功能的程式了，或許我之後可以找個時間寫幾篇部落格來介紹這幾個演算法。</p>

<h3 id="如何安裝">如何安裝</h3>

<p>推薦用 pip 安裝 jieba 套件，或者使用 Virtualenv 安裝（未來可能會介紹如何使用 Virtualevn，這樣就可以同時在一台機器上跑不同的 Python 環境）：</p>

<pre><code>pip install jieba
</code></pre>

<h3 id="基本斷詞用法-使用預設詞庫">基本斷詞用法，使用預設詞庫</h3>

<p>Sample Code：</p>

<p>jieba-default-mode.py</p>

<pre><code>#encoding=utf-8
import jieba

sentence = &quot;獨立音樂需要大家一起來推廣，歡迎加入我們的行列！&quot;
print &quot;Input：&quot;, sentence
words = jieba.cut(sentence, cut_all=False)
print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word

sentence = &quot;独立音乐需要大家一起来推广，欢迎加入我们的行列！&quot;
print &quot;Input：&quot;, sentence
words = jieba.cut(sentence, cut_all=False)
print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word
</code></pre>

<p>得到的斷詞結果會是：</p>

<pre><code>獨立 / 音樂 / 需要 / 大家 / 一起 / 來 / 推廣 / ， / 歡迎 / 加入 / 我們 / 的 / 行列

独立 / 音乐 / 需要 / 大家 / 一 / 起来 / 推广 / ， / 欢迎 / 加入 / 我们 / 的 / 行列
</code></pre>

<p>據原作者的說法，使用預設詞庫的話，繁體中文的斷詞結果應該會比較差，畢竟原來的詞庫是簡體中文，但在這個例子中，我感覺是繁體中文的斷詞結果比較好，這應該只是特例，我們接下來試試看中文歌詞的斷詞結果如何。</p>

<h3 id="中文歌詞斷詞-使用預設詞庫">中文歌詞斷詞，使用預設詞庫</h3>

<p>現在我們使用 <a href="http://www.indievox.com/song/1">回聲樂團 - 座右銘</a> 的歌詞作為中文斷詞測試範例，歌詞我們先做成一個純文字檔，內容如下：</p>

<p>lyric.txt</p>

<pre><code>我沒有心
我沒有真實的自我
我只有消瘦的臉孔
所謂軟弱
所謂的順從一向是我
的座右銘

而我
沒有那海洋的寬闊
我只要熱情的撫摸
所謂空洞
所謂不安全感是我
的墓誌銘

而你
是否和我一般怯懦
是否和我一般矯作
和我一般囉唆

而你
是否和我一般退縮
是否和我一般肌迫
一般地困惑

我沒有力
我沒有滿腔的熱火
我只有滿肚的如果
所謂勇氣
所謂的認同感是我
隨便說說

而你
是否和我一般怯懦
是否和我一般矯作
是否對你來說
只是一場遊戲
雖然沒有把握

而你
是否和我一般退縮
是否和我一般肌迫
是否對你來說
只是逼不得已
雖然沒有藉口
</code></pre>

<p>Sample Code：</p>

<p>jieba_cut_lyric.py</p>

<pre><code>#encoding=utf-8
import jieba

content = open('lyric.txt', 'rb').read()

print &quot;Input：&quot;, content

words = jieba.cut(content, cut_all=False)

print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word
</code></pre>

<p>得到的斷詞結果會是：</p>

<pre><code>我 / 沒 / 有心 / 我 / 沒 / 有 / 真實 / 的 / 自我 / 我 / 只有 / 消瘦 / 的 / 臉孔 / 所謂 / 軟弱 / 所謂 / 的 / 順 / 從 / 一向 / 是 / 我 / 的 / 座 / 右銘 / 而 / 我 / 沒有 / 那 / 海洋 / 的 / 寬闊 / 我 / 只要 / 熱情 / 的 / 撫 / 摸 / 所謂 / 空洞 / / 所謂 / 不安全感 / 是 / 我 / 的 / 墓誌 / 銘 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 和 / 我 / 一般 / 囉 / 唆 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 一般 / 地 / 困惑 / 我 / 沒 / 有力 / 我 / 沒 / 有 / 滿腔 / 的 / 熱火 / 我 / 只有 / 滿肚 / 的 / 如果 / 所謂 / 勇氣 / 所謂 / 的 / 認 / 同感 / 是 / 我 / 隨便 / 說 / 說 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 是否 / 對 / 你 / 來 / 說 / 只是 / 一場 / 遊戲 / 雖然 / 沒 / 有把握 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 是否 / 對 / 你 / 來 / 說 / 只是 / 逼不得已 / 雖然 / 沒有 / 藉口
</code></pre>

<p>我們可以從結果看出斷詞已經開始出了一些問題，比如<strong>「座右銘」</strong>被斷成了<strong>「座 / 右銘」</strong>，<strong>「墓誌銘」</strong>被斷成了<strong>「墓誌 / 銘」</strong>，這應該就是因為預設詞庫是簡體中文所造成，因此繁體中文的斷詞結果會比較差，還好 jieba 也提供了可以切換詞庫的功能，並提供了一個繁體中文詞庫，所以我們可以使用切換詞庫的功能來改善斷詞結果。</p>

<h3 id="中文歌詞斷詞-使用繁體詞庫">中文歌詞斷詞，使用繁體詞庫</h3>

<p>Sample Code：</p>

<p>jieba_cut_lyric_zh.py</p>

<pre><code>#encoding=utf-8
import jieba

jieba.set_dictionary('dict.txt.big')

content = open('lyric.txt', 'rb').read()

print &quot;Input：&quot;, content

words = jieba.cut(content, cut_all=False)

print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word
</code></pre>

<p>我們在程式中多加一行 <code>jieba.set_dictionary(&lsquo;dict.txt.big&rsquo;)</code>，這樣就可以將斷詞詞庫切換到 dic.txt.big 這個檔案。</p>

<p>得到的斷詞結果會是：</p>

<pre><code>我 / 沒有 / 心 / 我 / 沒有 / 真實 / 的 / 自我 / 我 / 只有 / 消瘦 / 的 / 臉孔 / 所謂 / 軟弱 / 所謂 / 的 / 順從 / 一向 / 是 / 我 / 的 / 座右銘 / 而 / 我 / 沒有 / 那 / 海洋 / 的 / 寬闊 / 我 / 只要 / 熱情 / 的 / 撫摸 / 所謂 / 空洞 / 所謂 / 不安全感 / 是 / 我 / 的 / 墓誌銘 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 和 / 我 / 一般 / 囉唆 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 一般 / 地 / 困惑 / 我 / 沒有 / 力 / 我 / 沒有 / 滿腔 / 的 / 熱火 / 我 / 只有 / 滿肚 / 的 / 如果 / 所謂 / 勇氣 / 所謂 / 的 / 認同感 / 是 / 我 / 隨便說說 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 是否 / 對 / 你 / 來說 / 只是 / 一場 / 遊戲 / 雖然 / 沒有 / 把握 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 是否 / 對 / 你 / 來說 / 只是 / 逼不得已 / 雖然 / 沒有 / 藉口
</code></pre>

<p>我們可以看到<strong>「座右銘」</strong>成功斷成<strong>「座右銘」</strong>了！<strong>「墓誌銘」</strong>也成功斷成<strong>「墓誌銘」</strong>了！果然切換成繁體中文詞庫還是有用的！</p>

<h3 id="台語歌詞斷詞-使用繁體詞庫">台語歌詞斷詞，使用繁體詞庫</h3>

<p>既然中文歌詞斷詞能夠得到不錯的斷詞結果了，那我們來試試看台語歌詞斷詞會是如何？在這邊我們使用 <a href="http://www.indievox.com/song/73716">滅火器 - 島嶼天光</a> 的歌詞作為台語斷詞測試範例，歌詞我們先做成一個純文字檔，內容如下：</p>

<p>lyric_tw.txt</p>

<pre><code>親愛的媽媽
請你毋通煩惱我
原諒我
行袂開跤
我欲去對抗袂當原諒的人

歹勢啦
愛人啊
袂當陪你去看電影
原諒我
行袂開跤
我欲去對抗欺負咱的人

天色漸漸光
遮有一陣人
為了守護咱的夢
成做更加勇敢的人

天色漸漸光
已經不再驚惶
現在就是彼一工
換阮做守護恁的人

已經袂記
是第幾工
請毋通煩惱我
因為阮知道
無行過寒冬
袂有花開的一工

天色漸漸光
天色漸漸光
已經是更加勇敢的人

天色漸漸光
咱就大聲來唱著歌
一直到希望的光線
照光島嶼每一個人

天色漸漸光
咱就大聲來唱著歌
日頭一爬上山
就會使轉去啦
現在是彼一工
勇敢的台灣人
</code></pre>

<p>Sample Code：</p>

<p>jieba_cut_lyric_zh_tw.py</p>

<pre><code>#encoding=utf-8
import jieba

jieba.set_dictionary('dict.txt.big')

content = open('lyric_tw.txt', 'rb').read()

print &quot;Input：&quot;, content

words = jieba.cut(content, cut_all=False)

print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word
</code></pre>

<p>得到的斷詞結果會是：</p>

<pre><code>親愛 / 的 / 媽媽 / 請 / 你 / 毋通 / 煩惱 / 我 / 原諒 / 我 / 行袂 / 開跤 / 我 / 欲 / 去 / 對抗 / 袂 / 當 / 原諒 / 的 / 人 / 歹勢 / 啦 / 愛人 / 啊 / 袂 / 當 / 陪你去 / 看 / 電影 / 原諒 / 我 / 行袂 / 開跤 / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / 天色 / 漸漸 / 光 / 遮有 / 一陣 / 人 / 為 / 了 / 守護 / 咱 / 的 / 夢 / 成 / 做 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 已經 / 不再 / 驚惶 / 現在 / 就是 / 彼一工 / 換阮 / 做 / 守護 / 恁 / 的 / 人 / 已經 / 袂 / 記 / 是 / 第幾 / 工 / 請 / 毋通 / 煩惱 / 我 / 因為 / 阮 / 知道 / 無行過 / 寒冬 / 袂 / 有 / 花開 / 的 / 一工 / 天色 / 漸漸 / 光 / 天色 / 漸漸 / 光 / 已經 / 是 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / 一直 / 到 / 希望 / 的 / 光線 / 照光 / 島嶼 / 每 / 一個 / 人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / 日頭 / 一爬 / 上山 / 就 / 會 / 使 / 轉去 / 啦 / 現在 / 是 / 彼 / 一工 / 勇敢 / 的 / 台灣 / 人
</code></pre>

<p>原本猜想結果應該會蠻差的，畢竟詞庫中沒有台語的用詞，但是因為 HMM 的關係猜出了一些新詞，讓我們還是得到不錯的結果，<strong>「袂當」</strong>斷成了<strong>「袂」「當」</strong>，<strong>「袂記」</strong>斷成了<strong>「袂」「記」</strong>，<strong>「袂有」</strong>斷成了<strong>「袂」「有」</strong>等等，我們要如何改善這些結果呢？</p>

<p>jieba 提供了一個功能讓使用者可以增加自定義詞庫，這種無法用 HMM 判斷出來的新詞就可以得到改善，我們就來試試看吧！</p>

<h3 id="台語歌詞斷詞-使用繁體詞庫加自定義詞庫">台語歌詞斷詞，使用繁體詞庫加自定義詞庫</h3>

<p>首先我們新增一個純文字檔建立自定義詞庫，格式如下：</p>

<p>userdict.txt</p>

<pre><code>行袂開跤 2 v
袂當 4 d
袂記 4 v
袂有 4 d
唱著 4 v
每一個 4 m
會使 70 d
</code></pre>

<p>其中每一行代表一筆語料資料，首先填上自定義詞如：<strong>「袂當」</strong>、<strong>「袂記」</strong>，然後填上權重，權重值可以依照斷詞結果做自己想做的調整，最後填上詞性，但詞性非必要填寫，詞性列表可以參考 <a href="https://github.com/ansjsun/ansj_seg/wiki/%E8%AF%8D%E6%80%A7%E5%AF%B9%E7%85%A7%E8%AF%B4%E6%98%8E.%E4%B8%AD%E7%A7%91%E9%99%A2%E7%89%88%E6%9C%AC">词性对照说明.中科院版本</a>。</p>

<p>Sample Code：</p>

<p>jieba_cut_lyric_zh_tw_custom.py</p>

<pre><code>#encoding=utf-8
import jieba

jieba.set_dictionary('dict.txt.big')
jieba.load_userdict(&quot;userdict.txt&quot;)

content = open('lyric_tw.txt', 'rb').read()

print &quot;Input：&quot;, content

words = jieba.cut(content, cut_all=False)

print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word
</code></pre>

<p>我們在程式中多加一行 <code>jieba.load_userdict(&ldquo;userdict.txt&rdquo;)</code>，這樣就可以將自定義詞庫加進來了，超級簡單的。</p>

<p>得到的斷詞結果會是：</p>

<pre><code>親愛 / 的 / 媽媽 / 請 / 你 / 毋通 / 煩惱 / 我 / 原諒 / 我 / 行袂開跤 / 我 / 欲 / 去 / 對抗 / 袂當 / 原諒 / 的 / 人 / 歹勢 / 啦 / 愛人 / 啊 / 袂當 / 陪你去 / 看 / 電影 / 原諒 / 我 / 行袂開跤 / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / 天色 / 漸漸 / 光 / 遮有 / 一陣 / 人 / 為 / 了 / 守護 / 咱 / 的 / 夢 / 成 / 做 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 已經 / 不再 / 驚惶 / 現在 / 就是 / 彼一工 / 換阮 / 做 / 守護 / 恁 / 的 / 人 / 已經 / 袂記 / 是 / 第幾 / 工 / 請 / 毋通 / 煩惱 / 我 / 因為 / 阮 / 知道 / 無行過 / 寒冬 / 袂有 / 花開 / 的 / 一工 / 天色 / 漸漸 / 光 / 天色 / 漸漸 / 光 / 已經 / 是 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱著 / 歌 / 一直 / 到 / 希望 / 的 / 光線 / 照光 / 島嶼 / 每 / 一個 / 人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱著 / 歌 / 日頭 / 一爬 / 上山 / 就 / 會使 / 轉去 / 啦 / 現在 / 是 / 彼 / 一工 / 勇敢 / 的 / 台灣 / 人
</code></pre>

<p>完美！</p>

<h3 id="取出斷詞詞性">取出斷詞詞性</h3>

<p>大部份的斷詞系統都可以列出斷詞的詞性，jieba 也有這個功能，但結果可能不是那麼好，這其實是跟所使用的語料庫有關係，不過既然是 Open Source，希望未來能有語言學家可以加入，讓 jieba 可以得到更好的效果。</p>

<p>Sample Code：</p>

<p>jieba_cut_lyric_zh_flag.py</p>

<pre><code>#encoding=utf-8
import jieba
import jieba.posseg as pseg

jieba.set_dictionary('dict.txt.big')

content = open('lyric.txt', 'rb').read()

print &quot;Input：&quot;, content

words = pseg.cut(content)

print &quot;Output 精確模式 Full Mode：&quot;
for word in words:
    print word.word, word.flag
</code></pre>

<p>得到的結果會是：</p>

<pre><code>我 r
沒有 x
心 n

我 r
沒有 x
真實 x
的 uj
自我 r

...
</code></pre>

<h3 id="取出斷詞位置">取出斷詞位置</h3>

<p>有時我們會需要得到斷詞在文章中的位置：</p>

<p>Sample Code：</p>

<p>jieba_cut_lyric_zh_tokenize.py</p>

<pre><code>#encoding=utf-8
import jieba

jieba.set_dictionary('dict.txt.big')

content = open('lyric.txt', 'rb').read()

print &quot;Input：&quot;, content

words = jieba.tokenize(unicode(content, 'utf-8'))

print &quot;Output 精確模式 Full Mode：&quot;
for tk in words:
    print &quot;word %s\t\t start: %d \t\t end:%d&quot; % (tk[0],tk[1],tk[2])
</code></pre>

<p>得到的結果會是：</p>

<pre><code>word 我 start: 0 end:1
word 沒有 start: 1 end:3
word 心 start: 3 end:4
word start: 4 end:5
word 我 start: 5 end:6
word 沒有 start: 6 end:8
word 真實 start: 8 end:10
word 的 start: 10 end:11
word 自我 start: 11 end:13

...
</code></pre>

<h3 id="取出文章中的關鍵詞">取出文章中的關鍵詞</h3>

<p>jieba 使用了 tf-idf 方法來實作萃取出文章中關鍵詞的功能：</p>

<p>Sample Code：</p>

<p>jieba_cut_lyric_zh_keyword.py</p>

<pre><code>#encoding=utf-8
import jieba
import jieba.analyse

jieba.set_dictionary('dict.txt.big')

content = open('lyric.txt', 'rb').read()

print &quot;Input：&quot;, content

tags = jieba.analyse.extract_tags(content, 10)

print &quot;Output：&quot;
print &quot;,&quot;.join(tags)
</code></pre>

<p>程式中的 <code>jieba.analyse.extract_tags(content, 10)</code>，就是告訴 jieba 我們要從這個文章中取出前 10 個 tf-idf 值最大的關鍵詞。</p>

<p>得到的結果會是：</p>

<pre><code>沒有,所謂,是否,一般,雖然,退縮,肌迫,矯作,來說,怯懦
</code></pre>

<p>一開始使用這個功能的時候，會不知道 jieba 的 idf 值是從哪裡來的，看了一下 souce code 才知道原來 jieba 有提供一個 idf 的語料庫，但在實務上每個人所使用的語料庫可能會不太一樣，有時我們會想要使用自己的idf 語料庫，stop words 的語料庫也可能會想換成自己的，比如目前的結果中，最重要的「座右銘」並沒有出現在關鍵詞裡，我就會想要將「座右銘」加到 idf 語料庫，並讓 idf 值高一點，而「沒有」這個關鍵詞對我來說是沒有用的，我就會想把它加到 stop words 語料庫，這樣「沒有」就不會出現在關鍵詞裡。</p>

<p>可惜目前 pip 安裝的 jieba 版本並不能切換 idf 及 stop words 語料庫，所以我才會修改了一下 jieba，讓它可以支援 idf 及 stop words 語料庫的切換，目前在 github 上的版本已經可以支援 idf 及 stop words 切換的功能了！</p>

<h3 id="結語">結語</h3>

<p>使用了 jieba 之後，其實有蠻深的感嘆，其實中研院的斷詞核心必非不好，想要收費也不是問題，但是 API 做得這麼差，根本就沒人有信心敢花錢下去使用這樣不可靠的系統，目前又有 jieba 這樣的 open source project，中研院的斷詞系統前途堪慮啊！</p>

<h3 id="後記-2017-07-18">後記 2017/07/18</h3>

<p>我後來有好幾次受邀演講關於中文斷詞的講題，相關的投影片都有分享在網路上了，大家也可以交叉參考：中文斷詞：斷句不要悲劇 / Head first Chinese text segmentation <a href="https://speakerdeck.com/fukuball/head-first-chinese-text-segmentation">投影片</a>、<a href="https://bit.ly/chinese-seg">範例程式碼</a>。</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-jieba-%E7%B5%90%E5%B7%B4%E4%B8%AD%E6%96%87%E5%88%86%E8%A9%9E%E7%A8%8B%E5%BC%8F/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
            
  <div class="pagination-bar">
    <ul class="pagination">
      
        
          <li class="pagination-prev">
            <a class="btn btn--default btn--small" href="https://blog.fukuball.com/tags/machine-learning/page/4/">
              <i class="fa fa-angle-left text-base icon-mr"></i>
              <span></span>
            </a>
          </li>
        
        
      
      <li class="pagination-number"> </li>
    </ul>
  </div>


          </section>
        
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Fukuball. 
  </span>
</footer>

      </div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
    
    <h4 id="about-card-name">Fukuball</h4>
    
      <div id="about-card-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Co-Founder / Head of Engineering at OurSong
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Taipei, Taiwan
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center"></div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-23smart-contract-%E5%88%9D%E6%8E%A2%E5%BE%9E-bytecode-%E5%88%B0-solidity/">
                <h3 class="media-heading">Ethereum 開發筆記 2–3：Smart Contract 初探，從 Bytecode 到 Solidity</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Ethereum 上的 EVM（Ethereum Virtual Machine）可以執行程式，而 EVM 上的可執行程式基本上是 Bytecode 的形式，所以所謂的 Smart Contract 就是存放在 Ethereum 上的 Bytecode，然後可由 EVM 來執行。
Bytecode Smart Contract 直接用 Bytecode 寫 Smart Contract 我們來嘗試一下直接用 Bytecode 來寫 Smart Contract，以下這段程式碼主要內容是執行運算後，將運算結果存放在 0 這個位置：
PUSH1 0x03 PUSH1 0x05 ADD // 3 + 5 -&gt; 8 PUSH1 0x02 MUL // 8 * 2 -&gt; 16 PUSH1 0x00 SSTORE // 將 16 存到 0 這個位置  這段程式轉成 Bytecode 就是：
0x60 0x03 0x60 0x05 0x01 0x60 0x02 0x02 0x60 0x00 0x55  也就是：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-22geth-%E5%9F%BA%E7%A4%8E%E7%94%A8%E6%B3%95%E5%8F%8A%E6%9E%B6%E8%A8%AD-muti-nodes-%E7%A7%81%E6%9C%89%E9%8F%88/">
                <h3 class="media-heading">Ethereum 開發筆記 2–2：Geth 基礎用法及架設 Muti-Nodes 私有鏈</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">要連上 Ethereum 就需要安裝 Ethereum Node，在這邊我們選擇使用 Geth 來安裝 Ethereum Node，接下來就來一步一步的學學怎麼使用 Geth，甚至如何使用 Geth 來架設自己的 Ethereum 私有鏈。
安裝環境 首先我們在 AWS 上開啟兩台 Ubuntu 虛擬機器，記得開 t2.medium（2 vCPU, 4 GB RAM）這個規格以上才跑得動，硬碟可以開 100 G，Security Group 將 TCP 30303 打開，Ethereum Node 之間是用 30303 這個 port 來溝通的。
接下來使用以下指令安裝 Geth：
$ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:ethereum/ethereum $ sudo apt-get update $ sudo apt-get install -y ethereum  兩台虛擬機器都要安裝，應該幾分鐘就可以裝好了。
使用 Main Net 安裝完 Geth 之後，我們就可以透過 Geth 連上 Ethereum Network 了，我們就來連上 Main Net 看看：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-21ethereum-%E9%96%8B%E7%99%BC%E6%95%B4%E9%AB%94%E8%84%88%E7%B5%A1/">
                <h3 class="media-heading">Ethereum 開發筆記 2–1：Ethereum 開發整體脈絡</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在第一次接觸 Ethereum 應用程式開發時，會發現有各式各樣工具，不知要從何下手，我們用一個圖來說明一下與 Ethereum 互動時的整體脈絡及這之間的工具主要做了什麼事，了解之後自己就可以挑選開發時、甚至使用在產品上時要用什麼適合的工具了。
要在自己的機器接上 Ethereum 首先需要安裝 Ethereum Node，我們之前安裝的 Mist 其實就會在我們的機器上安裝 Ethereum Node 並同步帳本，而像這樣安裝 Node 並同步帳本甚至進行挖礦的軟體有很多，大家可以去選擇適合自己使用的。Mist 其實是將一個叫 geth 的軟體用 GUI 包裝起來，如果是開發者的話，可以選擇直接安裝 geth。
geth 提供了許多 API 指令可以讓我們跟 Ethereum 做互動，但有時下指令並不是那麼親和，所以 geth 提供了 RPC(Remote Procedure Calls) 與 IPC(Inter-process Communications) 兩種方式來與 geth 互動，如果你要在 local 機器連上 geth，那就可以使用 IPC；如果要讓遠端連上 geth，那就使用 RPC，可以開 HTTP 或 Web Socket 兩種方式來讓遠端使用。
以上就是 Ethereum 應用程式開發的基礎環境，接下來跟開發網頁應用程式一樣，Ethereum 應用程式也分成後端與前端，後端程式就是 Smart Contract，前端程式就是 Dapp。Smart Contract 可使用 Solidity 撰寫，目前也有許多其他語言可以撰寫 Smart Contract。Smart Contract 要在 Ethereum 上的 EVM 執行要先 Compile 成 Byte Code 之後，再透過 IPC 或 RPC 發佈到 Ethereum 上。前端程式的 Dapp 可用 Web3 JavaScript 透過 RPC 接上 Ethereum，以及使用網頁應用常用到的 HTML、CSS、JavaScript 製作成使用者互動介面，如此就能執行發佈在 Ethereum 上 Smart Contract 所提供的一些程式功能了。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-18ethereum-%E7%9A%84%E7%8D%8E%E5%8B%B5%E6%A9%9F%E5%88%B6/">
                <h3 class="media-heading">Ethereum 開發筆記 1–8：Ethereum 的獎勵機制</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Bitcoin 的獎勵機制基本上是挖到新區塊的節點獲得記帳權及獎勵，Ethereum 大體也是遵循這樣的概念，但做了一些調整與變化，讓我們整個脈絡了解一下。
由於 Blockchain 是一種去中心化的系統，所有的礦工（節點）可以同時挖礦（計算合法 hash），彼此獨立運作，所以極有可能出現兩的礦工同時發現不同的滿足條件的區塊，如此就會產生我們之前有提過的分叉（Fork）。
那我們該採用誰的區塊當主鏈呢？我們會先依工作量最大的區塊為主鏈，如果工作量一樣，就看誰先接了子區塊，一般來說只有成了主鏈的區塊才能獲得獎勵。但這樣沒有變成主鏈的區塊之前的算力就都白費了，所以 Ethereum 創造了 Uncle Block（叔塊）這樣的概念，不能成為主鏈的區塊如果後來被收留成為 Uncle Block，那這些沒有成為主鏈的區塊也有機會可以做為 Uncle Block 而獲得獎勵。
這就是 Ethereum 共識機制中的 GHOST（Greedy Heaviest Observed Subtree）協議，Ethereum 會這樣設計的原因，是由於 Ethereum 產生區塊的速度較快，也因此較容易產生分叉，也會使得新區塊較難以在整個網絡傳播，這對於傳播速度較慢的區塊並不公平。且分叉後的區塊可能在幾個區塊之後整併起來，我們會發現裡面的交易可能會與主鏈一致（雖然單獨查看分塊交易內容不同，不過數個區塊整體一起看交易內容就一致了），符合這種條件的分叉區塊我們就會納入主鏈參考，這些區塊就成了所謂的 Uncle Block，這某種角度也是更確認了 Blockchain 上的交易內容一致，因此 Uncle Block 也有貢獻，應該給予獎勵。
以上我們已經了解了 Ethereum 上的區塊大致分成兩種，普通區塊和 Uncle Block，Ethereum 對這兩種區塊的獎勵方式是不同的。我們分別來看一下。
普通區塊獎勵  固定獎勵 5 ETH 區塊內所有的 Gas Fee 如果區塊納入了 Uncle Block，那每包含一個 Uncle Block 可以得到固定獎勵 5 ETH * 1/32，也就是 0.15625 ETH，一個區塊最多隻能包含 2 個 Uncle Block，也因此不會無限延伸，同時又可鼓勵區塊納入 Uncle Block，增加交易內容的一致性。  Uncle Block 獎勵  用公式計算：（Uncle Block 高度 + 8 - 包含此 Uncle Block 的區塊的高度）* 普通區塊固定獎勵 / 8  我們用個實例來看一下獎勵怎麼算。首先我們來看一個普通區塊：https://etherscan.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-17blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E6%80%A7%E8%B3%AA/">
                <h3 class="media-heading">Ethereum 開發筆記 1–7：Blockchain 的一些重要性質</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">我們這邊再次總結一下 Blockchain 中幾點較重要的性質，包含共識機制、不可竄改、經濟激勵三項。
共識機制（Consensus） 在分散式系統中，我們需要有一套用於協同合作的共識機制來組織行動，但有時候系統中的成員可能會出錯或是故意傳送出錯誤的資訊，而使得網路中不同成員對於全體協作的策略得出不同的結論，進而破壞系統的一致性，這就是所謂的拜占庭將軍問題。
拜占庭將軍問題（Byzantine Generals Problem） 拜占庭將軍問題這個故事是這樣的：
 一組拜占庭將軍分別各率領一支軍隊共同圍困一座城市，這個敵人雖不比拜占庭帝國，但也足以抵禦 5 支拜占庭軍隊的同時襲擊。這 10 支軍隊在分開的包圍狀態下，他們任 1 支軍隊單獨進攻都毫無勝算，除非有至少 6 支軍隊（一半以上）同時襲擊才能攻下敵國。他們分散在敵國的四周，依靠通信兵騎馬相互通信來協商進攻意向及進攻時間。困擾這些將軍的問題是，他們不確定他們中是否有叛徒，叛徒可能擅自變更進攻意向或者進攻時間。在這種狀態下，拜占庭將軍們才能保證有多於 6 支軍隊在同一時間一起發起進攻，從而贏取戰鬥？
 上述的故事對映到電腦系統裡，將軍便成了電腦，而通信兵就是通訊系統。叛徒發送前後不一致的進攻提議，被稱為「拜占庭錯誤」，而能夠處理拜占庭錯誤的這種容錯性稱為「Byzantine Fault Tolerance」。Blockchain 上的共識機制通常具有容錯的設計來達成一致性，主要比較常見的共識機制方法有兩個，「工作量證明」以及「股權證明」兩種方法。
工作量證明演算法（Proof of Work, PoW） 中本聰在 Bitcoin 中創造性的引入了「工作量證明」（俗稱挖礦）來解決拜占庭將軍問題，顧名思義，工作量證明就是用來證明你做了一定量的工作，可用工作成果來證明完成相應的工作量。其中的工作技術原理可以看之前這篇文章：Ethereum 開發筆記 1–4：Blockchain 技術原理簡介
由於工作量證明具相當高的計算成本，因此無誘因去偽造，只有遵守協議約定，才能夠回收成本並獲得收益，也因此減少了叛徒的產生，減少拜占庭錯誤。
股權證明演算法（Proof of Stake, PoS） 股權證明的出現，主要是希望取代工作量證明，進而減少「挖礦」的大量運算。它與工作量證明不同地方在於：工作量證明中，大家比的是「算力」（運算能力），透過大量運算得出符合難度的 Hash 值，進而得到獎勵；而在股權證明，大家比拼的是「股權」，「股權」越大的人（節點）越大機會負責產生新區塊，進而得到獎勵。
舉例來說，在股權證明系統中所有擁有股權（此 Blockchain 的數位貨幣）的人都有機會被挑選為產生新區塊（也就是記帳）的人，擁有更多股權的人被選中的機率越大。假這這個系統中共有三個人：Alice 持有 50 股、Bob 持有 30 股、Cathy 持有 20 股，那每次 Alice 被選為記帳人的機率會是 Cathy 的兩倍。所以股權證明會驅使人們購買更多的股權，進而增加獲選為記帳人的機率，以買股權來代替挖礦，同樣需要付出高成本，也因此可以減少叛徒的產生，減少拜占庭錯誤。
不可竄改（Immutability） Blockchain 不可竄改的性質主要來自資料結構及 hash 方式的設計，讓資料的順序緊密鏈結，若從中竄改了某些資料，那之後的鏈結 hash 都會發生錯誤，形成了 Blockchain 不可竄改的特性。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-16blockchain-%E7%9B%B8%E9%97%9C%E7%9A%84%E5%8A%A0%E5%AF%86%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98/">
                <h3 class="media-heading">Ethereum 開發筆記 1–6：Blockchain 相關的加密基礎知識</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Blockchain 裡應用了一些加密技術來保證及驗證交易訊息的正確性，這也更加強了 Blockchain 資料不可竄改的特性。我們來介紹其中比較重要的「公私鑰加密」以及「Merkle Tree」加密樹。
公私鑰加密 公私鑰加密算法是目前資訊通訊安全的基石，它保證了加密訊息不可被破解，相關的加解密原理大家可以參考這兩篇文章：
 RSA算法原理（一）http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html RSA算法原理（二）http://www.ruanyifeng.com/blog/2013/07/rsa_algorithm_part_two.html  加密與解密 公私鑰加密方法是一種非對稱式加密，透過公鑰加密過後的訊息只有私鑰可以解密，也因此只要保護好私鑰就能保證資訊的安全。
現在假設 Alice 要傳一個訊息給 Bob，希望訊息加密過後只有 Bob 可以解密，大概會經過如下步驟：
 Bob 傳他的公鑰給 Alice Alice 使用 Bob 的公鑰加密訊息 Alice 將加密過後的訊息傳給 Bob Bob 用他的私鑰解密訊息  我們這邊使用 openssl 來練習一下加密與解密，首先我們來產生一對公私鑰：
// Create RSA private key $ openssl genrsa -des3 -out rsa-key.pem 2048 // Create public key $ openssl rsa -in rsa-key.pem -outform PEM -pubout -out rsa-key-pub.pem  其中 rsa-key.pem 就是私鑰，rsa-key-pub.pem 為公鑰，私鑰會要求設置密碼，請妥善記下密碼。
我們先用 rsa-key-pub.pem 加密資料：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-15blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%BE%A9%E8%88%87%E5%90%8D%E8%A9%9E/">
                <h3 class="media-heading">Ethereum 開發筆記 1–5：Blockchain 的一些定義與名詞</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在 Ethereum 開發筆記 1–4 應該已經將 Blockchain 的技術原理說明得很清楚了，不過如果要向一般大眾簡單說明 Blockchain 是什麼，要怎麼說呢？我會說：Blockchain 就是一個分散式帳本，大家都有一樣的帳本，大家都可一起參與記帳，且記完帳大家的帳本就會自動更新到最新版本，而帳裡的紀錄都會分塊並用密碼按順序鏈結起來，用以驗證帳的正確性，如果中間有人改了資料，那後面的鏈結密碼都會發生錯誤，因此沒有人可以亂改帳，這就是 Blockchain。
但 Blockchain 這個名詞還包含了許多概念與內涵，我們之前說過，Blockchain 是因為分散式去中心化帳本的發展而慢慢產生出來的，這樣慢慢被統稱出來的名詞裡底下也就會包含了許多內涵，很難用三言兩語來說明，所以有一些 Blockchain 相關的定義與名詞我們都可以了解一下，這樣就能更了解 Blockchain。
交易（Transaction） 交易是 Blockchain 帳本中的原子單位，如果將交易再往下拆分就會變得沒有意義，比如下列就是一個交易：
 A 減少了 $10 B 增加了 $9 C 增加了 $1  如果只看 1，我們就會想那減少的 $10 到哪裡去了？所以 1、2、3 一起看才算是一個交易。
Blockchain 是一個分散式帳本（Distributed Ledger） 不像銀行依靠自己的帳本來記帳，Blockchain 提供了可靠的分散式帳本，當銀行之間要進行交易時，會需要一個受信任的第三方來進行銀行之間的交易，這也是為何你在做跨國轉帳時，需要付出高昂的手續費以及等待數天處理交易，Blockchain 可靠的分散式帳本讓跨國交易可以在幾分鐘甚至幾秒之內完成，這也是為何銀行想要應用 Blockchain 在金融交易上以降低交易成本。
Blockchain 是一個資料結構（Data Structure） 通常 Blockchain 的資料結構如下組成：
 交易是原子單位 區塊是由一系列的交易組成 區塊鏈由排序良好的區塊所組成  Blockchain 會有分叉（Fork） 當有兩名礦工 A 及 B 幾乎在相同時間內算出了合法的 hash，這兩個區塊傳播到鄰近節點時，有些節點收到了 A 的區塊，有些節點收到了 B 的區塊，這兩個區塊都可以是主鏈的延伸，這時就會產生區塊鏈分叉。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-14blockchain-%E6%8A%80%E8%A1%93%E5%8E%9F%E7%90%86%E7%B0%A1%E4%BB%8B/">
                <h3 class="media-heading">Ethereum 開發筆記 1–4：Blockchain 技術原理簡介</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前我們簡單地介紹過 Blockchain 了，但我們還是對 Blockchain 背後的技術原理不是那麼了解，我們知道 Blockchain 是因為一個數位貨幣帳本這樣的概念被創造出來的，而數位貨幣最擔心的是什麼問題呢？其實就是雙重支付（Double-Spending）這樣的問題。
數位貨幣不像實體貨幣，數位資產比起實體資產容易複製，也因此如果花用數位貨幣的行為如果沒有處理好，就會產生憑空多出其他交易，這就像是偽鈔一樣，會造成通貨膨脹而導致貨幣貶值，讓人不再信任並願意持與流通。因此數位貨幣的支付通常需要一個受信任的第三方來做驗證，這樣的做法雖然簡單，卻存在單點脆弱性，只要這第三方受到攻擊或是監守自盜也一樣會讓這個數位貨幣變成一個失敗的貨幣。
分散式去中心化帳本能解決單點脆弱性的問題，但在驗證正確性這點難度卻很高，所有的節點都有記帳的權利，要如何確定由誰來記帳、記的帳對不對？如果無法確定帳是對的，那就存在雙重支付的風險。
為了改善單點脆弱性及雙重支付這樣的問題，許多分散式的雙重支付防範方法慢慢被提出來，中本聰提出了去中心化（以受信任第三方為中心）的方法來展示解決雙重支付問題，並實作出了 Bitcoin，使用共識機制來解決記帳及驗證的問題，這帶來去中心化數位貨幣帳本的成功。
Bitcoin 的共識協議主要由「工作量證明」（Proof-of-Work, PoW）和「最長鏈機制」兩部分組成，Bitcoin 上的各個節點就是透過共識機制中的工作量證明來決定誰有記帳權，然後取得記帳權的節點就能將新的區塊記帳加到最長鏈上並給予該節點獎勵（新區塊獎勵及交易費收益）。
Bitcoin 的 工作量證明大概會做以下的事情：
 收集還未記到帳上的交易 檢查每個交易中付款地址有沒有足夠的餘額 驗證交易是否有正確的簽名 把驗證通過的交易信息進行打包（組成 Merkle Tree） 為自己增加一個交易紀錄獲得 Bitcoin 獎勵金 計算合法的 hash 爭奪記帳權  計算合法 hash 的方式請見下方影片說明，個人覺得這個影片是目前將 Blockchain 加密機制說明得最清楚的影片。我這邊簡略說明一下，合法的 hash 公式大致看起來像這樣：hash(交易內容+交易簽名+nonce+上一個區塊的 hash)，我們要取得記帳權，就需要找出前面開頭有 N 個 0 的 hash，由於交易內容、交易簽名及上一個區塊的 hash 都是不可變的，所以每個節點就是不斷的調整 nonce 來計算得出不同的 hash，直到找到開頭 N 個 0 的 hash 為止，第一個找的節點就能獲得記帳權，而其他的節點只要計算 hash 對不對就能驗證這個帳對不對。其中 N 個 0 開頭的 hash 就代表了計算的難度，越多 0 代表越難找到這樣的 hash，也因此可以調整計算難度。就是這樣的設計解決了去中心化分散式系統驗證資料及決定記帳順序的難題，也就改善了數位貨幣單點脆弱性及雙重支付的問題。
  以上的內容看完應該就能大體了解 Blockchain 的原理了，甚至要自己做一個 Blockchain 都沒問題！了解了 Blockchain 的技術原理之後，應該能更信任去中心化的數位貨幣的安全性，或許有天大家都信任了去中心化的數位貨幣我們就真的能廣泛使用數位貨幣，為經濟活動帶來更有效率的流通。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98%E7%B7%B4%E7%BF%92-1%E4%BD%BF%E7%94%A8-mist-%E7%99%BC%E8%A1%8C%E8%87%AA%E5%B7%B1%E7%9A%84-token/">
                <h3 class="media-heading">Ethereum 開發筆記練習 1：使用 Mist 發行自己的 Token</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前說過，Blockchain 基本上是因為金流帳本這樣的問題而被創造出來的，也就是說區塊鏈非常適合運用在金流的應用上，我們也可以建立自己的 Blockchain 來搭建自己的金流系統，不過在 Ethereum 上 Smart Contract 這種設計讓我們擁有可以在 Ethereum 區塊鏈上創造自己金流系統的能力，如此我們就不需要自己建一條鏈了。
我們使用 Smart Contract 仿造貨幣性質創造了數位資產（說穿了其實就是在 Smart Contract 上紀錄的變數而已），而這種具貨幣性質的數位資產又被稱作 Token，如此我們就可以在應用程式中使用這個去中心化的金流系統，由於 Token 的應用很普遍，大部分的功能都已經標準化了，我們只要仿造標準來實作就可以發行自己的數位貨幣了。
在這邊我們就練習一下怎麼使用 Mist 發佈 Token Smart Contract 來發行自己的數位貨幣。（目前我們還沒有學習過如何撰寫 Smart Contract，因此這邊會先直接提供範例程式碼，實作的部分我們之後再慢慢學習）
以下是我們的範例程式碼：
 請打開 Mist，如下圖點擊 Contract，然後點擊 Deploy New Contract。
你會看到如下圖的頁面，請在 Solidity Contract Source Code 中貼上我們上面提供的範例程式碼。
貼上範例程式碼之後，Mist 會自動編譯程式，檢查是否有語法上的錯誤，如果沒問題，右方的 Select Contract to Deploy 就會出現選項，在這邊我們選擇 Token ERC 20。
選擇 Token ERC 20 之後，右方會出現要初始化 Contract 的參數表單，有 Initial supply、Token name、Token symbol 需要填寫。Initial supply 代表 Token 的總發行量是多少，我這邊設定成 7777777777，你可以設成你想要的數字。Token name 就是這個 Token 要叫什麼名字，這邊我設定成 7 Token，你想要取 Dog Coin 或是 Cat Coin 也都可以。Token symbol 就是這個 Token 要用什麼代號，像是美金就是用 $、Ether 是用 ETH，這邊我設定成 7token，你可以取自己覺得帥的代號。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-13%E4%BD%BF%E7%94%A8-mist/">
                <h3 class="media-heading">Ethereum 開發筆記 1–3：使用 Mist</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Mist 跟前回介紹的 MetaMask 一樣是可以與 Ethereum 進行互動的工具，除了可以管理 Ethereum 相關密鑰之外，Mist 還包含了 Ethereum 節點以及網頁瀏覽器，方便大家瀏覽 Dapp 網頁。
首先請到這邊安裝 Mist，請選擇適於自己的作業系統安裝。
由於 Mist 會安裝節點在你的電腦裡，也因此會同步整個帳本下來，所以會花上不少時間同時也會佔用許多硬碟空間。我們目前僅是要使用測試鏈，所以請切換到 Ropsten 測試鏈（如下圖），這樣就不用花這麼多時間與空間了。
在 Mist 的左下角可以觀察目前已同步到你的電腦的區塊數（如下圖），如果這個數字跟 Etherscan（Etherscan 是一個可以查看 Ethereum 區塊鏈所有交易的網站） 上的最新區塊數一致的話，那就代表已經同步完成了。
接下來讓我們用 Mist 開一個 Ethereum 帳戶，請點擊 Add Account，並依指示輸入密碼後創建帳號，密碼請務必要記下來，將來交易時都會需要輸入你的密碼。
學會創建 Ethereum 帳戶之後，我們要來看一下 Mist 要怎麼備份帳號，請點擊 Mist 上方選單的 File -&gt; Backup -&gt;Accounts（如下圖），這樣就會打開帳號存放的資料夾，所有的帳號都會加密存在這邊，所以只要備份這些檔案及當時設定的密碼，你就可以在別台電腦復原你的帳號。
現在你這個 Ethereum 帳戶還沒有任何 Ether，我們仿造之前用 MetaMask 來跟水龍頭要 Ether 的步驟來取得 Ether 看看。
我個人提供了一個水龍頭 Dapp，請前往這個網址來取得 Ether：https://blog.fukuball.com/dapp/faucet/
由於 Mist 也是一個 Dapp 網頁瀏覽器，請在 Mist 上方的網址列輸入：https://blog.fukuball.com/dapp/faucet/
Mist 在揭露你的 Ethereum 帳戶資訊給 Dapp 網頁時都會詢問你的同意，請先選擇要瀏覽這個 Dapp 網頁的帳號（你可能在 Mist 有多個帳號，所以就需要選擇目前要用哪個帳號瀏覽這個網頁）。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero=""
         data-message-one=""
         data-message-other="">
         76 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://blog.fukuball.com/images/ok.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://blog.fukuball.com/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>






<script async src="https://www.googletagmanager.com/gtag/js?id=UA-41911929-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-41911929-4');
</script>

    
  </body>
</html>

