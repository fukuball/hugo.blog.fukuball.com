


  




  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.48 with theme Tranquilpeak 0.4.3-BETA">
    <title>Machine Learning</title>
    <meta name="author" content="Fukuball">
    <meta name="keywords" content="">

    <link rel="icon" href="images/favicon.ico">
    
      <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.fukuball.com/tags/machine-learning/index.xml">
    

    
    <meta name="description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Machine Learning">
    <meta property="og:url" content="/tags/machine-learning/">
    <meta property="og:site_name" content="I am Fukuball">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="I am Fukuball">
    <meta name="twitter:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    
    

    
    

    
      <meta property="og:image" content="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=640">
    

    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://blog.fukuball.com/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="1">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://blog.fukuball.com/">I am Fukuball</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://blog.fukuball.com/#about">
    
    
    
      
        <img class="header-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=90" alt="" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="1">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://blog.fukuball.com/#about">
          <img class="sidebar-profile-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
        </a>
        <h4 class="sidebar-profile-name">Fukuball</h4>
        
          <h5 class="sidebar-profile-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://facebook.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-facebook"></i>
      
      <span class="sidebar-button-desc">Facebook</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">Blog</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      
        

      
      <div id="main" data-behavior="1"
        class="
               hasCoverMetaIn
               ">
        
          <section class="postShorten-group main-content-wrap">
            
            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 12 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-02-14T15:18:51&#43;08:00">
        
  February 14, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/">第十一講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中，我們將線性分類的模型擴展到可以進行多元分類，擴展的方法很直覺，就是使用 One vs One 及 One vs All 兩種分解成二元分類的方式來做到多元分類。在這一講中將講解如何讓線性模型擴展到非線性模型，讓我們可以將機器學習演算法的複雜度提高以解決更複雜的問題，並說明非線性模型會有什麼影響。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-1.png">
</p>

<h3 id="線性假設">線性假設</h3>

<p>之前的演算法目前都是基於線性的假設之下去找出分類最好的線，但這在線性不可分的情況下，會得到較大的 Ein，理論上較大的 Ein 未來 Eout 效果也會不佳，有沒有辦法讓我們演算法得出的線不一定要是一條直線以得到更佳的 Ein 來增加學習效果呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-2.png">
</p>

<h3 id="圈圈可分">圈圈可分</h3>

<p>我們從肉眼觀察可以發現右邊的資料點是一個「圈圈可分」的情況，所以我們要解這個問題，我們可以基於圈圈可分的情況去推導之前所有的演算法，但這樣有點麻煩，沒有沒其他更通用的方法？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-3.png">
</p>

<h3 id="比較圈圈可分及線性可分">比較圈圈可分及線性可分</h3>

<p>為了讓演算法可以通用，我們會思考，如果我們可以讓圈圈可分轉換到一個空間之後變成線性可分，那就太好了。我們比較一下圈圈可分及線性可分，當我們將 Xn 圈圈可分的資料點，透過一個圈圈方程式轉換到 Z 空間，這時資料點 Zn 在 Z 空間就是一個線性可分的情況，不過在 Z 空間線性可分，在 X 空間不一定會是圈圈可分。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-4.png">
</p>

<h3 id="z-空間的線性假設">Z 空間的線性假設</h3>

<p>觀察在 Z 空間的線性方程式，不同的參數在 X 空間會是不同的曲線，有可能是圓、橢圓、雙曲線等等，因此我們了解在 Z 空間的線會是 X 空間的二次曲線。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-5.png">
</p>

<h3 id="一般化二次假設">一般化二次假設</h3>

<p>我們剛剛是使用 x0, x1^2, X2^2 來簡化理解這個問題，現在將問題更一般化，將原本的 xn 用 Phi 二次展開來一般化剛剛個問題，這樣的 Z 空間學習出來的線性方程式在 X 空間就不一定會是以原點為中心，這樣所有的二次曲線都有辦法在 Z 空間學習到了，而起原本在 X 空間的線性方程式也會包含在按次曲線中。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-6.png">
</p>

<h3 id="好的二次空間假設">好的二次空間假設</h3>

<p>所以原本的問題可以透過這樣的非線性轉換到二次 Z 空間進行機器學習演算法，在 Z 空間的線性可分就可以對應到 X 空間的二次曲線可分。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-7.png">
</p>

<h3 id="非線性轉換的學習步驟">非線性轉換的學習步驟</h3>

<p>了解了這樣的思路之後，非線性轉換的學習步驟就是先將資料點透過 Phi 轉換到非線性空間，然後使用之前學過的線性演算法進行機器學習，由於學習出來的 Z 空間線性方程式不一定能轉回 X 空間，我們實務的上做法是將測試資料透過 Phi 轉換到 Z 空間，再進行預測。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-8.png">
</p>

<h3 id="非線性模型是潘朵拉的盒子">非線性模型是潘朵拉的盒子</h3>

<p>學會了特徵轉換使用非線性模型就像打開了潘朵拉的盒子，我們可以任意的將資料轉換到更高維的空間來進行機器學習，如此可以得到更低的 Ein，但這對機器學習效果不一定好，因此要慎用。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-9.png">
</p>

<h3 id="代價一-更高的計算及儲存代價">代價一：更高的計算及儲存代價</h3>

<p>我們可以將資料點進行 Q 次轉換，這樣原本的資料點會有 0 次項、1 次項、2 次項 &hellip;. Q 次項，每筆資料的維度都增加了，理所當然計算量及儲存量也都變高了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-10.png">
</p>

<h3 id="代價二-更高的模型複雜度">代價二：更高的模型複雜度</h3>

<p>進行了 Q 次轉換後，資料的為度更高了，理論上 VC dimention 也跟著增加了， VC dimention 代表著模型複雜度，在之前的課程中我們知道較複雜的模型會讓 Eout 變高。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-11.png">
</p>

<h3 id="可能會面臨的問題">可能會面臨的問題</h3>

<p>我們用下圖來說明我們可能會面臨的問題，左圖雖然不是線性可分，但一眼看來其實也是一個不錯的結果，右圖可以得到完美的 Ein，但會覺得有點過頭，我們會面臨的問題就是要如何抉擇左邊或右邊？較高的 Q 次轉換會造成 Eout 與 Ein 不會很接近，但可以得到較小的 Ein，較低的 Q 次轉換可以保證 Eout 跟 Ein 很接近，但 Ein 的效果可能不好，怎麼選 Q 呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-12.png">
</p>

<h3 id="用看的選有風險">用看的選有風險</h3>

<p>就上述的例子我們可以用圖示來觀察，而使用較低的 Q 次轉換，但如果為度很高，我們是無法畫成圖來看的，而且用看圖的方式，我們可能會不小心用我們的人工運算，直接加上圈圈方程式來降低 VC dimention，這也可能會造成 Eout 效果不佳，因為 VC dimention 是經過人腦降低的，會讓我們低估 VC dimention 複雜度，所以我們應該要避免用看資料的方式來調整演算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-13.png">
</p>

<h3 id="多項式結構化">多項式結構化</h3>

<p>我們將 Q 次轉換用下面的式子及圖示結構化，我們可以發現 0 次轉換的假設會包含在 1 次轉換的假設中，1 次轉換的假設會包含在 2 次轉換的假設中，一直到 Q 次轉換這樣的結構，表示成 H0, H1, H2, &hellip;., Hq。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-14.png">
</p>

<h3 id="假設集合結構化">假設集合結構化</h3>

<p>從 H0 包含於 H1、H1 包含於 H2 &hellip;. Hq-1 包含於 Hq 這樣的關係中，我們可以推論 d_vc(H0) &lt;= d_vc(H1) &hellip; &lt;= d_vc(Hq)，而在理論上 Ein(g0) &gt;= Ein(g1) &hellip; &gt;= Ein(gq)，從之前學過的理論可知，out of sample Eout 在 d_vc 很高、Ein 很低的情況下，不一定會是最低點。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-15.png">
</p>

<h3 id="線性模型第一優先">線性模型第一優先</h3>

<p>所以依據理論，我們不該為了追求 Ein 低、訓練效果好來做機器學習，這樣是一種自我欺騙，我們要做的應該是使用線性模型為第一優先，如果 Ein 很差，則考慮做二次轉換，慢慢升高 d_vc，而不是一步登天。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-16.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講中我們打開了潘朵拉的盒子，學會了使用非線性轉換來得到更好的 Ein，但這會付出一些代價，會讓計算量增加、資料儲存量增加，若一次升高太多模型複雜度，還會造成學習效果不佳，Eout 會比 Ein 高很多，所以要慎用。最好的學習方式就是先從線性模型開始，然後再慢慢升高模型複雜度。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-17.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 11 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-02-01T09:45:06&#43;08:00">
        
  February 1, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/">第十講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中我們了解了 Logistic Regression 演算法並了解了如何使用 Logistic Regrssion 來預測心臟病發病機率這樣的問題，這一講中將延伸之前學過的演算法，在理論上說明 Linear Regression 以及 Logistic Regression 都可以用來解 Binary Classification 的問題。學會了 Binary Classification 之後，我們也可以用這樣的技巧來解 Multi-Classification 的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-1.png">
</p>

<h3 id="比較之前學過的演算法">比較之前學過的演算法</h3>

<p>比較之前學過的演算法，三個算法最後都會得到一個線性函數來輸出 scroe 值，但 PLA 做 Linear Classification 是一個 NP-hard 的問題，Linear Regression 及 Logistic Regression 則相對較容易求解，我們可以使用 Linear Regression 或是 Logistic Regression 演算法來解 Linear Classification 的問題嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-2.png">
</p>

<h3 id="將三個演算法的-error-function-整理一下">將三個演算法的 Error Function 整理一下</h3>

<p>依據各個 Error Function 的算法，我們都可以整理成 ys 的形式，在物理意義上，我們可以說 y 代表正確性，s 代表正確或錯誤程度多少。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-3.png">
</p>

<h3 id="畫成圖來瞧瞧">畫成圖來瞧瞧</h3>

<p>我們以 ys 為橫坐標，error 為縱坐標，把這三個函數畫出來。 0/1 error 在 ys &lt;= 0 時 error 是 1，squre error 在 ys &lt;&lt; 1 或 ys &gt;&gt; 1 時會很大，cross-entropy error 在 ys 很小時， error 也會變得很大，但當 squre error 跟 cross-entropy error 很小時，他們 ys 區間所對應的 squre error 也會很小，因此我們可以從這樣的圖得知 squre error 跟 cross-entropy error 都是 0/1 error 的上界。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-4.png">
</p>

<h3 id="套用至-vc-bound-理論">套用至 VC Bound 理論</h3>

<p>將這樣的 Error 上界關係套用 VC Bound 理論，只要 Logistic Regression 或 Linear Regression 的 E_in 小，那 Linear Classfication 的 E_out 就會小，因此理論上我們可以使用 Logistic Regression 及 Linear Regression 來代替 Linear Classfication。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-5.png">
</p>

<h3 id="使用-regression-來做分類">使用 Regression 來做分類</h3>

<p>依據以上的理論，我們可以用 Regression 來代替 PLA/Pocket 做分類，會比較有效率（PLA 是一個 NP-hard 的問題），比較一下三個演算法，(1) PLA 在線性可分的時候可以得到一個最佳解，但資料常常不會是線性可分，所以就會用 Pocker 來替代。（2）Linear Regression 可以很快的優化求解，但當 |ys| 很大的時候，positive direction 及 negative direction 的 bound 都太鬆，E_out 可能會效果不好。(3) Logistic Regression 可以用 gradient descent 求解，但在 negative direction 的 bound 會太鬆，E_out 可能會效果不好。</p>

<p>以據上述的特性，我們可以使用 Linear Regression 跑出一個 w 作為 (PLA/Pocket/Logistic Regression) 的 w0，然後再使用 w0 來跑其他模型，這樣可以加快其他模型的優化速度。然後實務上拿到的資料常常不是線性可分的，所以我們會比較常使用 Logistic Regression 而不是 PLA/Pocket。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-6.png">
</p>

<h3 id="優化-logistic-regression">優化 Logistic Regression</h3>

<p>實務上我們會比較常使用 Logistic Regression，但 Logistic Regression 比起 PLA 比較沒有效率，因為 Logistic Regression 在決定優化方向時，會觀察所有的資料點再做決定，時間複雜度是 O(N)，但 PLA 每次只看一個點，時間複雜度是 O(1)，我們可以讓 Logistic Regression 優化到 O(1) 嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-7.png">
</p>

<h3 id="隨機梯度下降">隨機梯度下降</h3>

<p>我們可以使用隨機梯度下降的方式讓 Logistic Regresiion 優化到 O(1)，這樣的方法就是每次只透過隨機選取一個資料點（xi, yi）來取梯度，然後再用這個梯度對 w 進行更新，這種優化方法就叫做隨機梯度下降。</p>

<p>原來的演算法是用所有的資料點在算梯度，然後取平均，再更新 W，隨機梯度下降是不用每次算所有的點，每次只算一個點來代替所有點的平均。可以這樣做的背後原理，我們可以想成隨機取一個點取很多次之後，大概就是跟所有的點做平均差不多，所以我們可以用隨機梯度下降取代原本的梯度下降。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-8.png">
</p>

<h3 id="隨機梯度下降與-pla-的關係">隨機梯度下降與 PLA 的關係</h3>

<p>將隨機梯度下降與 PLA 算式放在一起觀察，可以發現隨機梯度是一個軟性的 PLA，每次調整 0~1 ynxn，但 PLA 只有調與不調，比較硬。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-9.png">
</p>

<h3 id="多元分類">多元分類</h3>

<p>我們現在已經會 Binary Classification 了，那 Multi-Classification 的問題要怎麼解呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-10.png">
</p>

<h3 id="一次分一個類別出來">一次分一個類別出來</h3>

<p>既然我們會二元分類，我們可以一次分一個類別出來，讓現在要分類出來的資料是 o，其他資料設成 x 來做分類。以這個例子就是先把正方形變成 o，其他變成 x。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-11.png">
</p>

<h3 id="結合所有的二元分類">結合所有的二元分類</h3>

<p>結合所有的二元分類模型之後，我們就可以做多元分類了，不過會有一些區域有模糊地帶。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-12.png">
</p>

<h3 id="使用軟性二元分類來解模糊地帶的問題">使用軟性二元分類來解模糊地帶的問題</h3>

<p>在這邊我們可以使用軟性二元分類來解模糊地帶的問題，比如將所有的二元分類模型用 Logistic Regression 來訓練，就能讓每個分類器預測出資料是屬於哪一類的百分比，這樣我們就可以從所有的分類器裡面找出百分比最大的那個來預測這個資料點是屬於哪一類，而解決模糊地帶的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-13.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-14.png">
</p>

<h3 id="one-versus-all-ova-decomposition">One-Versus-ALL (OVA) Decomposition</h3>

<p>這樣的方法就是 One-Versus-ALL (OVA) Decomposition，每次把一個類別和非這個類別的當成兩類，用 Logistic Regresion 分類，當分類器輸入某個點，就看這個點在哪一個類別的機率最大。不過這個方法的缺點是，當類別很多的時候，比如 k=100，那每次用 logistic Regression 分類時正樣本和負樣本的差別就會非常大，訓練出來的結果可能會不好。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-15.png">
</p>

<h3 id="一次只比較兩個類別">一次只比較兩個類別</h3>

<p>我們可以用一次只比較兩個類別這樣的方法來解決 OVA 樣本資料差距過大的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-16.png">
</p>

<h3 id="投票預測分類">投票預測分類</h3>

<p>這樣的方法，每次只取兩個類別來做訓練，如果一共有 K 類的話，就要做  C(K, 2) 次的 Logistic Regression。當一個資料點輸入做預測時，就用這 C(K, 2) 個分類器給所有 K 個類別投票，取票數大的作為輸出結果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-17.png">
</p>

<h3 id="one-versus-one-ovo-decomposition">One-Versus-One (OVO) Decomposition</h3>

<p>這樣的方法就是 One-Versus-One (OVO) Decomposition，這種方法的好處是可以應用在任何 Binary Classification 方法，缺點是效率可能會低一些，因為要訓練 C(K,2) 個類別，不過如果類別很多且每個類別的樣本量都差不多的時候，OVO 的方法不一定會比 OVA 方法效率低。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-18.png">
</p>

<h3 id="總結">總結</h3>

<p>在第十一講中，我們比較了 PLA、Linear Regression 及 Logistic Regression，然後理論上這三個演算法都可以應用在 Linear Classifition 上，然後也學會了使用 Stochastic Greadient Descent 來讓 Logistic Regression 跟 PLA 演算法的計算時間複雜度一樣。學會了 Binary Classification 之後，我們也可以將這樣的方法運用 OVA 及 OVO 的方式來解 Multi-Classification 的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-19.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 10 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-01-17T18:37:49&#43;08:00">
        
  January 17, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/">第九講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在上一講中我們了解了如何使用線性迴歸的方法來讓機器學習回答實數解的問題，第十講我們將介紹使用 Logistic Regression 來讓機器學習回答可能機率的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-1.png">
</p>

<h3 id="可能性問題">可能性問題</h3>

<p>我們用心臟病發生的機率這樣的問題來做為例子，假設我們有一組病患的數據，我們需要預測他們在一段時間之後患上心臟病的「可能性」為何。我們拿到的歷史數據跟之前的二元分類問題是一樣的，我們只知道病患之後有或者沒有發生心臟病，從這樣的數據我們用之前學過的二元分類學習方法可以讓機器學習到如何預測病患會不會發病，但現在我們想讓機器回答的是可能性，不像二元分類那麼硬，所以又稱軟性二元分類，即 f(x) = P(+1|x)，取值的範圍區間在 [0, 1]。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-2.png">
</p>

<h3 id="軟性二元分類">軟性二元分類</h3>

<p>我們的訓練數據跟二元分類是完全一樣的，x 是病人的特徵值，y 是 +1（患心臟病）或 -1（沒有患心臟病），沒有告訴我們有關「患病機率」的訊息。回想在二元分類中，我們使用 w*x 得到一個「分數」之後，再利用取號運算 sign 來預測 y 是 +1 或是 -1。對於目前這個問題，如果我們能夠將這個「分數」映射到 [0, 1] 區間，似乎就可以解這個問題了～</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-3.png">
</p>

<h3 id="logistic-假設">Logistic 假設</h3>

<p>我們把上面的想法寫成式子，就是 h(x) = theta(wx)，映射分數到 [0, 1] 的 function 就叫做 logistic function，用 theta 來表示</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-4.png">
</p>

<h3 id="logistic-函式">Logistic 函式</h3>

<p>Logistic regression 選擇使用了 sigmoid 來將值域映射到 [0, 1]，sigmoid 是 f(s) = 1 / (1 + exp(-s))，f(x) 單調遞增，0 &lt;= f(x) &lt;= 1。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-5.png">
</p>

<h3 id="如何定義-logistic-regression-的-error-function">如何定義 Logistic Regression 的 Error Function</h3>

<p>那麼對於 Logistic 假設，我們要如何定義 Logistic Regression 的 Error Function E_in(h)？回顧一下之前學過的方法，Linear Classificaiton 是使用 0/1 Error，Linear Regression 是使用 Squared Error。</p>

<p>Logistic Regression 的目標是找到 h(x) 接近 f(x)＝P(+1|x)，也就是說當 y=+1 時，P(y|x) = f(x)，當 y=-1 時，P(y|x) = 1-f(x)。我們要讓 Error 小，就是要讓 h(x) 與 f(x) 接近。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-6.png">
</p>

<h3 id="使用-likelihood-的概念">使用 Likelihood 的概念</h3>

<p>我們知道資料集 D 是由 f(x) 產生的，我們可以用這樣的思路去想，f(x) 跟 h(x) 生成目前看到的資料集 D 的機率是多少，用這種 Likelihood 的概念，我們只要在訓練過程讓 h(x) 跟 f(x) 產生資料集 D 的機率接近就可以。</p>

<p>不過我們並不知道 f(x)，但我們可以知道 f(x) 產生資料集 D 的機率很高，因為確實就是由 f(x) 產生的。</p>

<p>簡而言之就是產生目前所看到的資料集是一個我們不知道的 f(x)，f(x) 可能會產生出很多種資料集，我們看到的只是其中之一，但 f(x) 是我們無法知道的，我們可以去算 g(x)，從眾多的 g(x) 去算出那一個最可能產生我們目前看到的資料集，這就是去算 max likelihood，之後我們就可以說 g(x) 跟 f(x) 很像～</p>

<p>所以我們只要計算讓 h(x) Likelihood 越高越好。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-7.png">
</p>

<h3 id="將-likelihood-套用到-logistic-regression">將 Likelihood 套用到 Logistic Regression</h3>

<p>所以現在問題轉換成了計算 h(x) 越高越好了，得到最高 likelihood 機率值的就是我們所要的 g(x)。</p>

<p>式子就是 g = argmax likelihood(h)</p>

<p>likelihood(h) 的式子可以描述成 P(x1)h(x1) x P(x2)(1-h(x2)) &hellip; P(xn)h(1-h(xn))</p>

<p>又因為 sigmoid 性質 1-h(x) = h(-x)，所以 likelihood(h) 的式子可以描述成 P(x1)h(+x1) x P(x2)(-h(x2)) &hellip; P(xn)h(-h(xn))</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-8.png">
</p>

<h3 id="將式子做一些轉換">將式子做一些轉換</h3>

<p>likelihood(h)＝P(x1)h(+x1) x P(x2)(-h(x2)) &hellip; P(xn)h(-h(xn))，我們要計算的是 max likelihood，所以  max likelihood(h) 正比於 h(score) 連乘，正比於 theta(yn)theta(wxn) 連乘。</p>

<p>我們取上 ln，不會影響計算結果，所以取 ln 做後續的推導。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-10.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-11.png">
</p>

<h3 id="cross-entropy-error">Cross-Entropy Error</h3>

<p>如果 max Likelihood 轉換成 Error，作為 Logistic Regression 的 Error Function，就會是求 min 的 -ln theta(ynwxn) 連加，代進 sigmoid theta，就會得到 ln(1+exp(-ywx))，這又稱為 Cross-Entropy Error</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-12.png">
</p>

<h3 id="最小化-e-in-w">最小化 E_in(W)</h3>

<p>機器學習的訓練過程就是最小化 E_in(W)，推導出 Cross-Entropy Error 之後，就可以將 E_in(W) 寫成如下圖所示，該函數是連續、可微，並且是凸函數。所以求解最小值就是用偏微分找到谷底梯度為 0 的地方。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-13.png">
</p>

<h3 id="微分過程">微分過程</h3>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-14.png">
</p>

<h3 id="e-in-w-梯度為-0-無法容易求出解">E_in(W) 梯度為 0 無法容易求出解</h3>

<p>和之前的 Linear Regression 不同，這個 ▿Ein(w) 不是一個線性的式子，要求解 ▿Ein(w)=0 是很困難的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-15.png">
</p>

<h3 id="使用-pla-優化的方式最佳化">使用 PLA 優化的方式最佳化</h3>

<p>這種情況可以使用類似 PLA 利用迭代的方式來求解，這樣的方法又稱為梯度下降法（Gradient Descent）</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-16.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-17.png">
</p>

<h3 id="逐步優化">逐步優化</h3>

<p>要尋找目標函數曲線的波谷，可以想像成一個人站在半山腰，每次都選擇往某個方向走一小步，讓他可以距離谷底更近，哪個方向可以更近谷底(greedy)，就選擇往這個方向前進。</p>

<p>所以優化方式就是 wt+1 = wt + ita(v)，v 代表方向（單位長度），ita 代表步伐大小。</p>

<p>找出 min Ein(wt+ita(v)) 就可以完成機器的訓練了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-18.png">
</p>

<h3 id="轉換成線性">轉換成線性</h3>

<p>min Ein(wt+ita(v)) 還是非線性的，很難求解，但在細微的觀點上決定怎麼走下一步，原本的 min Ein(wt+ita(v)) 可以使用泰勒展開式轉換成線性的 min Ein(Wt) + ita(V)▿Ein(w)。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-19.png">
</p>

<h3 id="gradient-descent-梯度下降">Gradient Descent 梯度下降</h3>

<p>所以我們真正要求解的是 v 乘上梯度如何才能越小越好，也就是 v 與梯度是完全相反的方向。想像一下：如果一條直線的斜率 k&gt;0，說明向右是上升的方向，應該要向左走，反之，斜率 k&lt;0，向右走才是對的。如此才能逐步降低 E_in，求得 min Ein。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-20.png">
</p>

<h3 id="ita-參數的選擇">ita 參數的選擇</h3>

<p>解決的方向的問題，代表步伐大小的 ita 也很重要，如果步伐太小，到達山谷的速度會太慢，如果步伐太大，則會很不穩定，E_in 會一下太高一下太低，有可能也會無法到達山谷。理想上在距離谷底較遠時，步伐要大一些，接近谷底時，步伐要小一些，所以我們可以讓步伐與梯度數值成正相關，來做到我們想要的效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-21.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-22.png">
</p>

<h3 id="完整的-logistic-regression-learning-algorithm">完整的 Logistic Regression Learning Algorithm</h3>

<p>先初始化 w0，第一步計算 Logistic Regression 的 Ein 梯度，然後更新 w，wt+1 = wt - ita(V)▿Ein(wt)，直到 ▿Ein(wt+1) = 0 或是足夠多的回合之後回傳最後的 wt+1。（每回合計算的時間複雜度與 Pocket PLA 一樣）</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-23.png">
</p>

<h3 id="總結">總結</h3>

<p>第十講中我們了解了當要用 Logistic Regression 解可能性這樣的問題的時候，我們使用的 sigmoid 來將函數做轉換，因此導出了 cross-enropy error function，最佳化無法直接用偏微分的方式求出解，所以使用了 Gradient Descent 這樣的方式來逐步優化來求出解。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-24.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 9 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2016-01-06T13:29:31&#43;08:00">
        
  January 6, 2016

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/">第八講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在第八講中我們機器學習流程圖裡加入了 error function 及 noise 的概念，並了解在這樣的情況下機器學習還是可行的。前面花了很大的篇幅在說機器為何可以學習，接下來是要說明機器是怎麼學習的。本篇以眾所皆知的線性迴歸為例，從方程式的形式、誤差的衡量方式、如何最小化 Ein，讓我們對線性迴歸在機器學習上的應用有些理解。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-1.png">
</p>

<h3 id="信用卡額度問題">信用卡額度問題</h3>

<p>回到信用卡這個例子，假設我們現在需要的是判定要發多少信用額度給申請者，這種問題的答案就從發不發卡變成了一個實數，這就是線性迴歸的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-2.png">
</p>

<h3 id="線性迴歸">線性迴歸</h3>

<p>將上述問題寫成線性迴歸式就如下圖，其實與 perceptron 很像，但不用再取正負號。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-3.png">
</p>

<h3 id="圖解線性迴歸">圖解線性迴歸</h3>

<p>圖解線性迴歸問題就會如下圖所示，找一條線或超平面來讓所有的資料點與之的差距最小。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-4.png">
</p>

<h3 id="線性迴歸的錯誤衡量">線性迴歸的錯誤衡量</h3>

<p>線性迴歸的錯誤衡量一般是使用平方錯誤，所以在計算 Ein 的過程是算出平均平方錯誤最小的值，而 Eout 的差距就可以用平方錯誤來計算效果。最小的 Ein 如何計算呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-5.png">
</p>

<h3 id="整理成矩陣的形式">整理成矩陣的形式</h3>

<p>我們將 Ein 的式子整理成矩陣的形式，推導過程如下。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-6.png">
</p>

<h3 id="最小的-ein">最小的 Ein</h3>

<p>整理成這種式子的 Ein 理論上已知是個連續、處處可微的凸函數，所以計算最小的 Ein 就是去解每一個維度進行偏微分為 0 所得到的值。他的物理意義就是在這個凸函數中這個點在各個維度都無法再下滑。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-7.png">
</p>

<h3 id="最小的-ein-就是計算-ein-的梯度">最小的 Ein 就是計算 Ein 的梯度</h3>

<p>這就是在計算 Ein 的梯度，我們將原本的式子展開後，並對這個式子做偏微分。對矩陣的偏微分的理解我們可以從 1 個維度慢慢推廣，如此就可知 XTXw - XTy = 0 時可以得到最佳解。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-8.png">
</p>

<h3 id="線性迴歸最佳解">線性迴歸最佳解</h3>

<p>有上所得到的結果，如果 X^TX 是有反矩陣的話，那就可以很容易的算出 (X^TX)-X^Ty 是我們得到的最佳解，其中 (X^TX)-X^T 又稱為 pseudo-inverse，但如果 X^TX 是 singular，那就無法這樣算出一個最佳的 pseudo-inverse，但還是有辦法得出一個 pseudo-inverse，記為 X+，通常我們會用內建的數學方法得出 pseudo-inverse。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-9.png">
</p>

<h3 id="線性迴歸機器學習演算法">線性迴歸機器學習演算法</h3>

<p>如此線性迴歸機器學習演算法就會是這樣，第一步，特徵資料組成一個矩陣 X，答案組成一個向量 y，第二步，計算 X 的 pseudo-inverse，第三步，將 X pseudo-inverse 與 y 做內積，如此就可以得到最佳解 W。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-10.png">
</p>

<h3 id="線性迴歸到底是不是機器學習演算法">線性迴歸到底是不是機器學習演算法</h3>

<p>由於好像是用一個式子就算出答案，所以有人會懷疑線性迴歸到底是不是機器學習演算法，線性迴歸在本質上的確跟遵循機器學習流程，在計算 psedo-inverse 的過程其實就有學習調整的過程，只是我們直接用別人寫好的函式沒辦法看出來，另外，機器學習其實就是想要做到 Eout 好，如果 Eout 好那就是一個機器學習演算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-11.png">
</p>

<h3 id="分析線性迴歸的-ein-與-eout">分析線性迴歸的 Ein 與 Eout</h3>

<p>用一些理論來分析線性迴歸的 Ein 與 Eout，Ein 平均可以推導到 noise level x (1-(d+1)/N)，第一步推導如下圖，我們又稱 XX+ 為 hat matrix H，因為它讓真實的答案 y 變成了預測的答案 y hat，所以就叫 hat matrix H。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-12.png">
</p>

<h3 id="hat-matrix-h-的物理意義">Hat Matrix H 的物理意義</h3>

<p>y hat 物理意義就是 X 的線性組合，原本有很多個 y hat，可以組成一個線性組合空間，迴歸分析就是在算 y - yhat 最小的，Hat Matrix H 就是把 y 投影到 yhat，然後 I-H 可以將 y 線性轉換到 y - yhat，代表真實答案與預測答案的差距。其中數學上有個性質 trace(I-H) = N-(d+1)，這個沒有證明，只有用自由度來說明。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-13.png">
</p>

<h3 id="用這樣的物理意義來證明-noise-level-x-1-d-1-n">用這樣的物理意義來證明 noise level x (1-(d+1)/N)</h3>

<p>我們用這樣的物理意義來證明 noise level x (1-(d+1)/N)，我們可以想成 y 是從 f(x) 加上一些 noise 得出來的，這個 noise 被 I-H 線性轉換到 y-yhat，如就可以計算出 Ein 平均會是 noise level x (1-(d+1)/N)。</p>

<p>有趣的是 Eout 會是 noise level x (1＋(d+1)/N)，但證明很複雜。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-14.png">
</p>

<h3 id="學習曲線">學習曲線</h3>

<p>Ein 跟 Eout 都會隨著 N 越大而收斂，而且 Ein 及 Eout 的差距會被 bound 住，所以也有 VC bound 的性質，因此迴歸分析的確可以達成機器學習效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-15.png">
</p>

<h3 id="線性分類跟線性迴歸">線性分類跟線性迴歸</h3>

<p>分類 y 是 +1 或 -1，迴歸是實數；分類的假設集合要取 sign，迴歸不用；分類是 0/1 錯誤，迴歸是平方錯誤；分類是個 NP-hard 的問題，不容易算出最佳解，但迴歸可以容易算出最佳解，我們可以把分類問題轉成用迴歸來解嗎？</p>

<p>直覺來想，我們可以算出迴歸解之後，再取 sign，那就變成分類模型了，理論上可以嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-16.png">
</p>

<h3 id="觀察分類與迴歸的錯誤衡量">觀察分類與迴歸的錯誤衡量</h3>

<p>觀察分類與迴歸的錯誤衡量我們可以發現迴歸的平方錯誤曲線是壓在分類的0/1錯誤曲線上的，也就是說平方錯誤一定會比較大。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-17.png">
</p>

<h3 id="使用-vc-bound-理論將迴歸套用到二元分類">使用 VC bound 理論將迴歸套用到二元分類</h3>

<p>使用 VC bound 理論將迴歸套用到二元分類，可以知道迴歸錯誤會是上界，如果迴歸可以得到 Ein 小，那 Eout 就會小，因此可以將迴歸套用在二元分類上，這樣會比算 PLA 有效率，雖然效果不一定比較好。</p>

<p>我們也可以將迴歸用在先期計算，先計算出最好的 PLA 初始線，然後再進行 PLA 演算法，如此就可以增加 PLA 的效能。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-18.png">
</p>

<h3 id="總結">總結</h3>

<p>當我們要機器回答實數解的時候，那就是線性迴歸問題。而線性迴歸演算法可以用 pseudo-inverse 很快地算出來，Ein 與 Eout 的差距會隨著 N 越大而收斂，而且在理論上我們可以用線性迴歸來解分類問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-19.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 8 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-12-23T13:08:10&#43;08:00">
        
  December 23, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/">第七講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>在第七講中我們定義了 VC Dimension，就是最大的 non-break point，當 d_vc 是有限的，且資料 N 夠大，Ein 很小的時候，理論上機器學習是可以達成目標的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-1.png">
</p>

<h3 id="重溫機器學習流程圖">重溫機器學習流程圖</h3>

<p>重溫機器學習流程圖，大致的理論我們都已經完備了，但這時又會想，如果資料來源有雜訊（noise）又會如何呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-2.png">
</p>

<h3 id="雜訊-noise-是什麼">雜訊（Noise）是什麼</h3>

<p>雜訊是什麼呢？以之前的銀行發卡的例子來說明，比如該發卡未發卡、不該發卡卻發了卡、或是一開始收集的基本資料就是錯的，這些就會是我們搜集到資料時的雜訊，在有雜訊的情況下 VC bond 還會正常運作嗎？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-3.png">
</p>

<h3 id="用彈珠顏色會改變來代表雜訊來看-vc-bound">用彈珠顏色會改變來代表雜訊來看 VC bound</h3>

<p>之前在推導 VC bound 時是用彈珠來說明，我們可以用不固定顏色的彈珠來代表雜訊推導 VC bound（以 pocket algorithm 可以想成是 o 和 x 在同一線上，所以無法確定 o 或 x，這就是雜訊），也就是資料來源會多了一個 y ~ P(y|x)這個條件，從之前的理論推導中，我們了解了訓練資料跟測試資料都來自同一個資料分佈的話，那 VC bound 就會成立。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-4.png">
</p>

<h3 id="新的機器學習流程圖">新的機器學習流程圖</h3>

<p>所以新的機器學習流程圖就可以容忍雜訊。也因此可以容忍雜訊的 pocket algorithm 就有理論基礎了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-5.png">
</p>

<h3 id="錯誤衡量">錯誤衡量</h3>

<p>接下來探討如何衡量 g 跟 f 是否很接近，我們使用 E_out(g) 來衡量，這個衡量方式有三個特點：1. 使用 out of sample 來衡量 2. 用 point-wise 的方式一個點一個點衡量 3. 如果是二元分類的問題就會使用 0/1 error 來衡量。但其實也不一定要遵照這三個特點，有很多不同的方法被提出來，只是我們同常還是會這樣來做錯誤衡量。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-6.png">
</p>

<h3 id="兩個重要的-pointwise-錯誤衡量">兩個重要的 Pointwise 錯誤衡量</h3>

<p>有兩個重要的 Pointwise 錯誤衡量方式可以記起來，一個是 0/1 錯誤，這個可以用在分類上；一個是平方錯誤，這個通常用在迴歸問題上，不同的錯誤衡量方式會怎麼影響機器學習呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-8.png">
</p>

<h3 id="ideal-mini-target">Ideal Mini-Target</h3>

<p>從下面可以例子，我們可以從資料來源 P(y|x) 及 error function 了解它們如何互相影響，在 0/1 錯誤的情況下，選擇 y=0.2 會得到最小的 error 0.3，但在平方錯誤的情況下，選擇 y=1.9 才會得到最小的 error 0.29，機器學習的過程也會在 P(y|x) 及 error function 的關係去找出 ideal mini-target f(x)。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-9.png">
</p>

<h3 id="再次更新機器學習流程圖">再次更新機器學習流程圖</h3>

<p>所以選擇用什麼 error function 也會影響機器學習的結果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-10.png">
</p>

<h3 id="錯誤衡量的選擇">錯誤衡量的選擇</h3>

<p>錯誤衡量的選擇也還會有其他考量，具體了解一下錯誤，在 0/1 error 這種 error function，會有 false reject 及 false accept 這兩種錯誤，我們會同等的看待這兩種錯誤，只要出錯，那 error 就是 1。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-11.png">
</p>

<h3 id="錯誤有權重-超級市場指紋辨識">錯誤有權重：超級市場指紋辨識</h3>

<p>但實際的世界可能會有這種情況，錯誤是有權重的，我們可以用超級市場指紋辨識給優惠這種例子來說明，如老顧客有優惠，新顧客沒有優惠，如果指紋辨識 false reject 老顧客沒優惠，那就會失去這位老顧客，影響較大，我們可以給這個錯誤較大的權重。但新顧客 false accept 就沒什麼大不了，虧點小錢而已，錯誤的權重應該比較小。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-12.png">
</p>

<h3 id="錯誤有權重-cia-指紋辨識">錯誤有權重：CIA 指紋辨識</h3>

<p>但如果是在 CIA 機密文件的指紋辨識上，那就反過來了，有權限的人 false reject 沒有什麼大不了，但沒權限的人 false accept 那就會出大問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-13.png">
</p>

<h3 id="最後更新機器學習流程圖">最後更新機器學習流程圖</h3>

<p>將這樣的概念套用到機器學習流程圖上面，其實就是在演算法 A 這邊對權重的情況作一些調整。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-14.png">
</p>

<h3 id="將錯誤權重套用到演算法">將錯誤權重套用到演算法</h3>

<p>如何將錯誤權重套用到演算法呢？我們用 CIA 指紋辨識這個例子來說明，我們可以直接將 false accept 的錯誤乘上 1000 倍，但這其實只做了一半。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-15.png">
</p>

<h3 id="更嚴謹一點">更嚴謹一點</h3>

<p>因為資料量沒有變，卻讓 error 直接乘上 1000 倍並不嚴謹，所以我們可以用下面這個方式來想，假設沒權限（false accept 錯誤）的資料複製了 1000 筆相同的資料下去訓練，這樣 false accept 乘上 1000 倍就合理，但實務上我們不會真的複製 1000 筆資料進去，而是在演算法上面多去使用沒權限（false accept 錯誤）的資料做運算的機率大了 1000 倍，這樣就很像虛擬的放大資料。用這樣的方式去調整演算法才比較嚴謹。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-17.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-18.png">
</p>

<h3 id="結語">結語</h3>

<p>在這一講中，我們了解了有雜訊的情況下，機器學習理論上還是可以運作的，並且介紹了錯誤衡量（error measure）的方式也會影響演算法挑選的結果，如果 error measure 有權重的話，那調整演算法時也要嚴謹些，直接在 error function 乘上權重並不夠嚴謹。這一講完備之後，機器學習的基礎理論已經告一段落了，下一講將開始介紹其他機器學習演算法。（我們目前只會 PLA 及 Pocket PLA）</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 7 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-12-09T19:31:24&#43;08:00">
        
  December 9, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/">第六講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<p>在第六講我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，這個上界我們就稱之為 VC Bond。因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果，而且也不會有 Ein 與 Eout 誤差過大的情況發生，而這一講將進一步說明 VC Bond。</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講，我們從 break point 的性質慢慢推導出成長函數是一個多項式，因此我們可以保證 Ein 與 Eout 的差距在 N 資料量夠大的情況下不會因為假設集合的無限累積而差距變得很大。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-1.png">
</p>

<h3 id="將成長函數寫得更簡潔一點">將成長函數寫得更簡潔一點</h3>

<p>目前我們知道假設集合的成長函數是 B(N, k) 這個多項式，實際算出來的值如左圖，其實我們可以直接用 N 的 k-1 次方來表示，實際算出來的值如右圖，總之 B(N, k) 會小於 N 的 k-1 次方，所以我們可以直接用 N 的 k-1 次方來表示成長函數。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-2.png">
</p>

<h3 id="vc-bond-告訴我們什麼">VC Bond 告訴我們什麼</h3>

<p>將 VC Bond 簡化的成長函數 N 的 k-1 次方帶進上一講的霍夫丁不等式裡，式子可以簡化成下圖。這個式子告訴我們：1. 當成長函數有 break point k，也就是說假設集合很好，2. N 的資料量夠大，也就是說有足夠的好資料集，3. 如果我們有一個演算法可以找出一個 g 讓 Ein 很小，也就是說我們有好的演算法，那麼我們大概就可以說我們讓機器可以學到技巧了。</p>

<p>（當然還是要有好運氣，因為有可能壞事情還是會發生，只是機率較低）</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-3.png">
</p>

<h3 id="vc-dimension">VC Dimension</h3>

<p>我們將最大的非 break point 的維度稱之為 VC Dimension。所以 d＿vc 就是 k-1。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-4.png">
</p>

<h3 id="定義好-vc-dimension-之後">定義好 VC Dimension 之後</h3>

<p>定義好 VC Dimension 之後，之前舉的 break point 例子就可以改寫成 vc dimension，其中 2D perceptrons 的 vc dimension 就是 3（break point 是 4）。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-5.png">
</p>

<h3 id="vc-dimension-與機器學習">VC Dimension 與機器學習</h3>

<p>我們回到機器學習的概觀圖，vc dimension 的概念告訴我們，當我們有有限的 d＿vc 時，此時演算法所找出的 g 會讓 Eout 與 Ein 很接近。而且不用管用的演算法 A 是什麼、資料 D 是從哪種資料分佈產生的、也不用管 target f 到底是什麼。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-6.png">
</p>

<h3 id="對應到-2d-pla">對應到 2D PLA</h3>

<p>我們把這樣的概念對應到 2D PLA，2D PLA 會從線性可分的資料集 D 中找出一個 g 讓 Ein 為 0。然後 VC Dimention 告訴我們 2D PLA 的 d＿vc 是 3，在資料集 N 夠大的情況下可以保證 Ein(g) - Eout(g) 的差距很小，因此我們就可以說 2D PLA 的 Eout(g) 也會接近 0，這也就是說我們讓機器學習到技巧了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-7.png">
</p>

<h3 id="vc-dimention-跟-perceptrons-的關係">VC Dimention 跟 Perceptrons 的關係</h3>

<p>我們知道 1D perceptron 的 d＿vc 是 2，2D Perceptron 的 d＿vc 是 3，所以我們會猜，d-D 的 perceptrons 的 d＿vc 會不會是 d+1，理論上證明這樣的想法是對的，證明的方法有兩個步驟，第一步證明 d＿vc &gt;= d+1，第二步證明 d＿vc &lt;= d+1，這邊不詳述證明細節。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-8.png">
</p>

<h3 id="模型複雜度">模型複雜度</h3>

<p>我們將新的霍夫丁不等式做一些轉換，將右式表示成 δ（壞事情發生的機率），那 1-δ 就代表是好事情發生的機率，就就是 Ein(g) - Eout(g) &lt;= ε，我們可以知道 d＿vc 會影響壞事情（Ein 與 Eout 差距大）發生的機率，d＿vc 越高，就代表假設集合模型複雜度越高，但也可能讓壞事情發生的機率變高。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-9.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-10.png">
</p>

<h3 id="模型複雜度圖示">模型複雜度圖示</h3>

<p>從式子中我們可以看出機器學習的模型複雜度關係如下圖，當 d＿vc 上升時，Ein 會下降，但 Ω 會上升，當 d＿vc 下降時，Ein 會上升，但 Ω 會下降，所以 Eout 要最小 d＿vc 一定是在中間，也就是我們的模型複雜度不能太高也不能太低。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-11.png">
</p>

<h3 id="樣本複雜度">樣本複雜度</h3>

<p>從新的霍夫丁不等式我們也可以來算一下樣本複雜度，比如現在我們需要壞事情 Ein 與 Eout 的差距 &gt; ε＝0.1 的機率 &lt;= δ=0.1，且已知 d＿vc=3，這樣我們會需要多少個樣本呢？理論上我們會大概需要 10000 X d＿vc 個樣本點，也就是說 2D perceptrons 大概就需要 30000 個樣本點。但實務上發現只要大概 10 x d＿vc 個樣本點就足夠了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-12.png">
</p>

<h3 id="總結">總結</h3>

<p>在第七講中我們定義了 VC Dimension，就是最大的 non-break point，然後在 perceptrons 這樣的模型中，d＿vc 剛好等於 d+1，物理意義上可以想成可以調整的自由度有幾個維度。引進 VC Dimension 的概念之後，我們可以了解到機器學習的模型複雜度及樣本複雜度關係。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-13.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習基石 Machine Learning Foundations 第 6 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2015-11-21T15:59:16&#43;08:00">
        
  November 21, 2015

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/">第五講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<p>在第五講中我們了解了如何將 PLA 無限的假設集合透過 Dichotomy、Break Point 這樣的方式轉換成有限的集合，在第六講中我們將更進一步去推導其實這個假設集合的成長函數會是一個多項式，如此我們就可以完全相信PLA 機器學習方法的確在數學理論上是可行的了。</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>我們先來回顧一下上一講的內容，在上一講我們知道了成長函數似乎跟 break point 有些關係，這一講我們將慢慢導出這樣的關係其實是一個多項式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-1.png">
</p>

<h3 id="從-break-point-找其他線索">從 break point 找其他線索</h3>

<p>從上一講提到的 break point 我們知道了成長函數至少會小於 2 的 N 次方，且如果 break point 在k取到了，那 k+1, k+2,&hellip; 都會是 break point。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-2.png">
</p>

<h3 id="由例子觀察-break-point">由例子觀察 break point</h3>

<p>這邊林軒田教授用了一連串的例子說明了如果 H 有Break Point k，那當 N 大於 k 時，mH(N) 成長函數會大大縮減（以 binary classification 這個問題來說，所有的 H 為 2 的 N 次方個）。</p>

<p>既然知道 mH(N) 成長函數會大大縮減，那究竟能縮減到什麼程度？會不會是一個多項式的形式呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-3.png">
</p>

<h3 id="bounding-function">Bounding Function</h3>

<p>接下来，引出了 Bounding Function 的概念，Bounding Function 的意義就是為了看在 N 個樣本點的情況下，如果 Break Point 為 K，那 mH(N) 會縮減到什麼程度的一個函式。</p>

<p>值得一提的是，這裡的 Bounding Function B(N, K) 只與 N 和 K 有關，與假設集合 H 無關，這樣在 Binary Classification 這個問題下，無論是 positive intervals 還是 1D perceptrons 他們的 Bounding Function 就都會是一樣的，如果我們能夠得到 Bounding Function 的結果，那就可以推廣到各個 Binary Classification 的問題。</p>

<p>所以我們新的目標就是找出 B(N, K) 是不是小於等於 poly(N) 了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-4.png">
</p>

<h3 id="推導-bounding-function-數學式">推導 Bounding Function 數學式</h3>

<p>從以下投影片的 Dichotomies 例子可以導出 Bounding Function 的一個上界，所以 B(N, k) 會是小於等於 B(N-1, k)+B(N-1, k-1)。（詳細過程可以參考影片）</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-5.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-6.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-7.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-8.png">
</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-9.png">
</p>

<h3 id="bounding-function-定理">Bounding Function 定理</h3>

<p>Bounding Function 的上界出現了我們想要的多項式形式，這裡得出了一個定理，只要 Break Point 存在，那 mH(N) 成長函數一定能夠被一個多項式包含住，我們可以用投影片中的式子來表示 B(N, k)。</p>

<p>事實上式子中的&rdquo;小於等於&rdquo;可以直接是&rdquo;等於&rdquo;，這已有證明，但不在這次課程範圍，有興趣可以去找其他資料來看。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-10.png">
</p>

<h3 id="找出-break-point-就能算出成長函數">找出 break point 就能算出成長函數</h3>

<p>有了 Bounding Function 定理，我們就可以透過找出 break point 來算出假設集合的成長函數，比如上一講中我們說 PLA 2D perceptrons 的成長函數小於 2 的 N 次方，現在我們可以說PLA 2D perceptrons 的成長函數小於等於 B(N, 4)，因為 2D perceptrons 的 break point 出現在 4。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-11.png">
</p>

<h3 id="把-bound-function-的定理引入-hoeffding-不等式">把 Bound Function 的定理引入 Hoeffding 不等式</h3>

<p>有了上面的推導，我們知道 mH(N) 是可以在某種條件下（Break Point k）被 Bound 住。現在回到 Hoeffding 不等式的 union bound 形式，把 Bound Function 的定理概念引入這個 Hoeffding 不等式可以慢慢推導出投影片中下面的式子。</p>

<p>直觀的想法我們可以把 mH(N) 的上界直接代進 Hoeffding 不等式，但理論上我們不能這樣做，因為目前 Eout(h) 是無限個，因此我們需要做一些轉換才能購買 B(N, k) 引入這個 Hoeffding 不等式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-12.png">
</p>

<h3 id="第一步把-eout-h-轉換成有限個">第一步把 Eout(h) 轉換成有限個</h3>

<p>第一步我們需要把 Eout(h) 轉換成有限個，這裡沒有做嚴格的證明，只個了直觀的解釋，假設還有一筆資料 D‘（數量也是 N），得到的結果為 Ein‘，那 Ein 與 Eout 發生 BAD（差距很大），應該跟 Ein 與 Ein‘ 發生 BAD 的情況接近。</p>

<p>所以這個無限的 Eout(h) 就轉換成有限個的 Ein‘(h) 了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-13.png">
</p>

<h3 id="第二步整理式子中的成長函數-mh-n">第二步整理式子中的成長函數 mH(N)</h3>

<p>利用第一步的结果，再直觀想像一下：為了產生 Ein‘ 我們多了 N 個樣本點，所以成長函數中的 N 就變成了 2N 了。</p>

<p>我們可以想像就樣的結果其實就是我們用了 Bound Function 這個定理，將原本無限的 Union Bound 限縮在一個 BAD overlap 的空間。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-14.png">
</p>

<h3 id="第三步代進-hoeffding-不等式">第三步代進 Hoeffding 不等式</h3>

<p>接下來代進 Hoeffding 不等式就完成了。這樣的轉換就像是 2N 個樣本，現在抽了 N 的樣本出來，這 N 個樣本跟原來 2N 個樣本的真實情況相比發生 BAD 的機率會是多少。這樣還是一個 Hoeffding 不等式，只是就像罐子變小、誤差變小、不放回抽樣的 Hoeffding 不等式。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-15.png">
</p>

<h3 id="vapnik-chervonenkis-vc-bound">Vapnik-Chervonenkis（VC）bound</h3>

<p>這最終的式子就是 Vapnik-Chervonenkis（VC）bound，它將 Eout 用  Ein&rsquo; 來代換，然後將假設集合專換成分類（使用 Bound Function 證明為一個多項式），然後再代進 Hoeffding 不等式。</p>

<p>在 2D perceptrons 中，break point k=4，mH(N) 是 O(N3)，由 VC bound 可知 2D perceptrons 是可以從數據中得到學習效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-17.png">
</p>

<h3 id="總結">總結</h3>

<p>從這一講中，我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果的。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-16.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
            
  <div class="pagination-bar">
    <ul class="pagination">
      
        
          <li class="pagination-prev">
            <a class="btn btn--default btn--small" href="https://blog.fukuball.com/tags/machine-learning/page/3/">
              <i class="fa fa-angle-left text-base icon-mr"></i>
              <span></span>
            </a>
          </li>
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="https://blog.fukuball.com/tags/machine-learning/page/5/">
              <span></span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
      
      <li class="pagination-number"> </li>
    </ul>
  </div>


          </section>
        
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Fukuball. 
  </span>
</footer>

      </div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
    
    <h4 id="about-card-name">Fukuball</h4>
    
      <div id="about-card-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Co-Founder / Head of Engineering at OurSong
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Taipei, Taiwan
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center"></div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-23smart-contract-%E5%88%9D%E6%8E%A2%E5%BE%9E-bytecode-%E5%88%B0-solidity/">
                <h3 class="media-heading">Ethereum 開發筆記 2–3：Smart Contract 初探，從 Bytecode 到 Solidity</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Ethereum 上的 EVM（Ethereum Virtual Machine）可以執行程式，而 EVM 上的可執行程式基本上是 Bytecode 的形式，所以所謂的 Smart Contract 就是存放在 Ethereum 上的 Bytecode，然後可由 EVM 來執行。
Bytecode Smart Contract 直接用 Bytecode 寫 Smart Contract 我們來嘗試一下直接用 Bytecode 來寫 Smart Contract，以下這段程式碼主要內容是執行運算後，將運算結果存放在 0 這個位置：
PUSH1 0x03 PUSH1 0x05 ADD // 3 + 5 -&gt; 8 PUSH1 0x02 MUL // 8 * 2 -&gt; 16 PUSH1 0x00 SSTORE // 將 16 存到 0 這個位置  這段程式轉成 Bytecode 就是：
0x60 0x03 0x60 0x05 0x01 0x60 0x02 0x02 0x60 0x00 0x55  也就是：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-22geth-%E5%9F%BA%E7%A4%8E%E7%94%A8%E6%B3%95%E5%8F%8A%E6%9E%B6%E8%A8%AD-muti-nodes-%E7%A7%81%E6%9C%89%E9%8F%88/">
                <h3 class="media-heading">Ethereum 開發筆記 2–2：Geth 基礎用法及架設 Muti-Nodes 私有鏈</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">要連上 Ethereum 就需要安裝 Ethereum Node，在這邊我們選擇使用 Geth 來安裝 Ethereum Node，接下來就來一步一步的學學怎麼使用 Geth，甚至如何使用 Geth 來架設自己的 Ethereum 私有鏈。
安裝環境 首先我們在 AWS 上開啟兩台 Ubuntu 虛擬機器，記得開 t2.medium（2 vCPU, 4 GB RAM）這個規格以上才跑得動，硬碟可以開 100 G，Security Group 將 TCP 30303 打開，Ethereum Node 之間是用 30303 這個 port 來溝通的。
接下來使用以下指令安裝 Geth：
$ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:ethereum/ethereum $ sudo apt-get update $ sudo apt-get install -y ethereum  兩台虛擬機器都要安裝，應該幾分鐘就可以裝好了。
使用 Main Net 安裝完 Geth 之後，我們就可以透過 Geth 連上 Ethereum Network 了，我們就來連上 Main Net 看看：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-21ethereum-%E9%96%8B%E7%99%BC%E6%95%B4%E9%AB%94%E8%84%88%E7%B5%A1/">
                <h3 class="media-heading">Ethereum 開發筆記 2–1：Ethereum 開發整體脈絡</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在第一次接觸 Ethereum 應用程式開發時，會發現有各式各樣工具，不知要從何下手，我們用一個圖來說明一下與 Ethereum 互動時的整體脈絡及這之間的工具主要做了什麼事，了解之後自己就可以挑選開發時、甚至使用在產品上時要用什麼適合的工具了。
要在自己的機器接上 Ethereum 首先需要安裝 Ethereum Node，我們之前安裝的 Mist 其實就會在我們的機器上安裝 Ethereum Node 並同步帳本，而像這樣安裝 Node 並同步帳本甚至進行挖礦的軟體有很多，大家可以去選擇適合自己使用的。Mist 其實是將一個叫 geth 的軟體用 GUI 包裝起來，如果是開發者的話，可以選擇直接安裝 geth。
geth 提供了許多 API 指令可以讓我們跟 Ethereum 做互動，但有時下指令並不是那麼親和，所以 geth 提供了 RPC(Remote Procedure Calls) 與 IPC(Inter-process Communications) 兩種方式來與 geth 互動，如果你要在 local 機器連上 geth，那就可以使用 IPC；如果要讓遠端連上 geth，那就使用 RPC，可以開 HTTP 或 Web Socket 兩種方式來讓遠端使用。
以上就是 Ethereum 應用程式開發的基礎環境，接下來跟開發網頁應用程式一樣，Ethereum 應用程式也分成後端與前端，後端程式就是 Smart Contract，前端程式就是 Dapp。Smart Contract 可使用 Solidity 撰寫，目前也有許多其他語言可以撰寫 Smart Contract。Smart Contract 要在 Ethereum 上的 EVM 執行要先 Compile 成 Byte Code 之後，再透過 IPC 或 RPC 發佈到 Ethereum 上。前端程式的 Dapp 可用 Web3 JavaScript 透過 RPC 接上 Ethereum，以及使用網頁應用常用到的 HTML、CSS、JavaScript 製作成使用者互動介面，如此就能執行發佈在 Ethereum 上 Smart Contract 所提供的一些程式功能了。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-18ethereum-%E7%9A%84%E7%8D%8E%E5%8B%B5%E6%A9%9F%E5%88%B6/">
                <h3 class="media-heading">Ethereum 開發筆記 1–8：Ethereum 的獎勵機制</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Bitcoin 的獎勵機制基本上是挖到新區塊的節點獲得記帳權及獎勵，Ethereum 大體也是遵循這樣的概念，但做了一些調整與變化，讓我們整個脈絡了解一下。
由於 Blockchain 是一種去中心化的系統，所有的礦工（節點）可以同時挖礦（計算合法 hash），彼此獨立運作，所以極有可能出現兩的礦工同時發現不同的滿足條件的區塊，如此就會產生我們之前有提過的分叉（Fork）。
那我們該採用誰的區塊當主鏈呢？我們會先依工作量最大的區塊為主鏈，如果工作量一樣，就看誰先接了子區塊，一般來說只有成了主鏈的區塊才能獲得獎勵。但這樣沒有變成主鏈的區塊之前的算力就都白費了，所以 Ethereum 創造了 Uncle Block（叔塊）這樣的概念，不能成為主鏈的區塊如果後來被收留成為 Uncle Block，那這些沒有成為主鏈的區塊也有機會可以做為 Uncle Block 而獲得獎勵。
這就是 Ethereum 共識機制中的 GHOST（Greedy Heaviest Observed Subtree）協議，Ethereum 會這樣設計的原因，是由於 Ethereum 產生區塊的速度較快，也因此較容易產生分叉，也會使得新區塊較難以在整個網絡傳播，這對於傳播速度較慢的區塊並不公平。且分叉後的區塊可能在幾個區塊之後整併起來，我們會發現裡面的交易可能會與主鏈一致（雖然單獨查看分塊交易內容不同，不過數個區塊整體一起看交易內容就一致了），符合這種條件的分叉區塊我們就會納入主鏈參考，這些區塊就成了所謂的 Uncle Block，這某種角度也是更確認了 Blockchain 上的交易內容一致，因此 Uncle Block 也有貢獻，應該給予獎勵。
以上我們已經了解了 Ethereum 上的區塊大致分成兩種，普通區塊和 Uncle Block，Ethereum 對這兩種區塊的獎勵方式是不同的。我們分別來看一下。
普通區塊獎勵  固定獎勵 5 ETH 區塊內所有的 Gas Fee 如果區塊納入了 Uncle Block，那每包含一個 Uncle Block 可以得到固定獎勵 5 ETH * 1/32，也就是 0.15625 ETH，一個區塊最多隻能包含 2 個 Uncle Block，也因此不會無限延伸，同時又可鼓勵區塊納入 Uncle Block，增加交易內容的一致性。  Uncle Block 獎勵  用公式計算：（Uncle Block 高度 + 8 - 包含此 Uncle Block 的區塊的高度）* 普通區塊固定獎勵 / 8  我們用個實例來看一下獎勵怎麼算。首先我們來看一個普通區塊：https://etherscan.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-17blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E6%80%A7%E8%B3%AA/">
                <h3 class="media-heading">Ethereum 開發筆記 1–7：Blockchain 的一些重要性質</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">我們這邊再次總結一下 Blockchain 中幾點較重要的性質，包含共識機制、不可竄改、經濟激勵三項。
共識機制（Consensus） 在分散式系統中，我們需要有一套用於協同合作的共識機制來組織行動，但有時候系統中的成員可能會出錯或是故意傳送出錯誤的資訊，而使得網路中不同成員對於全體協作的策略得出不同的結論，進而破壞系統的一致性，這就是所謂的拜占庭將軍問題。
拜占庭將軍問題（Byzantine Generals Problem） 拜占庭將軍問題這個故事是這樣的：
 一組拜占庭將軍分別各率領一支軍隊共同圍困一座城市，這個敵人雖不比拜占庭帝國，但也足以抵禦 5 支拜占庭軍隊的同時襲擊。這 10 支軍隊在分開的包圍狀態下，他們任 1 支軍隊單獨進攻都毫無勝算，除非有至少 6 支軍隊（一半以上）同時襲擊才能攻下敵國。他們分散在敵國的四周，依靠通信兵騎馬相互通信來協商進攻意向及進攻時間。困擾這些將軍的問題是，他們不確定他們中是否有叛徒，叛徒可能擅自變更進攻意向或者進攻時間。在這種狀態下，拜占庭將軍們才能保證有多於 6 支軍隊在同一時間一起發起進攻，從而贏取戰鬥？
 上述的故事對映到電腦系統裡，將軍便成了電腦，而通信兵就是通訊系統。叛徒發送前後不一致的進攻提議，被稱為「拜占庭錯誤」，而能夠處理拜占庭錯誤的這種容錯性稱為「Byzantine Fault Tolerance」。Blockchain 上的共識機制通常具有容錯的設計來達成一致性，主要比較常見的共識機制方法有兩個，「工作量證明」以及「股權證明」兩種方法。
工作量證明演算法（Proof of Work, PoW） 中本聰在 Bitcoin 中創造性的引入了「工作量證明」（俗稱挖礦）來解決拜占庭將軍問題，顧名思義，工作量證明就是用來證明你做了一定量的工作，可用工作成果來證明完成相應的工作量。其中的工作技術原理可以看之前這篇文章：Ethereum 開發筆記 1–4：Blockchain 技術原理簡介
由於工作量證明具相當高的計算成本，因此無誘因去偽造，只有遵守協議約定，才能夠回收成本並獲得收益，也因此減少了叛徒的產生，減少拜占庭錯誤。
股權證明演算法（Proof of Stake, PoS） 股權證明的出現，主要是希望取代工作量證明，進而減少「挖礦」的大量運算。它與工作量證明不同地方在於：工作量證明中，大家比的是「算力」（運算能力），透過大量運算得出符合難度的 Hash 值，進而得到獎勵；而在股權證明，大家比拼的是「股權」，「股權」越大的人（節點）越大機會負責產生新區塊，進而得到獎勵。
舉例來說，在股權證明系統中所有擁有股權（此 Blockchain 的數位貨幣）的人都有機會被挑選為產生新區塊（也就是記帳）的人，擁有更多股權的人被選中的機率越大。假這這個系統中共有三個人：Alice 持有 50 股、Bob 持有 30 股、Cathy 持有 20 股，那每次 Alice 被選為記帳人的機率會是 Cathy 的兩倍。所以股權證明會驅使人們購買更多的股權，進而增加獲選為記帳人的機率，以買股權來代替挖礦，同樣需要付出高成本，也因此可以減少叛徒的產生，減少拜占庭錯誤。
不可竄改（Immutability） Blockchain 不可竄改的性質主要來自資料結構及 hash 方式的設計，讓資料的順序緊密鏈結，若從中竄改了某些資料，那之後的鏈結 hash 都會發生錯誤，形成了 Blockchain 不可竄改的特性。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-16blockchain-%E7%9B%B8%E9%97%9C%E7%9A%84%E5%8A%A0%E5%AF%86%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98/">
                <h3 class="media-heading">Ethereum 開發筆記 1–6：Blockchain 相關的加密基礎知識</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Blockchain 裡應用了一些加密技術來保證及驗證交易訊息的正確性，這也更加強了 Blockchain 資料不可竄改的特性。我們來介紹其中比較重要的「公私鑰加密」以及「Merkle Tree」加密樹。
公私鑰加密 公私鑰加密算法是目前資訊通訊安全的基石，它保證了加密訊息不可被破解，相關的加解密原理大家可以參考這兩篇文章：
 RSA算法原理（一）http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html RSA算法原理（二）http://www.ruanyifeng.com/blog/2013/07/rsa_algorithm_part_two.html  加密與解密 公私鑰加密方法是一種非對稱式加密，透過公鑰加密過後的訊息只有私鑰可以解密，也因此只要保護好私鑰就能保證資訊的安全。
現在假設 Alice 要傳一個訊息給 Bob，希望訊息加密過後只有 Bob 可以解密，大概會經過如下步驟：
 Bob 傳他的公鑰給 Alice Alice 使用 Bob 的公鑰加密訊息 Alice 將加密過後的訊息傳給 Bob Bob 用他的私鑰解密訊息  我們這邊使用 openssl 來練習一下加密與解密，首先我們來產生一對公私鑰：
// Create RSA private key $ openssl genrsa -des3 -out rsa-key.pem 2048 // Create public key $ openssl rsa -in rsa-key.pem -outform PEM -pubout -out rsa-key-pub.pem  其中 rsa-key.pem 就是私鑰，rsa-key-pub.pem 為公鑰，私鑰會要求設置密碼，請妥善記下密碼。
我們先用 rsa-key-pub.pem 加密資料：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-15blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%BE%A9%E8%88%87%E5%90%8D%E8%A9%9E/">
                <h3 class="media-heading">Ethereum 開發筆記 1–5：Blockchain 的一些定義與名詞</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在 Ethereum 開發筆記 1–4 應該已經將 Blockchain 的技術原理說明得很清楚了，不過如果要向一般大眾簡單說明 Blockchain 是什麼，要怎麼說呢？我會說：Blockchain 就是一個分散式帳本，大家都有一樣的帳本，大家都可一起參與記帳，且記完帳大家的帳本就會自動更新到最新版本，而帳裡的紀錄都會分塊並用密碼按順序鏈結起來，用以驗證帳的正確性，如果中間有人改了資料，那後面的鏈結密碼都會發生錯誤，因此沒有人可以亂改帳，這就是 Blockchain。
但 Blockchain 這個名詞還包含了許多概念與內涵，我們之前說過，Blockchain 是因為分散式去中心化帳本的發展而慢慢產生出來的，這樣慢慢被統稱出來的名詞裡底下也就會包含了許多內涵，很難用三言兩語來說明，所以有一些 Blockchain 相關的定義與名詞我們都可以了解一下，這樣就能更了解 Blockchain。
交易（Transaction） 交易是 Blockchain 帳本中的原子單位，如果將交易再往下拆分就會變得沒有意義，比如下列就是一個交易：
 A 減少了 $10 B 增加了 $9 C 增加了 $1  如果只看 1，我們就會想那減少的 $10 到哪裡去了？所以 1、2、3 一起看才算是一個交易。
Blockchain 是一個分散式帳本（Distributed Ledger） 不像銀行依靠自己的帳本來記帳，Blockchain 提供了可靠的分散式帳本，當銀行之間要進行交易時，會需要一個受信任的第三方來進行銀行之間的交易，這也是為何你在做跨國轉帳時，需要付出高昂的手續費以及等待數天處理交易，Blockchain 可靠的分散式帳本讓跨國交易可以在幾分鐘甚至幾秒之內完成，這也是為何銀行想要應用 Blockchain 在金融交易上以降低交易成本。
Blockchain 是一個資料結構（Data Structure） 通常 Blockchain 的資料結構如下組成：
 交易是原子單位 區塊是由一系列的交易組成 區塊鏈由排序良好的區塊所組成  Blockchain 會有分叉（Fork） 當有兩名礦工 A 及 B 幾乎在相同時間內算出了合法的 hash，這兩個區塊傳播到鄰近節點時，有些節點收到了 A 的區塊，有些節點收到了 B 的區塊，這兩個區塊都可以是主鏈的延伸，這時就會產生區塊鏈分叉。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-14blockchain-%E6%8A%80%E8%A1%93%E5%8E%9F%E7%90%86%E7%B0%A1%E4%BB%8B/">
                <h3 class="media-heading">Ethereum 開發筆記 1–4：Blockchain 技術原理簡介</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前我們簡單地介紹過 Blockchain 了，但我們還是對 Blockchain 背後的技術原理不是那麼了解，我們知道 Blockchain 是因為一個數位貨幣帳本這樣的概念被創造出來的，而數位貨幣最擔心的是什麼問題呢？其實就是雙重支付（Double-Spending）這樣的問題。
數位貨幣不像實體貨幣，數位資產比起實體資產容易複製，也因此如果花用數位貨幣的行為如果沒有處理好，就會產生憑空多出其他交易，這就像是偽鈔一樣，會造成通貨膨脹而導致貨幣貶值，讓人不再信任並願意持與流通。因此數位貨幣的支付通常需要一個受信任的第三方來做驗證，這樣的做法雖然簡單，卻存在單點脆弱性，只要這第三方受到攻擊或是監守自盜也一樣會讓這個數位貨幣變成一個失敗的貨幣。
分散式去中心化帳本能解決單點脆弱性的問題，但在驗證正確性這點難度卻很高，所有的節點都有記帳的權利，要如何確定由誰來記帳、記的帳對不對？如果無法確定帳是對的，那就存在雙重支付的風險。
為了改善單點脆弱性及雙重支付這樣的問題，許多分散式的雙重支付防範方法慢慢被提出來，中本聰提出了去中心化（以受信任第三方為中心）的方法來展示解決雙重支付問題，並實作出了 Bitcoin，使用共識機制來解決記帳及驗證的問題，這帶來去中心化數位貨幣帳本的成功。
Bitcoin 的共識協議主要由「工作量證明」（Proof-of-Work, PoW）和「最長鏈機制」兩部分組成，Bitcoin 上的各個節點就是透過共識機制中的工作量證明來決定誰有記帳權，然後取得記帳權的節點就能將新的區塊記帳加到最長鏈上並給予該節點獎勵（新區塊獎勵及交易費收益）。
Bitcoin 的 工作量證明大概會做以下的事情：
 收集還未記到帳上的交易 檢查每個交易中付款地址有沒有足夠的餘額 驗證交易是否有正確的簽名 把驗證通過的交易信息進行打包（組成 Merkle Tree） 為自己增加一個交易紀錄獲得 Bitcoin 獎勵金 計算合法的 hash 爭奪記帳權  計算合法 hash 的方式請見下方影片說明，個人覺得這個影片是目前將 Blockchain 加密機制說明得最清楚的影片。我這邊簡略說明一下，合法的 hash 公式大致看起來像這樣：hash(交易內容+交易簽名+nonce+上一個區塊的 hash)，我們要取得記帳權，就需要找出前面開頭有 N 個 0 的 hash，由於交易內容、交易簽名及上一個區塊的 hash 都是不可變的，所以每個節點就是不斷的調整 nonce 來計算得出不同的 hash，直到找到開頭 N 個 0 的 hash 為止，第一個找的節點就能獲得記帳權，而其他的節點只要計算 hash 對不對就能驗證這個帳對不對。其中 N 個 0 開頭的 hash 就代表了計算的難度，越多 0 代表越難找到這樣的 hash，也因此可以調整計算難度。就是這樣的設計解決了去中心化分散式系統驗證資料及決定記帳順序的難題，也就改善了數位貨幣單點脆弱性及雙重支付的問題。
  以上的內容看完應該就能大體了解 Blockchain 的原理了，甚至要自己做一個 Blockchain 都沒問題！了解了 Blockchain 的技術原理之後，應該能更信任去中心化的數位貨幣的安全性，或許有天大家都信任了去中心化的數位貨幣我們就真的能廣泛使用數位貨幣，為經濟活動帶來更有效率的流通。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98%E7%B7%B4%E7%BF%92-1%E4%BD%BF%E7%94%A8-mist-%E7%99%BC%E8%A1%8C%E8%87%AA%E5%B7%B1%E7%9A%84-token/">
                <h3 class="media-heading">Ethereum 開發筆記練習 1：使用 Mist 發行自己的 Token</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前說過，Blockchain 基本上是因為金流帳本這樣的問題而被創造出來的，也就是說區塊鏈非常適合運用在金流的應用上，我們也可以建立自己的 Blockchain 來搭建自己的金流系統，不過在 Ethereum 上 Smart Contract 這種設計讓我們擁有可以在 Ethereum 區塊鏈上創造自己金流系統的能力，如此我們就不需要自己建一條鏈了。
我們使用 Smart Contract 仿造貨幣性質創造了數位資產（說穿了其實就是在 Smart Contract 上紀錄的變數而已），而這種具貨幣性質的數位資產又被稱作 Token，如此我們就可以在應用程式中使用這個去中心化的金流系統，由於 Token 的應用很普遍，大部分的功能都已經標準化了，我們只要仿造標準來實作就可以發行自己的數位貨幣了。
在這邊我們就練習一下怎麼使用 Mist 發佈 Token Smart Contract 來發行自己的數位貨幣。（目前我們還沒有學習過如何撰寫 Smart Contract，因此這邊會先直接提供範例程式碼，實作的部分我們之後再慢慢學習）
以下是我們的範例程式碼：
 請打開 Mist，如下圖點擊 Contract，然後點擊 Deploy New Contract。
你會看到如下圖的頁面，請在 Solidity Contract Source Code 中貼上我們上面提供的範例程式碼。
貼上範例程式碼之後，Mist 會自動編譯程式，檢查是否有語法上的錯誤，如果沒問題，右方的 Select Contract to Deploy 就會出現選項，在這邊我們選擇 Token ERC 20。
選擇 Token ERC 20 之後，右方會出現要初始化 Contract 的參數表單，有 Initial supply、Token name、Token symbol 需要填寫。Initial supply 代表 Token 的總發行量是多少，我這邊設定成 7777777777，你可以設成你想要的數字。Token name 就是這個 Token 要叫什麼名字，這邊我設定成 7 Token，你想要取 Dog Coin 或是 Cat Coin 也都可以。Token symbol 就是這個 Token 要用什麼代號，像是美金就是用 $、Ether 是用 ETH，這邊我設定成 7token，你可以取自己覺得帥的代號。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-13%E4%BD%BF%E7%94%A8-mist/">
                <h3 class="media-heading">Ethereum 開發筆記 1–3：使用 Mist</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Mist 跟前回介紹的 MetaMask 一樣是可以與 Ethereum 進行互動的工具，除了可以管理 Ethereum 相關密鑰之外，Mist 還包含了 Ethereum 節點以及網頁瀏覽器，方便大家瀏覽 Dapp 網頁。
首先請到這邊安裝 Mist，請選擇適於自己的作業系統安裝。
由於 Mist 會安裝節點在你的電腦裡，也因此會同步整個帳本下來，所以會花上不少時間同時也會佔用許多硬碟空間。我們目前僅是要使用測試鏈，所以請切換到 Ropsten 測試鏈（如下圖），這樣就不用花這麼多時間與空間了。
在 Mist 的左下角可以觀察目前已同步到你的電腦的區塊數（如下圖），如果這個數字跟 Etherscan（Etherscan 是一個可以查看 Ethereum 區塊鏈所有交易的網站） 上的最新區塊數一致的話，那就代表已經同步完成了。
接下來讓我們用 Mist 開一個 Ethereum 帳戶，請點擊 Add Account，並依指示輸入密碼後創建帳號，密碼請務必要記下來，將來交易時都會需要輸入你的密碼。
學會創建 Ethereum 帳戶之後，我們要來看一下 Mist 要怎麼備份帳號，請點擊 Mist 上方選單的 File -&gt; Backup -&gt;Accounts（如下圖），這樣就會打開帳號存放的資料夾，所有的帳號都會加密存在這邊，所以只要備份這些檔案及當時設定的密碼，你就可以在別台電腦復原你的帳號。
現在你這個 Ethereum 帳戶還沒有任何 Ether，我們仿造之前用 MetaMask 來跟水龍頭要 Ether 的步驟來取得 Ether 看看。
我個人提供了一個水龍頭 Dapp，請前往這個網址來取得 Ether：https://blog.fukuball.com/dapp/faucet/
由於 Mist 也是一個 Dapp 網頁瀏覽器，請在 Mist 上方的網址列輸入：https://blog.fukuball.com/dapp/faucet/
Mist 在揭露你的 Ethereum 帳戶資訊給 Dapp 網頁時都會詢問你的同意，請先選擇要瀏覽這個 Dapp 網頁的帳號（你可能在 Mist 有多個帳號，所以就需要選擇目前要用哪個帳號瀏覽這個網頁）。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero=""
         data-message-one=""
         data-message-other="">
         76 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://blog.fukuball.com/images/ok.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://blog.fukuball.com/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>






<script async src="https://www.googletagmanager.com/gtag/js?id=UA-41911929-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-41911929-4');
</script>

    
  </body>
</html>

