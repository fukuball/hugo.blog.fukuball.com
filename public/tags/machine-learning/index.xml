<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on I am Fukuball</title>
    <link>https://blog.fukuball.com/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on I am Fukuball</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Mar 2018 08:16:40 +0800</lastBuildDate>
    
	<atom:link href="https://blog.fukuball.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>基於 LSTM 深度學習方法研發而成的張雨生歌詞產生模型，致敬張雨生</title>
      <link>https://blog.fukuball.com/ji-yu-lstm-shen-du-xue-xi-fang-fa-yan-fa-er-cheng-de-zhang-yu-sheng-ge-ci-chan-sheng-mo-xing-zhi-jing-zhang-yu-sheng/</link>
      <pubDate>Tue, 27 Mar 2018 08:16:40 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/ji-yu-lstm-shen-du-xue-xi-fang-fa-yan-fa-er-cheng-de-zhang-yu-sheng-ge-ci-chan-sheng-mo-xing-zhi-jing-zhang-yu-sheng/</guid>
      <description>之前看到〈『致敬赵雷』基于TensorFlow让机器生成赵雷曲风的歌词〉這篇文章覺得非常有趣，因此一直都想自己動手試試看，中國有趙雷，那台灣要找什麼值得紀念的音樂人來作這個歌詞機器學習模型呢？我想張雨生應該會是台灣非常值得令人紀念的音樂人之一了。
程式的基礎我使用了之前在 GitHub 上有點小小貢獻的一個 Project 作為程式碼基礎，這個 Project 是 char-rnn-tf，可以用於生成一段中文文本（訓練與料是英文時也可以用於生成英文），訓練語料庫我收集了張雨生的百餘首歌詞（包含由張雨生演唱或作曲的歌詞），由於這樣的歌詞語料還是有些不足，因此也加入了林夕、其他著名歌詞、新詩作為輔助，整個語料庫大致包含 74856 個字、2612 個不重複字（其實語料庫還是不足）。
演算法基本上就是 LSTM，細節在此就不多加著墨，若有興趣可以在這篇文章了解一下，沒有時間的人，也可以看看 char-rnn-tf 這個 Project 作者所做的這張圖（見下圖），對概念了解一下。
相關程式碼我放在這邊：Tom-Chang-Deep-Lyrics，如何安裝環境、如何訓練、如何生成歌詞，基本上都寫在 Readme 了，大家可以前往瞧瞧。
歌詞產生結果 範例一：夢想 訓練完模型之後（用 macbook air 大致上需要 1 天的時間），由於大眾對張雨生歌詞的印象應該就是「我的未來不是夢」，因此我首先使用「夢想」作為 seed，結果產生歌詞如下：
夢想會有心 我不願再區福　也不是一種把你一樣偷偷 我的心中有無奈 在我的心裡流　你的身影　你的轉身　你的沈靜　框進畫裡印象派的意 我有個朋友聽我說故舊　這一路悠揚的街長 我是天堂飄輝在天空裡 期待愛人看不同的眼睛 我等待與你身邊 你的歡念　你的灑明　在我心底都是飄逸水墨 我想你　愛過了我的一切 為你一起孤定我的美麗  產生的結果，歌詞機器學習模型先把詞補成句子「夢想會有心」，其實補得蠻好的啊！
「我不願再區福　也不是一種把你一樣偷偷 我的心中有無奈」
這邊雖有錯字，但也不至於不能理解。
「在我的心裡流　你的身影　你的轉身　你的沈靜　框進畫裡印象派的意」
這裡則結合了一首新詩，自創了歌詞。&amp;rsquo;
「我有個朋友聽我說故舊　這一路悠揚的街長」
這一句歌詞結合了張雨生的歌曲永公街的街長，說明歌詞機器學習模型的確有張雨生的影子，但悠揚的街長感覺怪怪的 XD
範例二：我的未來不是夢 從上一個範例，我們可以了解這個歌詞機器學習模型的效果還算不錯，且看起來比起〈『致敬赵雷』基于TensorFlow让机器生成赵雷曲风的歌词〉這篇所產生的歌詞還要好，仔細看趙雷歌詞產生的結果就會覺得歌詞有點不知所云，而我這邊訓練完的結果，看起來語意會比較明確一些。
接著上個範例，我們來試試看「我的未來不是夢」作為 seed。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 16 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-16-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 12 Feb 2018 10:24:38 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-16-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 15 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講中我們學到了如何使用矩陣分解方法來解推薦問題，機器學習技法課程也到這邊告一段落了，這一講終將會總結回顧一下我們在機器學習技法中學到的所有機器學習演算法，也許還有許多算法沒有介紹到，但基本概念都可以延伸。
特徵技巧：Kernel 我們學習到了如何使用 Kernel 來表現資料特徵，使用到 Kernel 技巧的相關演算法如下：
特徵技巧：Aggregation 我們也可以使用 Aggregation 方法來結合資料特徵，藉以合成更強大的學習演算法，使用到 Aggregation 技巧的相關演算法如下：
特徵技巧：Extration 我們可以使用 Extration 技巧來取得重要的資料特徵，使用到 Extration 技巧的相關演算法如下：
特徵技巧：Low-Dim 我們也會使用降維這個特徵技巧來取得資料的重要特徵，用到降維技巧的相關演算法如下：
優化技巧：Gradient Decent 在類神經網路大量用到了 Gradient Decent 技巧來進行 Error 優化，用到 Gradient Decent 技巧的相關演算法如下：
優化技巧：Equivalent Solution 在許多困難的問題，我們很難找到優化的方法，我們會使用 Equivalent Solution 找到優化的方法，例如 Dual SVM 我們使用 covex QP、Kernel LogReg 我們用 representer、PCA 我們用 eigenproblem 來解。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 15 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-15-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 12 Feb 2018 09:03:17 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-15-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 14 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講介紹了 RBF Network，基本上就是透過距離公式及中心點來對資料點進行投票的一個算法，這一講將介紹矩陣分解系列的算法。
推薦系統問題 之前的課程中曾經提到過推薦系統的問題，我們的資料集是使用者對電影的評分，希望讓機器學習算法學習到可以推薦使用者也會高評分的電影，這樣的問題 Netflix 曾經舉行過競賽。我們如何解這樣的問題呢？
類別編碼 這個問題首先會需要進行編碼，因為使用者資料可能只是一連串的使用者編號，這是類別資料，不太能用來直接用於運算（僅有 Decision Tree 可以直接用來做類別運算），所以我們會先將類別資料編碼成數值資料，編碼的方法常用 binary vector encoding，如下所示：
特徵選取 我們可以將使用者評分電影的過程視做一組特徵轉換，能夠將 X 轉換成 Y，轉換的過程如果分成兩個矩陣 Wni、Wim，那左邊的矩陣代表的意義就是使用者對電影中哪些特徵很在意，右邊的矩陣代表的意義就是電影中有哪些特徵成份。
矩陣分解 因此這個推薦問題可以寫成底下的矩陣分解，首先我們把評分做成一個 Rnm 矩陣，需要嘗試把它分解成 VT W 兩個矩陣，使用者的喜好會對映一組特徵，電影的成分也會對應到這組特徵，我們要將這組特徵萃取出來。
矩陣分解學習 Alternating Least Squares 矩陣分解學習算法如下，首先決定特徵維度 d，然後隨機初始化使用者對特徵的喜好 Vn，電影中特徵的強度 Wm，然後優化 Ein，先固定 Vn 去優化 Wm，再固定 Wm 去優化 Vn，如此重複直到收斂，這樣就可以得到與 Rnm 最相似的 Vn X Wm。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 14 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-14-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 02 Oct 2017 09:10:41 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-14-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 13 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講介紹了深度學習神經網路，基本上神經網路林軒田老師只說明了兩講，這一講將進入一個新的 Machine Learning 演算法 Radial Basis Function Network（我個人不太覺得這個是神經網路演算法），並延伸介紹了其中會使用到的 K-means 分群演算法。
回顧一下 Gaussian SVM Gaussian Kernel 也稱為是 Radial Basis Function(RBF)，定義一種距離關係，Gaussiam SVM 也就是使用這些 RBF 經過線性組合的預測模型。
RBF Network 的定義 RBF Network 定義是資料點與代表性中心點投票出來的結果作為預測，其中每個中心點具有代表性，資料點經過 RBF 距離公式的轉換之後（距離越小，越有影響力），再經過 beta 投票。
而 RBF Network 要學習的參數就是中心點 u 以及每個中心點距離公式的權重 beta。
Full RBF Network 了解定義之後，我們就要求出 RBF Network 最佳化的 u 及 beta，一種情況我們是將所有的資料點都當成是重要的中心點，這種情況，然後 beta 直接用 y 當成加權，這樣就是所謂的 Full RBF Network，這樣的預測模型就完全沒有訓練過程，只要將所有的點記下來，然後用 RBF 距離公式計算投票來定義新的資料點應該是屬於什麼。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 13 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-13-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 10 Jul 2017 04:59:44 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-13-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 12 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講說明了什麼是類神經網路，以及類神經網路的核心演算法 Backpropagation，及如何使用 Gradient Decent 算法來計算最佳解，這一講將介紹類神經網路的延伸 - 深度學習。
不過林軒田的課程在深度學習的介紹上只有一講，其實並不是很深入，大家有興趣可以去找李宏毅老師的課程來看看。
再看一次類神經網路 讓我們再看一次類神經網路，我們知道類神經網路中每一層的神經元就是在做一些細微的 pattern 比對，那我們應該要怎麼設計類神經網路的結構呢？主觀上，你可以設計，客觀上，我們必須對神經網路進行驗證，神經網路的結構在類神經網路這樣的領域上也是個重要的議題。
淺跟深 既然結構是個重要的議題，那淺層跟深層的神經網路有何不同呢？深層的神經網路就是我們所謂的深度學習，一般的淺層神經網路，如果有足夠多的神經元其實已經就能夠解決蠻多問題了，深層神經網路理論上會更強，相對的運算量也會比較大、容易 overfitting，但深層的神經網路是有其物理意義的，也因此深度學習在近年資料量多、運算速度變快之後開始快速發展。
深度學習的物理意義 我們知道類神經網路的每個神經元就是在做 raw data 的 pattern 比對，每個神經元只做非常細微的比對，如果我們再加上一層 layer，那這一層 layer 就是在做前一層 pattern 的 pattern 比對，比對的 pattern 會比前一層更具體，因此加上越多 layer 的話，就能夠將更具體的 pattern 找出來。
因為這樣的特性，我們會將深度學習的方法用在比較無法具體找出 feature 的訊號問題上，像是影像處理、語音處理等等，讓深度學習來幫我們找出具體 pattern。
深度學習的關鍵問題與技術 深度學習所面臨的關鍵問題與技術大概有以下幾項，第一，如何覺得結構是一個問題，有時我們會利用一些領域知識來決定神經網路的結構，比如在影像問題上，我們會使用 CNN 這種特殊結構的神經網路。
另外由於模型很複雜，我們需要更多的資料來做計算，且需要使用 regularization 方法來避免 overfitting，常用的方法有 dropout 及 denoising。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 12 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-12-jiang-xue-xi-bi-ji/</link>
      <pubDate>Thu, 01 Jun 2017 07:24:52 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-12-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 11 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，大家不一定要記住所有演算法的細節，但大致上對 Aggregation 的方式有些概念就可以啦！
如果要記，就要記住這個核心概念：Aggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。
這一講我們將開始介紹現在很紅的類神經網路機器學習演算法。
Perceptron 的線性組合 我們先看一下最簡單的類神經網路，其實可以看成是多個 Perceptron 的線性組合，如果之前學過的 Aggregation，這樣組合多個 Perceptron 就能帶來更複雜的學習效果。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 11 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/</link>
      <pubDate>Tue, 25 Apr 2017 09:35:59 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 10 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講的 Random Forest 演算核心主要就是利用 bootstrap data 的方式訓練出許多不同的 Decision Trees 再 uniform 結合起來。
AdaBoost Decision Tree 這一講接下來要介紹的 AdaBoost Decision Tree 其實乍看有些類似，但它的訓練資料集並不是透過 bootstrap 來打亂，而是使用之前 AdaBoost 的方式再每一輪資料計算加權 u(t) 去訓練出許多不同的 Decision Tree，最後再以 alpha(t) 的權重將所有的 Decision Tree 結合起來。
權重會影響演算法 由於 AdaBoost Decision Tree 會考慮到權重，因此應該要像之前介紹過的 AdaBoost 會將權重傳進 Decision Stump 一樣，AdaBoost Decision Tree 應該也要將權重傳進 Decision Tree 裡做訓練，但這樣就需要調整 Decision Tree 原本的演算法，我們不喜歡這樣。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 10 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-10-jiang-xue-xi-bi-ji/</link>
      <pubDate>Tue, 28 Mar 2017 06:15:43 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-10-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 9 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講介紹了 Decision Tree，如同之前介紹的 blending 算法，我們也可以進一步使用在 Decision Tree，這就是這一講要介紹的 Random Forest。
回憶 Bagging 與 Decision Tree 回憶一下 Bagging 與 Decision Tree 的特點，Bagging 的結合 weak learner 的方式主要是為何減少差異化，讓未來的預測可以更好，Decision Tree 結合 weak learner 的方式則是著重差異化，讓 modle 在訓練時得到的預測效果更好，我們有辦法結合這兩個特點嗎？
Random Forest Random Forest 就可以達到上述的目的，每次會用類似 Bagging 的方法取得一個新的 Decision Tree，再將所有的 Decision Tree 結合起來。這個方法可以很容易地平行化運算，且不僅能夠保持 Decision Tree 的差異行，還能減少 Decision Tree 的 fully grown 的 overfitting。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 9 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-9-jiang-xue-xi-bi-ji/</link>
      <pubDate>Wed, 30 Nov 2016 09:40:52 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-9-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 8 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講我們介紹了 Adaptive Boosting 這種可以結合多個 Weak Learner 的 Linear Aggregation Model，這一講將介紹另一種 Aggregation Model - Decision Tree，Decision Tree 其實就是一種 Non-Linear 的 Aggregation Model。
Aggregation Model 表格 我們可以將這幾講種所介紹的 Aggregation Model 用這個表格整理出來，我們可以知道 AdaBoost 跟 Decision Tree 都是 Weak Learner Aggregation 的模型，只是 AdaBoost 是 Linear Aggregation，會讓所有的 Weak Learner 一起發揮作用，但 Decision Tree 則是 Non-Linear 的 Conditional Aggregation，會每次根據 condition 讓某個 Weak Learner 發揮作用。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 8 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji/</link>
      <pubDate>Tue, 08 Nov 2016 10:26:42 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 7 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講我們介紹了如何使用 Blending 及 Bagging 的技巧來做到 Aggregation Model，可以使用 Uniform 及 Linear 的方式融合不同的 Model。至於以 Non-linear 的方式融合 Model 就需要依據想展現的特性去調整演算法來做到，這一講將介紹 Adaptive Boosting 這種特別的演算法。
幼稚園學生學認識蘋果的故事（一） 我們用一個幼稚園學生在課堂上學認識蘋果的故事來作為開頭說明，在課堂上老師問 Michael 說「上面的圖片哪些是蘋果呢？」，Michael 回答「蘋果是圓的」，的確蘋果很多是圓的，但是有些水果是圓的但不是蘋果，有些蘋果也不一定是圓的，因此 Michael 的回答在藍色的這些圖片犯了錯誤。
幼稚園學生學認識蘋果的故事（二） 於是老師為了讓學生可以更精確地回答，將 Michael 犯錯的圖片放大了，答對的圖片則縮小了，讓學生的可針對這些錯誤再修正答案。於是 Tina 回答「蘋果是紅的」，這的確是一個很好的觀察，但一樣在底下藍色標示的這是個圖片犯了錯，番茄跟草莓也是紅的、青蘋果的話就是綠的。
幼稚園學生學認識蘋果的故事（三） 於是老師又將 Tina 犯錯的圖片放大了，答對的圖片縮小，讓學生繼續精確的回覆蘋果的特徵。
動機 這樣的教學過程也是一種可以用來教機器如何學習的過程，每個學生都只會一些簡單的假設 gt（蘋果是紅的），綜合所有學生的假設就可以好好地認識出蘋果的特徵形成 G，而老師則像是一個演算法一樣指導學生方向，讓錯誤越來越少。
接下來我們就要介紹如何用演算法來模擬這樣的學習過程。
有權重的 Ein 老師調整圖片放大縮小的教學方式，在數學上我們可以為每個點犯錯時加上一個權重來表示。
每一回合調整權重 那我們如何調整權重呢？我們每次調整權重，是希望每個學生能學出不一樣的觀點，這樣才能配合所有學生的觀點做出對蘋果完整的認識，因此挑整權重時應該要讓第一個學習到的 gt 在 u(t+1) 時這樣的權重下表現很差。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 7 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/</link>
      <pubDate>Wed, 14 Sep 2016 09:05:58 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 6 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 前 6 講我們對 SVM 做了完整的介紹，從基本的 SVM 分類器到使用 Support Vector 性質發展出來的 regression 演算法 SVR，在機器學習基石中學過的各種問題，SVM 都有對應的演算法可以解。
第 7 講我們要介紹 Aggregation Models，顧名思義就是要講多種模型結合起來，看能不能在機器學習上有更好的效果。
Aggregation 的故事 我們用一個簡單的故事來說明 Aggregation，假設現在你有很多個朋友可以預測股票會漲還是會跌，那你要選擇相信誰的說法呢？這就像我們有很多個機器學習預測模型，我們要選擇哪一個來做預測。
一個方式是選擇裡面最準的那一個人的說法，在機器學習就是使用 Validation 來做選擇。
一個方式是綜合所有人的意見，每個人代表一票，然後選擇票數最多的預測。在機器學習也可以用這樣的方法綜合所有模型的預測。
另一個方式也是綜合所有人的意見，只是每個人的票數不一樣，比較準的人票數較多，比較沒那麼準的人票數較少。在機器學習上，我們也可以為每個模型放上不同的權重來做到這樣的效果。
最後一個方式就是會依據條件來選擇相信誰的說法，因為每個人擅長的預測可能不多，有的人擅長科技類股，有的人擅長傳統類股，所以我們需要依據條件來做調整。在機器學習上也會有類似的演算法來整合各個預測模型。
Aggregation 大致就是依照上述方式來整合各個模型。
用 Validation 選擇預測模型 我們已經學過如何使用 Validation 來選擇預測模型，這個方式有一個問題就是，需要其中有一個強的預設模型才會有用，如果所有的預設模型都不準確，那也只是從廢渣裡面選一個比較不廢的而已。
所以 Validation 在機器學習上還是有一些限制的，那我們有辦法透過 Aggregation 來讓所有的廢渣整合起來，然後變強，讓預測變得更準確嗎？
為何 Aggregation 會有用？ 首先我們看左圖，如果現在預設模型只能切垂直線或水平線，其實預測效果可想而知是不會好到哪裡去的。但是如果我們將多個垂直線或水平線的預測模型整合起來，就有辦法做好一些更複雜的分類。這某種程度像是做了特徵轉換到高維度，讓預測模型變得更強、更準確。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 6 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji-2/</link>
      <pubDate>Sat, 06 Aug 2016 06:53:29 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji-2/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 5 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM Kernel Trick 所啟發的 Kernel Logistic Rregression。這一講我們將繼續介紹如何延伸到解 Regression 的問題。
利用 Representer Theorem 延伸 從數學模型上，我們發現 L2-regularized 線性模型都可以轉換成 Kernel 形式，而 Linear/Ridge Regression 都有公式解，那麼 Kernel Ridge Regession 也可以推導出公式解嗎？
Kernel Ridge Regression 數學式 我們使用 Representer Theorem 將 Kernel 應用至 Ridge Regression 的數學式上，得到以下 Kernel Ridge Regression 數學式。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 5 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/</link>
      <pubDate>Tue, 28 Jun 2016 07:41:37 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 4 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們介紹了 Soft Margin SVM，讓 SVM 可以容忍一些小錯以避免 Overfitting，由於強度與容忍度兼具，Soft Margin SVM 比較通用，其實大家平常口中所說的 SVM 就是指 Soft Margin SVM。
前面四講我們都在討論 SVM 這個分類演算法，那我們有可能用 SVM 來做 Logistic Regression 或是 Regression 嗎？在這一講中我們將介紹如何使用 SVM 的方法來做 Logistic Regression。
觀察 SVM 的容錯項 我觀察一下 Soft Margin SVM 的容錯項，我們可以把原本的限制式整合到要最小化的式子裡來看看，如下圖所示，如此就沒有限制式了。
觀察沒有限制式的 SVM 數學式 我們再仔細觀察一下沒有限制式的 SVM 數學式，發現形式跟 L2 regularized Logistic Regression 有點像，只是沒有限制式的 SVM 數學式有個 max 的函數在裡面，這樣的數學式不再是一個 QP 問題了，然後也不是一個可以微分的式子，因此很難最佳化。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 4 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 13 Jun 2016 13:06:59 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 3 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們介紹了使用 kernel 這樣的方法來處理高維度特徵的轉換，如此我們就能省下在高維度空間進行的運算，也因此無限多維的轉換也能輕易做到，讓 SVM 可能有更強的效果。
截至目前為止所學的 SVM 模型都是 Hard Margin SVM，這樣的 SVM 就是會將資料完美的分好，也因此在越強的學習模型中越可能會有 Overfiting 的情況發生（雖然 fat margin 有避免一些，但可能還是會發生）。所以這一講我們希望能允許 SVM 能容忍一些小錯誤（雜訊），這樣的 SVM 就是 Soft Margin SVM。
Hard-Margin SVM 的缺點 由於 Hard Margin SVM 堅持分好資料，所以在高維 Polynomial 及 Gaussian SVM 的學習模型可能會有 Overfitting 的現象，即使 SVM 的 Fat Margin 性質可以避掉一些，但 Overfitting 還是有可能發生，如下圖。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 3 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 23 May 2016 18:58:10 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 2 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，能夠將演算法跟高維度的計算脫鉤，但上一講中的 Q 矩陣實際上還是計算了高維度特徵矩陣內積，所以並沒有真的解決問題。
Dual SVM 仍與高維度 d 有依賴關係 目前推導出來的 Dual SVM 仍與高維度 d 有依賴關係，我們能不能簡化 Q 矩陣的高維度特徵矩陣內積計算呢？
觀察矩陣內積的每一個運算 我們用二次轉換拆開來觀察，發現原本將矩陣進行特徵轉換之後在做矩陣內積，可以分成 0 次項、1 次項、 2 次項分開來計算，結果會是一樣的。而更高維度的轉換也會有相同的性質。如此我們就可以限制計算量只在原本的特徵維度 O(d)。
Kernel 的概念 有了上述的性質，我們可以引進 Kernel 的概念，之前都是將矩陣進行特徵轉換之後再去計算內積，現在我們可以改成使用 kernel function 來做計算，znTzm 可以改成對應的 kernel function K(xn, xm)。
由於我們都不去 z 空間做計算了，因此也無法得到 z 空間的 w，所以 b 與 w 的式子都要改成使用 kernel function K(xn, xm) 來計算。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 2 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sat, 07 May 2016 14:24:53 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第 1 講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。
非線性 SVM 學會了 Hard Margin Linear SVM 之後，如果我們想要訓練非線性模型要怎麼做呢？跟之前的學習模型一樣，我們只要將資料點經過非線性轉換之後，在高維空間做訓練就可以了。
非線性的轉換其實可以依我們的需求轉換到非常高維，甚至可能到無限多維，如果是無限多維的話，我們怎麼使用 QP Solver 來解 SVM 呢？如果 SVM 模型可以轉換到與 feature 維度無關，那我們就可以使用無限多維的轉換了。
與特徵維度無關的 SVM 為了可以做到無限多維特徵轉換，我們需要將 SVM 轉為另外一個問題，在數學上已證明這兩個問題其實是一樣的，所以又稱為是 SVM 的對偶問題，Dual SVM，由於背後的數學證明很複雜，這門課程只會解釋一些必要的原理來讓我們理解。
使用 Lagrange Multipliers 當工具 我們在正規化那一講中曾經使用過 Lagrange Multipliers 來推導正規化的數學式，在推導 Dual SVM 也會使用到 Lagrange Multiplier。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習技法 Machine Learning Techniques 第 1 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-1-jiang-xue-xi-bi-ji/</link>
      <pubDate>Thu, 21 Apr 2016 09:00:27 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-1-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 機器學習基石系列 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。
線性分類回憶 回憶一下之前的課程中，我們使用 PLA 及 Pocket 來學習出可以分出兩類的線。
哪條線最好？ 但其實可以將訓練資料分類的線可能會有很多條線，如下圖所示。我們要怎麼選呢？如果用眼睛來看，你或許會覺得右邊的這條線最好。
為何右邊這條線最好？ 為何會覺得右邊這條線最好呢？假設先在我們再一次取得資料，可以預期資料與訓練資料會有點接近，但並不會完全一樣，這是因為 noise 的原因。所以偏差了一點點的 X 及 O 再左邊這條線可能就會不小心超出現，所以就會被誤判了，但在右邊這條線就可以容忍更多的誤差，也就比較不容易 overfitting，也因此右邊這條線最好。
如何描述這條線？我們可以說這條線與最近的訓練資料距離是所有的線中最大的。
胖的線 我們希望得到的線與最近的資料點的距離最大，換的角度，我們也可以說，我們想要得到最胖的線，而且這個胖線還可以將訓練資料分好分類。
Large-Margin Separating Hyperplane 這種胖的線名稱就叫 Large-Margin Separating Hyperplane，原本的問題就可以定義成要找最大的 margin，而且還要分好類，也就是 yn = sign(wTXn)。
最大的 margin 可以轉換成點與超平面之間最小的距離 distance(Xn, w)，然後 yn = sign(wTXn)，就代表 Yn 與 score 同號，所以可以轉換成 YnwTXn &amp;gt; 0，我們需要求解滿足這些條件的超平面。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 16 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-liu-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sun, 20 Mar 2016 17:16:59 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-liu-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第十五講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們了解了如何使用 Cross Validation 來幫助我們客觀選擇較好的模型，基本上機器學習所有相關的基本知識都已經具備了，這一講是林軒田老師給的三個錦囊妙計，算是一種經驗分享吧～
第一計 奧卡姆剃刀 資料的解釋應該要越簡單越好，我們應該要用剃刀剃掉過分的解釋，據說這句話是愛因斯坦說的。
如下圖，我們在使用機器學習時，也希望學習出來的模型會是左較簡單的模型。在直覺上我們會覺得左圖會比右圖夠有解釋性，當然理論上也證明如此了。
較簡單的模型 什麼是叫簡單的模型呢？較教簡單的模型，就是看起來很簡單，假設較少、參數較少，假設集合也比較好。
簡單比較好 那為什麼簡單會比較好呢？除了之前數學上的解釋之外，我們可以有這樣直觀的解釋：如果一個簡單的模型可以為數據做一個好的鑒別，那就代表這個模型的假設很有解釋性，如果是複雜的模型，由於它永遠都可以把訓練資料分的很好，這樣其實是沒有什麼解釋性的，也因此用簡單的模型會是比較好的。
所以根據這一計的想法，我們應該要先試線性模型，然後盡可能了解自己是不是已經盡可能地用了簡單的模型。
第二計 避免取樣偏差 取樣有可能會有偏差，VC 理論其中的一個假設就是訓練資料與測試資料要來自於同一個分佈，否則就無法成立。如果取樣有偏差，那機器學習的效果就會不好。
處理取樣偏差 要避免取樣偏差，要好好了解測試環境，讓訓練環境跟測試環境可以儘可能接近。舉例來說，如果測試環境會使用時間軸近期的資料，那訓練時要想辦法對時間軸較近的資料做一些權重的調整，在做 Validation 的時候也應該要選擇時間軸較近的資料。
另一個例子，其實信用卡核卡問題也有取樣偏差的風險，因為銀行只會有錯誤核卡，申請人刷爆卡的記錄，卻沒有錯誤不核卡，但該位申請人信用良好的資料。因此搜集到的資料本身就已經有被篩選過了，也因此可以針對這個部分在做一些優化。
第三計 避免偷看資料 之前我們的課程中有說過，我們可能會因為看過資料而猜測圈圈會有最好的效果，但這樣就會造成我們的學習過程沒有考慮到人腦幫忙計算過的 model complexity，所以我們要避免偷看資料。
資料重複利用地偷看 其實使用資料的過程中，我們就不斷地偷看資料，甚至看別人論文時，也是在累積偷看資料的過程，所以需要了解到這個概念，有可能讓你的機器學習受到影響。
處理資料偷看 實際上偷看資料的情況很容易發生，要做到完全不偷看資料很難，所以我們可以做的就是，一開始就將測試資料鎖起來，學習的過程中完全不用，然後使用 Validation 來避免偷看資料。
如果說希望將自己的 Domain Knowledge 加入假設，應該一開始就加進去，而不是看完資料再加進去。然後，要時時刻刻會實驗的結果存著懷疑之心，要有一種感覺這樣的結果可能受到的資料偷看污染的影響。
Power of Three 除了三個錦囊妙計，林軒田老師將機器學習的重點整理成 Power of Three，帶我們整個回顧一下。
第一個是機器學習有三個相關領域，Data Mining、Artificial Intelligence、Statistics。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 15 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-wu-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sat, 19 Mar 2016 18:44:00 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-wu-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第十四講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們進一步了解了如何透過正規化（Regularization）來避免 Overfitting，但正規化這個方法會有一個參數 lambda，這個 lambda 我們又要如何選擇呢？在這一講將會學習到使用 Validation 這個方法來幫助我們選擇比較好的 lambda 值，同理，這個方法也可以幫助我們用於選擇各種不同的學習模型。
許多學習模型可以使用 經過了前面 14 講，我們已經學會了許多學習模型，在演算法上我們有 PLA、Pocket、Linear Regression、Logistic Regression 可以做選擇；然後在模型學習的過程中，我們可以指定演算法要經過幾次的學習，每次學習優化的過程要走多大步；我們也可以有很多種線性轉換的方式將模型轉換到更複雜的空間來進行學習；如果模型太過複雜了，我們也有很多種正規化的方法來讓模型退回叫簡單的模型，並可透過 lambda 這個參數來調整退回的程度。
我們可以任意組合，但組合完之後我們要怎麼判斷哪個組合未來在做預測時效果會比較好呢？
用 Ein 來做選擇 如果我們用 Ein 來做選擇，那就永遠會選擇到比較複雜的模型，這在上一講中我們已經知道這很可能會有 Overfitting 發生。所以用 Ein 來做選擇是很危險的。
用 Etest 來做選擇 使用 Etest 來做選擇，基本上理論上是可行的，但 Etest 實際在我們訓練的過程中是不能拿來用的，直接拿 Etest 來幫助我們選擇模型其實是一種作弊行為。所以用 Etest 來做選擇也是不可行的。
引進 Eval 既然用 Ein 或 Etest 來做選擇是不可行的，那如果我們把我們手中的資料 D 保留一份下來作為 Dval，然後在訓練的過程中都不使用 Dval，等訓練完之後，在挑選各種模型的時候再用 Dval 來做選擇，那這樣會是一個比較安全的做法。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 14 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-si-jiang-xue-xi-bi-ji/</link>
      <pubDate>Tue, 15 Mar 2016 14:06:26 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-si-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第十三講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們更進一步的了解了什麼是 Overfitting 是因為 stochastic noise 及 deterministic noise 而造成，與簡易地介紹了幾個簡單的方法來避免 overfitting，這一講將介紹一個比較內行的方法來避免 overfitting，這個方法叫做正規化（Regularization）。
正規化 正規化（Regularization）的想法，就是我們了解 overfitting 發生時，有可能是因為我們訓練的假設模型本身就過於複雜，因此我們能不能讓複雜的假設模型退回至簡單的假設模型呢？這個退回去的方法就是正規化。
退回簡單模型就像是加了限制 假設我們現在是一個 10 次多項式的假設集合，我們想要退回成為較為簡單的 2 次多項式假設集合，其實可以想成就像是 2 次以上的項的係數都是 0，也就像是我們為求解的過程加上了一些限制，希望 2 次以上的項的係數都是 0。
使用較鬆的限制 直接將高維的項次設成 0 可能不是一個好方法，通常我們會希望由學習的過程來決定哪些項次要是 0，這樣的得到的學習效果可能會比較好。所以我們的限制就改成，希望不為 0 的係數不超過三個，由機器從資料來學習出最好的 w，這樣可能會得到比較好的結果。而這樣的限制並不是平滑的函數，所以這是一個 NP Hard 的問題。
換個方式得出較為平滑的限制 所以我們需要換個方式得出較為平滑的限制，這樣在演算法上會比較容易求解，在 Regression 這個問題上，我們可以把限制改為 ||w^2|| &amp;lt;= C 來代表 w 不超過三個係數不為 0，這個含義就像是讓 w 限制在某些值裡面，也許他不一定代表 w 不超過三個係數不為 0，但它可能可以包含，而且 C 的值是一個連續的數，求解上會比較容易。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 13 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-san-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 29 Feb 2016 18:01:32 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-san-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第十二講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們了解了如何使用非線性轉換來讓我們的機器學習演算法可以學習出非線性分類模型，也了解了這樣的方法可能會讓模型複雜度變高，造成 Overfitting 使未來 Eout 效果不佳的情況，所以要慎用此方法。在這一講中將更進一步說明什麼是 Overfitting，並講解如何避免 Overfitting。
Bad Generalization 無法舉一反三 我們來看個例子，現在我們使用一個二次多項式加上一點 noise 產生資料點，由於有 noise，我們是無法學習出一個二次多項式讓 Ein 為 0。但如果我們使用了非線性轉換到四次多項式來進行學習，我們可以找到一個 w 讓 Ein 為 0，看起來可能會像是圖中的紅線。但可想而知紅線的 Eout 可能會非常高，如此我們的機器學習是失敗的，無法舉一反三。
Overfitting 過度優化 其實這就是一種過度優化，當我們發現 Eout - Ein 很大時，就是發生了 Bad Generalization。
我們觀察圖中的 dvc*，當 dvc* 越來越高時，Ein 會下降，但 Eout 會上升，這時就是產生了 Overfitting。
當 dvc* 往左時，Ein 會上升，Eout 也會上升，這時就是產生了 Underfitting。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 12 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sun, 14 Feb 2016 15:18:51 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第十一講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中，我們將線性分類的模型擴展到可以進行多元分類，擴展的方法很直覺，就是使用 One vs One 及 One vs All 兩種分解成二元分類的方式來做到多元分類。在這一講中將講解如何讓線性模型擴展到非線性模型，讓我們可以將機器學習演算法的複雜度提高以解決更複雜的問題，並說明非線性模型會有什麼影響。
線性假設 之前的演算法目前都是基於線性的假設之下去找出分類最好的線，但這在線性不可分的情況下，會得到較大的 Ein，理論上較大的 Ein 未來 Eout 效果也會不佳，有沒有辦法讓我們演算法得出的線不一定要是一條直線以得到更佳的 Ein 來增加學習效果呢？
圈圈可分 我們從肉眼觀察可以發現右邊的資料點是一個「圈圈可分」的情況，所以我們要解這個問題，我們可以基於圈圈可分的情況去推導之前所有的演算法，但這樣有點麻煩，沒有沒其他更通用的方法？
比較圈圈可分及線性可分 為了讓演算法可以通用，我們會思考，如果我們可以讓圈圈可分轉換到一個空間之後變成線性可分，那就太好了。我們比較一下圈圈可分及線性可分，當我們將 Xn 圈圈可分的資料點，透過一個圈圈方程式轉換到 Z 空間，這時資料點 Zn 在 Z 空間就是一個線性可分的情況，不過在 Z 空間線性可分，在 X 空間不一定會是圈圈可分。
Z 空間的線性假設 觀察在 Z 空間的線性方程式，不同的參數在 X 空間會是不同的曲線，有可能是圓、橢圓、雙曲線等等，因此我們了解在 Z 空間的線會是 X 空間的二次曲線。
一般化二次假設 我們剛剛是使用 x0, x1^2, X2^2 來簡化理解這個問題，現在將問題更一般化，將原本的 xn 用 Phi 二次展開來一般化剛剛個問題，這樣的 Z 空間學習出來的線性方程式在 X 空間就不一定會是以原點為中心，這樣所有的二次曲線都有辦法在 Z 空間學習到了，而起原本在 X 空間的線性方程式也會包含在按次曲線中。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 11 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/</link>
      <pubDate>Mon, 01 Feb 2016 09:45:06 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第十講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中我們了解了 Logistic Regression 演算法並了解了如何使用 Logistic Regrssion 來預測心臟病發病機率這樣的問題，這一講中將延伸之前學過的演算法，在理論上說明 Linear Regression 以及 Logistic Regression 都可以用來解 Binary Classification 的問題。學會了 Binary Classification 之後，我們也可以用這樣的技巧來解 Multi-Classification 的問題。
比較之前學過的演算法 比較之前學過的演算法，三個算法最後都會得到一個線性函數來輸出 scroe 值，但 PLA 做 Linear Classification 是一個 NP-hard 的問題，Linear Regression 及 Logistic Regression 則相對較容易求解，我們可以使用 Linear Regression 或是 Logistic Regression 演算法來解 Linear Classification 的問題嗎？
將三個演算法的 Error Function 整理一下 依據各個 Error Function 的算法，我們都可以整理成 ys 的形式，在物理意義上，我們可以說 y 代表正確性，s 代表正確或錯誤程度多少。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 10 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sun, 17 Jan 2016 18:37:49 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第九講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在上一講中我們了解了如何使用線性迴歸的方法來讓機器學習回答實數解的問題，第十講我們將介紹使用 Logistic Regression 來讓機器學習回答可能機率的問題。
可能性問題 我們用心臟病發生的機率這樣的問題來做為例子，假設我們有一組病患的數據，我們需要預測他們在一段時間之後患上心臟病的「可能性」為何。我們拿到的歷史數據跟之前的二元分類問題是一樣的，我們只知道病患之後有或者沒有發生心臟病，從這樣的數據我們用之前學過的二元分類學習方法可以讓機器學習到如何預測病患會不會發病，但現在我們想讓機器回答的是可能性，不像二元分類那麼硬，所以又稱軟性二元分類，即 f(x) = P(+1|x)，取值的範圍區間在 [0, 1]。
軟性二元分類 我們的訓練數據跟二元分類是完全一樣的，x 是病人的特徵值，y 是 +1（患心臟病）或 -1（沒有患心臟病），沒有告訴我們有關「患病機率」的訊息。回想在二元分類中，我們使用 w*x 得到一個「分數」之後，再利用取號運算 sign 來預測 y 是 +1 或是 -1。對於目前這個問題，如果我們能夠將這個「分數」映射到 [0, 1] 區間，似乎就可以解這個問題了～
Logistic 假設 我們把上面的想法寫成式子，就是 h(x) = theta(wx)，映射分數到 [0, 1] 的 function 就叫做 logistic function，用 theta 來表示
Logistic 函式 Logistic regression 選擇使用了 sigmoid 來將值域映射到 [0, 1]，sigmoid 是 f(s) = 1 / (1 + exp(-s))，f(x) 單調遞增，0 &amp;lt;= f(x) &amp;lt;= 1。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 9 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/</link>
      <pubDate>Wed, 06 Jan 2016 13:29:31 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第八講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在第八講中我們機器學習流程圖裡加入了 error function 及 noise 的概念，並了解在這樣的情況下機器學習還是可行的。前面花了很大的篇幅在說機器為何可以學習，接下來是要說明機器是怎麼學習的。本篇以眾所皆知的線性迴歸為例，從方程式的形式、誤差的衡量方式、如何最小化 Ein，讓我們對線性迴歸在機器學習上的應用有些理解。
信用卡額度問題 回到信用卡這個例子，假設我們現在需要的是判定要發多少信用額度給申請者，這種問題的答案就從發不發卡變成了一個實數，這就是線性迴歸的問題。
線性迴歸 將上述問題寫成線性迴歸式就如下圖，其實與 perceptron 很像，但不用再取正負號。
圖解線性迴歸 圖解線性迴歸問題就會如下圖所示，找一條線或超平面來讓所有的資料點與之的差距最小。
線性迴歸的錯誤衡量 線性迴歸的錯誤衡量一般是使用平方錯誤，所以在計算 Ein 的過程是算出平均平方錯誤最小的值，而 Eout 的差距就可以用平方錯誤來計算效果。最小的 Ein 如何計算呢？
整理成矩陣的形式 我們將 Ein 的式子整理成矩陣的形式，推導過程如下。
最小的 Ein 整理成這種式子的 Ein 理論上已知是個連續、處處可微的凸函數，所以計算最小的 Ein 就是去解每一個維度進行偏微分為 0 所得到的值。他的物理意義就是在這個凸函數中這個點在各個維度都無法再下滑。
最小的 Ein 就是計算 Ein 的梯度 這就是在計算 Ein 的梯度，我們將原本的式子展開後，並對這個式子做偏微分。對矩陣的偏微分的理解我們可以從 1 個維度慢慢推廣，如此就可知 XTXw - XTy = 0 時可以得到最佳解。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 8 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/</link>
      <pubDate>Wed, 23 Dec 2015 13:08:10 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第七講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在第七講中我們定義了 VC Dimension，就是最大的 non-break point，當 d_vc 是有限的，且資料 N 夠大，Ein 很小的時候，理論上機器學習是可以達成目標的。
重溫機器學習流程圖 重溫機器學習流程圖，大致的理論我們都已經完備了，但這時又會想，如果資料來源有雜訊（noise）又會如何呢？
雜訊（Noise）是什麼 雜訊是什麼呢？以之前的銀行發卡的例子來說明，比如該發卡未發卡、不該發卡卻發了卡、或是一開始收集的基本資料就是錯的，這些就會是我們搜集到資料時的雜訊，在有雜訊的情況下 VC bond 還會正常運作嗎？
用彈珠顏色會改變來代表雜訊來看 VC bound 之前在推導 VC bound 時是用彈珠來說明，我們可以用不固定顏色的彈珠來代表雜訊推導 VC bound（以 pocket algorithm 可以想成是 o 和 x 在同一線上，所以無法確定 o 或 x，這就是雜訊），也就是資料來源會多了一個 y ~ P(y|x)這個條件，從之前的理論推導中，我們了解了訓練資料跟測試資料都來自同一個資料分佈的話，那 VC bound 就會成立。
新的機器學習流程圖 所以新的機器學習流程圖就可以容忍雜訊。也因此可以容忍雜訊的 pocket algorithm 就有理論基礎了。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 7 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/</link>
      <pubDate>Wed, 09 Dec 2015 19:31:24 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第六講 的碼農們，我建議可以先回頭去讀一下再回來喔！
在第六講我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，這個上界我們就稱之為 VC Bond。因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果，而且也不會有 Ein 與 Eout 誤差過大的情況發生，而這一講將進一步說明 VC Bond。
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 上一講，我們從 break point 的性質慢慢推導出成長函數是一個多項式，因此我們可以保證 Ein 與 Eout 的差距在 N 資料量夠大的情況下不會因為假設集合的無限累積而差距變得很大。
將成長函數寫得更簡潔一點 目前我們知道假設集合的成長函數是 B(N, k) 這個多項式，實際算出來的值如左圖，其實我們可以直接用 N 的 k-1 次方來表示，實際算出來的值如右圖，總之 B(N, k) 會小於 N 的 k-1 次方，所以我們可以直接用 N 的 k-1 次方來表示成長函數。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 6 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sat, 21 Nov 2015 15:59:16 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第五講 的碼農們，我建議可以先回頭去讀一下再回來喔！
在第五講中我們了解了如何將 PLA 無限的假設集合透過 Dichotomy、Break Point 這樣的方式轉換成有限的集合，在第六講中我們將更進一步去推導其實這個假設集合的成長函數會是一個多項式，如此我們就可以完全相信PLA 機器學習方法的確在數學理論上是可行的了。
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 我們先來回顧一下上一講的內容，在上一講我們知道了成長函數似乎跟 break point 有些關係，這一講我們將慢慢導出這樣的關係其實是一個多項式。
從 break point 找其他線索 從上一講提到的 break point 我們知道了成長函數至少會小於 2 的 N 次方，且如果 break point 在k取到了，那 k+1, k+2,&amp;hellip; 都會是 break point。
由例子觀察 break point 這邊林軒田教授用了一連串的例子說明了如果 H 有Break Point k，那當 N 大於 k 時，mH(N) 成長函數會大大縮減（以 binary classification 這個問題來說，所有的 H 為 2 的 N 次方個）。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 5 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/</link>
      <pubDate>Thu, 15 Oct 2015 13:08:14 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第四講 的碼農們，我建議可以先回頭去讀一下再回來喔！
在第四講中我們了解了在有限假設集合的情況下機器學習是可能的，而第五講就是想要將有限假設集合可以推廣出去，讓我們在無限的假設集合裡也可以透過一些理論慢慢收斂到一個多項式集合，如此我們就可以放心的利用機器學習來解決我們所面對的一些問題。
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 我們先來回顧一下上一講的內容，在上一講我們知道了機器學習在足夠的資料及有限的假設集合這種情況下是可行的。
更新機器學習流程圖 如果假設集合 H 是有限 M 個，然後資料量夠多 N，不管我們的演算法 A 是什麼，我們由定理可以知道 Eout 跟 Ein 是很接近的。所以如果 A 找到了一個假設 g 讓 Ein 近似於 0，那我們就可以說 Eout 大概就會是 0，因此機器學習在這樣的情況下是可行的。
有了這樣的概念，我們擴充了我們的機器學習流程圖。從輸入資料中去訓練機器學習，然後得到 Ein 近似於 0，之後再從同一個資料分佈中去測試機器學習的結果，如此可以證明機器有學習到技能。
機器學習的兩個核心問題 機器學習有兩個核心問題，我們希望 Eout 跟 Ein 是很接近的，這個意思就是說，我們希望後來測試學習的結果，會跟訓練時得到的結果很接近，這樣我們才能說機器有學習到技能。
另一個就是，我們希望 Ein 可以很小，也就是訓練的過程中，我們希望機器就可以得到很好的效果，也就是誤差很小。
那之前我們所說的有限集合 M 在這邊扮演什麼角色呢？
Ｍ 的兩難 如果假設集合 M 很小，我們可以保證 Eout 可以接近 Ein，但是因為假設集合小，可以挑選的選擇就少，也因此 Ein 可能會是一個不小的值，也就是誤差會大。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 4 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sun, 27 Sep 2015 10:49:15 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第三講 的碼農們，我建議可以先回頭去讀一下再回來喔！
第四講的內容主要是讓我們知道機器學習是否真的可能，並利用數學上的定理來說明機器學習在某些情境之下是可能的，有數學上定理的支持，我們就可以放心的利用機器學習來解決我們所面對的一些問題。
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 我們先來回顧一下上一講的內容，在上一講我們知道了各式各樣的機器學習方法及名詞，而我們未來會專注於二元分類及迴歸這樣的問題，然後使用大量監督式標示好的資料且定義明確的特徵來進行機器學習。
看看這個問題，想想如何使用學習 有人會問，說了這麼多，如何知道機器學習是不是真的可能？說不定根本無法做到。比如這個問題，g(x)可以回答 +1 還是 -1。
見仁見智的問題無法解 像這樣的問題，你可以回答 +1，因為 +1 的圖都是對稱的，而這個圖是對稱的，所以是 +1。你可以回答 -1，因為 -1 的圖都是左上方黑色的，而這個圖是左上方黑色的，所以是 -1。
套到二元分類的問題 現在我們套到二元分類的問題，給了 Xn 及 Yn，然後機器學習出了 g，我們可以說 g 近似於 f 嗎？
天下沒有白吃的午餐 在驗證 g 的時候，如果是用原本的 D，那我們很容易的說明 g 近似於 f，但是如果資料是用 D 以外的資料來驗證，那我們無法很明確的說明 g 近似於 f，但我們要的其實就是希望 g 在 D 以外的資料也近似於 f，這有可能嗎？
利用罐子取彈珠的例子來說明是否可能 現在想像我們有一個裡面有很多橘色和綠色彈珠的罐子，我們可能無法知道橘色彈珠的真實比例，但我們可以推估出橘色彈珠出現的機率嗎？</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 3 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/</link>
      <pubDate>Sat, 12 Sep 2015 11:32:31 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第二講 的碼農們，我建議可以先回頭去讀一下再回來喔！
第三講的內容偏向介紹各種機器學習方法，以前念論文的時候看到這些名詞都會覺得高深莫測，但其實這各式各樣的機器學習方法其實都是從最基礎的核心變化而來，所以不要被嚇到。了解各種機器學習方法的輸入輸出對於日後面對一些問題的時候，我們才能夠知道要挑選什麼機器學習方法來解決問題。
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 我們先來回顧一下上一講的內容，在上一講我們知道了如何使用 PLA 讓機器學會回答是非題這樣的兩類問題（Binary Classificaction），套到機器學習的那句名句，我們可以清楚的了解，PLA 這個演算法 A 觀察了線性可分（linear separable）的 D 及感知假設集合 H 去得到一個最好的假設 g，這一句話就可以概括到上一講的內容了。
從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification 接下來我們來了解一下各式各樣的學習方法，從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification，像是之前的是否發信用卡的例子就是 Binary Classification。
從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification 從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification，像是使用投飲機辨識錢幣的問題就是一個 Multiclass Classification 的問題，所以我們可以將分類問題推廣到分成 K 類，這樣 Binary Classificatin 就是一個 K=2 的分類問題。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 2 講學習筆記</title>
      <link>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/</link>
      <pubDate>Fri, 28 Aug 2015 09:05:06 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/</guid>
      <description>前言 本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 第一講 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下 在前一章我們基本上可以了解機器學習的架構大致上就是 *A takes D and H to get g*，也就是說我們會使用演算法來基於資料與假設集合計算出一個符合資料呈現結果的方程式 g，在這邊我們就會看到 H 會長什麼樣子，然後介紹 Perceptron Learning Algorithm（PLA）來讓機器學習如何回答是非題，比如讓機器回答銀行是否要發信用卡給申請人這樣的問題。
再看一次是否要發信用卡這個問題 是否要發信用卡這個問題我們可以想成它是一個方程式 f，而申請者的資料集合 X 丟進去就可以得到 Y 這些是否核發信用卡的記錄，我們現在不知道 f，將歷史資料 D 拿來當成訓練資料，其中每個 xi 就是申請者的資料，它會一個多維相向，比如第一個維度是年齡，第二個維度是性別&amp;hellip;等等，然後我們會將這些資料 D 及假設集合 H 丟到機器學習演算法 Ａ，最後算出一個最像 f 的 g，這個 g 其實就是從假設集合 H 挑出一個最好的假設的結果。
簡單的假設集合：感知器 要回答是否核發信用卡，可以用這樣簡單的想法來實現，現在我們知道申請者有很多基本資料，這些資料可以關係到是否核發信用卡，學術上就稱為是「特徵值」，這些特徵值有的重要、有的不重要，我們可以為這些特徵值依照重要性配上一個權重分數 wi，所以當這些分數加總起來之後，如果超過一個界線 threshold 時，我們就可以就可以決定核發信用卡，否則就不核發。這些 wi 及 threshold 就是所謂的假設集合，可以表示成如投影片中的線性方程式。</description>
    </item>
    
    <item>
      <title>林軒田教授機器學習基石 Machine Learning Foundations 第 1 講學習筆記</title>
      <link>https://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/</link>
      <pubDate>Mon, 17 Aug 2015 13:04:46 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/</guid>
      <description>前言 機器學習（Machine Learning）是一門很深的課程，要直接跳進來學習其實並不容易，因此系統性由淺而深的學習過程還是必須的。這一系列部落格文章我將分享我在 Coursera 上臺灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明，希望對有心學習 Machine Learning 的碼農們有些幫助。
範例原始碼：FukuML - 簡單易用的機器學習套件 我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
如何有效學習機器學習 從基礎來由淺入深，包含理論及實作技術用說故事的方式包裝，比如何時可以使用機器學習、為何機器可以學習、機器怎麼學習、如何讓機器學得更好，讓我們可以記得並加以應用。
從人的學習轉換到機器學習 人學習是為了習得一種技能，比如學習辨認男生或女生，而我們可以從觀察中累積經驗而學會辨認男生或女生，這就是人學習的過程，觀察 -&amp;gt; 累積經驗、學習 -&amp;gt; 習得技能；而機器怎麼學習呢？其實有點相似，機器為了學習一種技能，比如一樣是學習辨認男生或女生，電腦可以從觀察資料及計算累積模型而學會辨認男生或女生，這就是機器學習的過程，資料 -&amp;gt; 計算、學習出模型 -&amp;gt; 習得技能。
再定義一下什麼是技能 「師爺，翻譯翻譯什麼是他媽的技能」「技能不就是技能嗎」在機器學習上，技能就是透過計算所搜集到的資料來提升一些可量測的性能，比如預測得更準確，實例上像是我們可以搜集股票的交易資料，然後透過機器學習的計算及預測後，是否可以得到更多的投資報酬。如果可以增加預測的準確度，那麼我們就可以說電腦透過機器學習得到了預測股票買賣的技能了。
舉個例子 各位勞苦功高的碼農們，現在老闆心血來潮要你寫一個可以辨識樹的圖片的程式，你會怎麼寫呢？你可能寫一個程式檢查圖片中有沒有綠綠的或是有沒有像葉子的形狀的部份等等，然後寫了幾百條規則來完成辨識樹的圖片的功能，現在功能上線了，好死不死現在來了一張樹的圖片上面剛好都沒有葉子，你寫的幾百條規則都沒用了，辨識樹的圖片的功能只能以失敗收場。機器學習可以解決這樣的問題，透過觀察資料的方式來讓電腦自己辨識樹的圖片，可能會比寫幾百條判斷規則更有效。這有點像是教電腦學會釣魚（透過觀察資料學習），而不是直接給電腦魚吃（直接寫規則給電腦）。
那麼什麼時候可以使用機器學習呢 從上個例子我們可以大概了解使用機器學習的使用時機，大致上如果觀察到現在你想要解決的問題有以下三個現象，應該就是機器學習上場的時刻了：
 存在某種潛在規則 但沒有很辦法很簡單地用程式直接定義來作邏輯判斷（if else 就可以做到，就不用機器學習） 這些潛在規則有很多資料可以作為觀察、學習的來源  舉個實際的機器學習例子 1 Netflix 現在出了一個問題，如果你能讓使用者對電影喜好程度星級預測準確率提升 10%，那就可以獲得 100 萬美金，馬上讓你從碼農無產階級晉升到天龍人資產階級，而這個問題是這樣的：他們給了你大量使用者對一些電影的星級評分資料，你必須要讓電腦學到一個技能，這個技能可以預測到使用者對他還沒看過的電影評分會是多少星級，如果電腦能準確預測的話，那某種程度它就有了可以知道使用者會不會喜歡這些電影的技能，進而可以推薦使用者他們會喜歡的電影，讓他們從口袋裡拿錢過來～
舉個實際的機器學習例子 2 這邊偷偷告訴大家一個很常見的機器學習方法的模型，我們再來整理一下，其實這個問題可以轉化成這樣，使用者有很多個會喜歡這部電影的因素，比如電影中有沒有爆破場景、有沒有養眼畫面、有沒有外星人等等，這個我們就稱之為使用者的特徵值（feature），而電影本身也有很多因素，比如電影中有出現炸彈、是很有魅力的史嘉蕾·喬韓森所主演、片名是 ET 第二集等等，這個我們就稱之為電影的特徵值，我們把這兩個特徵值表示成向量（vector），如此如果使用者與電影特徵值有對應的特徵越多，那就代表使用者很有可能喜歡這部電影，而這可以很快地用向量內積的方式計算出來。也就是說，機器學習在這個問題上，只要能學習出這些會影響使用者喜好的因素也就是機器學習所說的特徵值會是什麼，那這樣當一部新的電影出來，我們只要叫電腦對一下這部新電影與使用者的特徵值的對應起來的向量內積值高不高就可以知道使用者會不會喜歡這部電影了～
將剛剛的問題用數學式來描述 我們在用銀行核發信用卡的例子來描述機器學習，我們可以把信用卡申請者的資料想成是 x，而 y 是銀行是否核發信用卡。所以這就是一個函式，它有一個潛在規則，可以讓 x 對應到 y，機器學習就是要算出這個 f 函式是什麼。現在我們有大量的信用卡對申請者核發信用卡的資料，就是 D，我們可以從資料觀察中得到一些假設，然後讓電腦去學習這些假設是對的還是錯的，慢慢習得技能，最後電腦可能會算出一個 g 函式，雖然不是完全跟 f 一樣，但跟 f 很像，所以能夠做出還蠻精確的預測。</description>
    </item>
    
    <item>
      <title>如何使用 jieba 結巴中文分詞程式</title>
      <link>https://blog.fukuball.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-jieba-%E7%B5%90%E5%B7%B4%E4%B8%AD%E6%96%87%E5%88%86%E8%A9%9E%E7%A8%8B%E5%BC%8F/</link>
      <pubDate>Wed, 06 Aug 2014 13:49:55 +0800</pubDate>
      
      <guid>https://blog.fukuball.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-jieba-%E7%B5%90%E5%B7%B4%E4%B8%AD%E6%96%87%E5%88%86%E8%A9%9E%E7%A8%8B%E5%BC%8F/</guid>
      <description>前言 自然語言處理的其中一個重要環節就是中文斷詞的處理，比起英文斷詞，中文斷詞在先天上就比較難處理，比如電腦要怎麼知道「全台大停電」要斷詞成「全台 / 大 / 停電」呢？如果是英文「Power outage all over Taiwan」，就可以直接用空白斷成「Power / outage / all / over / Taiwan」，可見中文斷詞真的是一個大問題啊～
這樣的問題其實已經有很多解法，比如中研院也有提供「中文斷詞系統」，但就是很難用，不僅 API Call 的次數有限制，還很難串，Server 也常常掛掉，真不曉得為何中研院不將核心開源出來，讓大家可以一起來改善這種現象，總之我要棄中研院的斷詞系統而去了。
近來玩了一下 jieba 結巴這個 Python Based 的開源中文斷詞程式，感覺大好，順手發了一些 pull request，今天早上就成為 contributor 了！ 感覺真爽！每次發 pull request 總是有種莫名的爽感，既期待被 merge 又怕被 reject，就跟告白的感覺類似啊～
這麼好用的開源中文斷詞系統，當然要介紹給大家用啊！
背後演算法 jieba 中文斷詞所使用的演算法是基於 Trie Tree 結構去生成句子中中文字所有可能成詞的情況，然後使用動態規劃（Dynamic programming）算法來找出最大機率的路徑，這個路徑就是基於詞頻的最大斷詞結果。對於辨識新詞（字典詞庫中不存在的詞）則使用了 HMM 模型（Hidden Markov Model）及 Viterbi 算法來辨識出來。基本上這樣就可以完成具有斷詞功能的程式了，或許我之後可以找個時間寫幾篇部落格來介紹這幾個演算法。
如何安裝 推薦用 pip 安裝 jieba 套件，或者使用 Virtualenv 安裝（未來可能會介紹如何使用 Virtualevn，這樣就可以同時在一台機器上跑不同的 Python 環境）：
pip install jieba  基本斷詞用法，使用預設詞庫 Sample Code：</description>
    </item>
    
  </channel>
</rss>