


  




  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.48 with theme Tranquilpeak 0.4.3-BETA">
    <title>Machine Learning</title>
    <meta name="author" content="Fukuball">
    <meta name="keywords" content="">

    <link rel="icon" href="images/favicon.ico">
    
      <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.fukuball.com/tags/machine-learning/index.xml">
    

    
    <meta name="description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Machine Learning">
    <meta property="og:url" content="/tags/machine-learning/">
    <meta property="og:site_name" content="I am Fukuball">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="I am Fukuball">
    <meta name="twitter:description" content="我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 https://www.fukuball.com">
    
    

    
    

    
      <meta property="og:image" content="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=640">
    

    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://blog.fukuball.com/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="1">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://blog.fukuball.com/">I am Fukuball</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://blog.fukuball.com/#about">
    
    
    
      
        <img class="header-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=90" alt="" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="1">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://blog.fukuball.com/#about">
          <img class="sidebar-profile-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
        </a>
        <h4 class="sidebar-profile-name">Fukuball</h4>
        
          <h5 class="sidebar-profile-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://facebook.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-facebook"></i>
      
      <span class="sidebar-button-desc">Facebook</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/fukuball" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://blog.fukuball.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">Blog</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      
        

      
      <div id="main" data-behavior="1"
        class="
               hasCoverMetaIn
               ">
        
          <section class="postShorten-group main-content-wrap">
            
            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/ji-yu-lstm-shen-du-xue-xi-fang-fa-yan-fa-er-cheng-de-zhang-yu-sheng-ge-ci-chan-sheng-mo-xing-zhi-jing-zhang-yu-sheng/">
          基於 LSTM 深度學習方法研發而成的張雨生歌詞產生模型，致敬張雨生
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2018-03-27T08:16:40&#43;08:00">
        
  March 27, 2018

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<p>之前看到〈<a href="http://mp.weixin.qq.com/s/uYiq2knJ0rrzdpuvbdmWhw">『致敬赵雷』基于TensorFlow让机器生成赵雷曲风的歌词</a>〉這篇文章覺得非常有趣，因此一直都想自己動手試試看，中國有趙雷，那台灣要找什麼值得紀念的音樂人來作這個歌詞機器學習模型呢？我想<a href="https://zh.wikipedia.org/wiki/%E5%BC%B5%E9%9B%A8%E7%94%9F">張雨生</a>應該會是台灣非常值得令人紀念的音樂人之一了。</p>

<p>程式的基礎我使用了之前在 GitHub 上有點小小貢獻的一個 Project 作為程式碼基礎，這個 Project 是 <a href="https://github.com/hit-computer/char-rnn-tf">char-rnn-tf</a>，可以用於生成一段中文文本（訓練與料是英文時也可以用於生成英文），訓練語料庫我收集了張雨生的百餘首歌詞（包含由張雨生演唱或作曲的歌詞），由於這樣的歌詞語料還是有些不足，因此也加入了林夕、其他著名歌詞、新詩作為輔助，整個語料庫大致包含 74856 個字、2612 個不重複字（其實語料庫還是不足）。</p>

<p>演算法基本上就是 LSTM，細節在此就不多加著墨，若有興趣可以在<a href="https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_rnns_lstm_work.html">這篇文章</a>了解一下，沒有時間的人，也可以看看 char-rnn-tf 這個 Project 作者所做的這張圖（見下圖），對概念了解一下。</p>

<p><img src="https://github.com/hit-computer/char-rnn-tf/blob/master/model.jpg?raw=true" alt="https://github.com/hit-computer/char-rnn-tf/blob/master/model.jpg?raw=true" /></p>

<p>相關程式碼我放在這邊：<a href="https://github.com/fukuball/Tom-Chang-Deep-Lyrics">Tom-Chang-Deep-Lyrics</a>，如何安裝環境、如何訓練、如何生成歌詞，基本上都寫在 Readme 了，大家可以前往瞧瞧。</p>

<hr />

<h3 id="歌詞產生結果">歌詞產生結果</h3>

<h4 id="範例一-夢想">範例一：夢想</h4>

<p>訓練完模型之後（用 macbook air 大致上需要 1 天的時間），由於大眾對張雨生歌詞的印象應該就是「我的未來不是夢」，因此我首先使用「夢想」作為 seed，結果產生歌詞如下：</p>

<pre><code>夢想會有心
我不願再區福　也不是一種把你一樣偷偷
我的心中有無奈

在我的心裡流　你的身影　你的轉身　你的沈靜　框進畫裡印象派的意

我有個朋友聽我說故舊　這一路悠揚的街長
我是天堂飄輝在天空裡
期待愛人看不同的眼睛
我等待與你身邊
你的歡念　你的灑明　在我心底都是飄逸水墨

我想你　愛過了我的一切
為你一起孤定我的美麗
</code></pre>

<p>產生的結果，歌詞機器學習模型先把詞補成句子<strong>「夢想會有心」</strong>，其實補得蠻好的啊！</p>

<p><strong>「我不願再區福　也不是一種把你一樣偷偷 我的心中有無奈」</strong></p>

<p>這邊雖有錯字，但也不至於不能理解。</p>

<p><strong>「在我的心裡流　你的身影　你的轉身　你的沈靜　框進畫裡印象派的意」</strong></p>

<p>這裡則結合了一首新詩，自創了歌詞。&rsquo;</p>

<p><strong>「我有個朋友聽我說故舊　這一路悠揚的街長」</strong></p>

<p>這一句歌詞結合了張雨生的歌曲永公街的街長，說明歌詞機器學習模型的確有張雨生的影子，但悠揚的街長感覺怪怪的 XD</p>

<hr />

<h4 id="範例二-我的未來不是夢">範例二：我的未來不是夢</h4>

<p>從上一個範例，我們可以了解這個歌詞機器學習模型的效果還算不錯，且看起來比起〈<a href="http://mp.weixin.qq.com/s/uYiq2knJ0rrzdpuvbdmWhw">『致敬赵雷』基于TensorFlow让机器生成赵雷曲风的歌词</a>〉這篇所產生的歌詞還要好，仔細看趙雷歌詞產生的結果就會覺得歌詞有點不知所云，而我這邊訓練完的結果，看起來語意會比較明確一些。</p>

<p>接著上個範例，我們來試試看「我的未來不是夢」作為 seed。</p>

<pre><code>我的未來不是夢
我只是真的勇敢

我是一棵秋天的樹　枯少的葉片顯過我
如果有愛能值得這些愁
這個世界保持安靜的距離

熱鬧容易　難得孤寂
我才捨不得問自己
從哪裡來又沒有用　感情和你的歸懷

你是我的姊妹
你是我的 BABY
讓我們的是 BAD BOY　難道我曾經默默縱容　那全是你犯的錯　喔

你贏　我不想與你再爭辯
我說你贏　我再也不想爭辯　請你轉過頭　我有哭泣的尊嚴
我是你的 BABY　我說得再多
</code></pre>

<p><strong>「我的未來不是夢 我只是真的勇敢」</strong></p>

<p>如果少了「只」感覺會更好一些。</p>

<p><strong>「我是一棵秋天的樹　枯少的葉片顯過我 如果有愛能值得這些愁」</strong></p>

<p>這邊則是致敬了張雨生的「我是一棵秋天的樹」的歌詞。</p>

<p><strong>「你是我的姊妹」、「讓我們的是 BAD BOY」、「你贏　我不想與你再爭辯」</strong></p>

<p>這邊則結合了「姊妹」「BAD BOY」「你贏」三首歌詞，有些地方說得通，但其實很難讓整首歌詞語意一致，這是目前這個歌詞機器學習模型需要改善的地方。</p>

<hr />

<h4 id="範例三-永遠不回頭">範例三：永遠不回頭</h4>

<p>接下來試試經典歌曲「永遠不回頭」作為 seed。</p>

<pre><code>永遠不回頭
我只能誠實你只能真摰　你無需幻想你無需作夢

當舞台來喝采聲星　為你的心呢的旋律
我是如此的愛　被我無深在心靈
我的愛沒有書香的清澈　我的愛沒有酒醇的狂語
我的愛沒有鑽石的雨　我是真的 人是假的唯

我要你的愛不曾衰竭　只要越來越不知道那無遲

別用你的眼　用我的心
好好一些感動

我在你心裡面看不見
我的世界裡用天地溫柔
永遠的感覺你和你
</code></pre>

<p><strong>「永遠不回頭 我只能誠實你只能真摰　你無需幻想你無需作夢」</strong></p>

<p>語意上蠻貼切的。</p>

<p><strong>「別用你的眼　用我的心 好好一些感動」</strong></p>

<p><strong>「我在你心裡面看不見 我的世界裡用天地溫柔 永遠的感覺你和你」</strong></p>

<p>這邊看不出是致敬哪首歌詞，比較像是歌詞機器學習模型自己從看過的語料中創作出來的，在字彙這麼少的情況下能夠這樣創作真的很不容易，想想自己小時候寫作文就知道這有多難，機器等於是一天學會作文。</p>

<hr />

<h4 id="範例四-我學會飛翔">範例四：我學會飛翔</h4>

<p>接下來使用「我學會飛翔」作為 seed，這是範例中唯一由張雨生寫的歌詞，其他範例都是合作過的歌曲（雖然也都放在語料庫中），哎呀，真不會選範例！</p>

<pre><code>我學會飛翔
深空之前如何困見　那裡有愛的人
為誰呼喚我　那一天我依稀回來
我們的愛情是迷霧　我總是深彎

我不能相信　何難自己

當風和日麗　陽春三月天
燦爛的心境　醉人的詩篇
或許紅磚上　或許小湖沿
繽紛的足跡　飛揚的笑靨
純潔的性靈　仿如剛清醒
又似蒙蔽　你的輪廓　在汗水中穿梭的角梗
吐著濃濃的煙霧恣任冥思出星
燦爛的陽光溫暖不到這樣的角落
一想到你呀 就讓我快樂
就好比蜻猴呀穿梭梭林　聚成了身邊
</code></pre>

<p><strong>「我們的愛情是迷霧」</strong></p>

<p>這也是歌詞機器學習模型自己創作出來的，覺得蠻厲害的。</p>

<p><strong>「當風和日麗　陽春三月天 燦爛的心境　醉人的詩篇」</strong></p>

<p>這邊的詞有點高水準，所以其實是 Overfitting，抄了張雨生的「三月的天真」這首歌詞。</p>

<hr />

<h4 id="範例五-沒有菸抽的日子">範例五：沒有菸抽的日子</h4>

<p>接下來試試另一首經典歌曲「沒有菸抽的日子」作為 seed。</p>

<pre><code>沒有菸抽的日子 喔

蒼匆的年宴不想留念你的存在
等待那一天鐘
不能說
我不會知道該怎麼做

當秋天葉落　西風來臨後
樹靈的枯枝　曳搖千萬萬　搖積不再有不停　是否不能呼吸
你也能夠 不顧

常常常覺得對自己嗎　難道我真的更好

oh 不醉 就學不會

好男人是不是都躲在家裡
為何身邊六塊肌全對女人沒興趣
有時我真懷疑 纏綿悱惻的劇情
幸福結局 只出現在日劇
女人的心 好比 放大鏡
</code></pre>

<p><strong>「沒有菸抽的日子 喔」</strong></p>

<p><strong>「oh 不醉 就學不會」</strong></p>

<p>這兩句真的喔的很貼切啊！很可惜後面 Overfitting 了～</p>

<hr />

<h4 id="範例六-漂向北方">範例六：漂向北方</h4>

<p>最後我們來試試不是張雨生的歌曲看看，刻意選了近期還蠻紅的歌曲「漂向北方」作為 seed。</p>

<pre><code>漂向北方 別問我家鄉
直到昏景 等到癡邊

我期待　在我的窗中
你給我一個感覺
肯定的愛情
擁有工作我的愛　請我的愛沒有書醇的狂向
直到傻風上煙中深起
你知道我不會猜你

我的笑容隨著煦煦和風 溫暖吹襲一股月色迷濛
我的歌聲衝出節節喉嚨 騷動八方就該天地星座 快同我怒吼

大度能寬容 滄海樂悠悠 誰又不想放下身段快意自由
失意也會有 得意也會有 反正都是生不帶來死不帶走
</code></pre>

<p><strong>「漂向北方 別問我家鄉 直到昏景 等到癡邊」</strong></p>

<p>補完的詞還不錯，「等到癡邊」比較不能理解，但算是有趣的創作吧。</p>

<p><strong>「我期待 在我的窗中 你給我一個感覺 肯定的愛情」</strong></p>

<p>致敬了我期待，感覺真的有在期待的感覺。</p>

<p>後面好像 Overfitting 了張雨生的「門外還有愛」，但整首詞的語意還算一致，算是一個不錯的結果。</p>

<hr />

<p>以上就是這個「基於 LSTM 深度學習方法研發而成的張雨生歌詞產生模型」的實驗結果，產生的詞算是可讀，而且有些還蠻有意思的，比較大的問題是上下文的語意可能會不一致，這樣的問題目前也有很多論文在解了，大體上就是用多層的 LSTM，可以將句子為 level 做 Encode 之後做一層 LSTM，將段落為 level 做 Encode 之後做一層 LSTM，結合原本的字詞 level 的 LSTM 模型，應該就可以做出上下文語意一致的歌詞產生模型了，如果大家有做出來，別忘了分享一下啊！</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/ji-yu-lstm-shen-du-xue-xi-fang-fa-yan-fa-er-cheng-de-zhang-yu-sheng-ge-ci-chan-sheng-mo-xing-zhi-jing-zhang-yu-sheng/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-16-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 16 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2018-02-12T10:24:38&#43;08:00">
        
  February 12, 2018

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-15-jiang-xue-xi-bi-ji/">第 15 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講中我們學到了如何使用矩陣分解方法來解推薦問題，機器學習技法課程也到這邊告一段落了，這一講終將會總結回顧一下我們在機器學習技法中學到的所有機器學習演算法，也許還有許多算法沒有介紹到，但基本概念都可以延伸。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-01.png">
</p>

<h3 id="特徵技巧-kernel">特徵技巧：Kernel</h3>

<p>我們學習到了如何使用 Kernel 來表現資料特徵，使用到 Kernel 技巧的相關演算法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-02.png">
</p>

<h3 id="特徵技巧-aggregation">特徵技巧：Aggregation</h3>

<p>我們也可以使用 Aggregation 方法來結合資料特徵，藉以合成更強大的學習演算法，使用到 Aggregation 技巧的相關演算法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-03.png">
</p>

<h3 id="特徵技巧-extration">特徵技巧：Extration</h3>

<p>我們可以使用 Extration 技巧來取得重要的資料特徵，使用到 Extration 技巧的相關演算法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-04.png">
</p>

<h3 id="特徵技巧-low-dim">特徵技巧：Low-Dim</h3>

<p>我們也會使用降維這個特徵技巧來取得資料的重要特徵，用到降維技巧的相關演算法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-05.png">
</p>

<h3 id="優化技巧-gradient-decent">優化技巧：Gradient Decent</h3>

<p>在類神經網路大量用到了 Gradient Decent 技巧來進行 Error 優化，用到 Gradient Decent 技巧的相關演算法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-06.png">
</p>

<h3 id="優化技巧-equivalent-solution">優化技巧：Equivalent Solution</h3>

<p>在許多困難的問題，我們很難找到優化的方法，我們會使用 Equivalent Solution 找到優化的方法，例如 Dual SVM 我們使用 covex QP、Kernel LogReg 我們用 representer、PCA 我們用 eigenproblem 來解。</p>

<p>未來若需要發展自己的演算法，也可以朝 Equivalent Solution 去想優化方法，只是這可能需要大量的數學推理知識。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-07.png">
</p>

<h3 id="優化技巧-multiple-steps">優化技巧：Multiple Steps</h3>

<p>有一些演算法我們會用 Multiple Steps 來一步一步進行優化，，用到 Multiple Steps 技巧的相關演算法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-08.png">
</p>

<h3 id="過擬似技巧-正規化">過擬似技巧：正規化</h3>

<p>由於演算法的能力越來越強，也因此很容易過擬似（Overfitting），所以我們必須要有方法來避免過擬似，其中一個方式就是正規化，我們大致學過的正規化方法如下：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-09.png">
</p>

<h3 id="過擬似技巧-validation">過擬似技巧：Validation</h3>

<p>另外我們也需要使用 Validation 方法讓我們在訓練過程就可以避免過擬似，在機器學習技法中我們學到的一些演算法有因為演算法特性而發展出來的 Valdation 方法：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-10.png">
</p>

<h3 id="機器學習叢林">機器學習叢林</h3>

<p>林軒田老師在機器學習技法課程的一開始就有放過這樣一張投影片，我們進入的是一個機器學習的叢林，從一開始可能對這投影片的所有演算法都不了解，但在這課程的尾聲我們重新回顧，相信大家多少都已經認識了這個叢林的險惡，也了解這個叢林是個多麽有趣與豐富！</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-16-11.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-16-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-15-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 15 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2018-02-12T09:03:17&#43;08:00">
        
  February 12, 2018

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-14-jiang-xue-xi-bi-ji/">第 14 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講介紹了 RBF Network，基本上就是透過距離公式及中心點來對資料點進行投票的一個算法，這一講將介紹矩陣分解系列的算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-01.png">
</p>

<h3 id="推薦系統問題">推薦系統問題</h3>

<p>之前的課程中曾經提到過推薦系統的問題，我們的資料集是使用者對電影的評分，希望讓機器學習算法學習到可以推薦使用者也會高評分的電影，這樣的問題 Netflix 曾經舉行過競賽。我們如何解這樣的問題呢？</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-02.png">
</p>

<h3 id="類別編碼">類別編碼</h3>

<p>這個問題首先會需要進行編碼，因為使用者資料可能只是一連串的使用者編號，這是類別資料，不太能用來直接用於運算（僅有 Decision Tree 可以直接用來做類別運算），所以我們會先將類別資料編碼成數值資料，編碼的方法常用 binary vector encoding，如下所示：</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-03.png">
</p>

<h3 id="特徵選取">特徵選取</h3>

<p>我們可以將使用者評分電影的過程視做一組特徵轉換，能夠將 X 轉換成 Y，轉換的過程如果分成兩個矩陣 Wni、Wim，那左邊的矩陣代表的意義就是使用者對電影中哪些特徵很在意，右邊的矩陣代表的意義就是電影中有哪些特徵成份。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-04.png">
</p>

<h3 id="矩陣分解">矩陣分解</h3>

<p>因此這個推薦問題可以寫成底下的矩陣分解，首先我們把評分做成一個 Rnm 矩陣，需要嘗試把它分解成 VT W 兩個矩陣，使用者的喜好會對映一組特徵，電影的成分也會對應到這組特徵，我們要將這組特徵萃取出來。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-07.png">
</p>

<h3 id="矩陣分解學習-alternating-least-squares">矩陣分解學習 Alternating Least Squares</h3>

<p>矩陣分解學習算法如下，首先決定特徵維度 d，然後隨機初始化使用者對特徵的喜好 Vn，電影中特徵的強度 Wm，然後優化 Ein，先固定 Vn 去優化 Wm，再固定 Wm 去優化 Vn，如此重複直到收斂，這樣就可以得到與 Rnm 最相似的 Vn X Wm。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-09.png">
</p>

<h3 id="linear-autoencoder-vs-矩陣分解">Linear Autoencoder vs 矩陣分解</h3>

<p>之前介紹過的 Linear Autoencoder 基本上也是一種矩陣分解，但意義上有些不同。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-10.png">
</p>

<h3 id="使用隨機梯度下降法解矩陣分解">使用隨機梯度下降法解矩陣分解</h3>

<p>我們也可以使用隨機梯度下降法來解矩陣分解的問題，與 Alternating Least Squares 比起來，隨機梯度下降法速度較快，且比較簡單。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-11.png">
</p>

<h3 id="觀察-error-function">觀察 Error Function</h3>

<p>隨機梯度下降是計算某個點的梯度來進行優化，所以我們可以用一個點作為範例來看看梯度優化的式子，例如我們要對 Vn 進行優化時，只要對 Vn 進行偏微分，即可得到優化的數學式，同理要對 Wm 進行優化時，也只要對 Wm 進行偏微分即可。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-12.png">
</p>

<h3 id="矩陣分解學習-sgd">矩陣分解學習 SGD</h3>

<p>因此 SGD 矩陣分解學習算法如下，首先一樣先決定特徵維度 d，然後隨機初始化使用者對特徵的喜好 Vn，電影中特徵的強度 Wm，然後隨機選取 Rnm 中的一點，計算 Rnm - WmVn，然後各自做偏微分取得新的 Vn 及 Wm 直到收斂。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-13.png">
</p>

<h3 id="矩陣分解學習-sgd-實務">矩陣分解學習 SGD 實務</h3>

<p>林軒田老師分享了矩陣分解學習 SGD 算法在實務上的應用，由於在 KDDCup 2011 年的問題中，測試資料與訓練資料是在不同時間收集到的，因此可以說是不同的資料分布，在做訓練上可能需要將時間的因素考量進去這樣未來做預測才會準確。</p>

<p>使用 SGD 矩陣分解學習算法，我們可以在訓練過程中讓後半段的訓練資料都選取較新的訓練資料，因此可以將時間因素也同時考量在訓練過程中了，這樣的調整讓台大隊伍拿下了比賽的冠軍。如果了解了機器學習算法的細節，我們就可以因應不同的問題做調整。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-14.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講中，我們學到了如何使用矩陣分解方法來解推薦問題，矩陣分解的演算法可以使用 Alternating Least Squares 或是隨機梯度下降法，雖然目前網路上已經一大堆矩陣分解程式可以使用，但當遇到要適度調整演算法的時候，了解實作演算法細節便可以自行調整以解決真實世界會遇到的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-15-18.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-15-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-14-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 14 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2017-10-02T09:10:41&#43;08:00">
        
  October 2, 2017

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-13-jiang-xue-xi-bi-ji/">第 13 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講介紹了深度學習神經網路，基本上神經網路林軒田老師只說明了兩講，這一講將進入一個新的 Machine Learning 演算法 Radial Basis Function Network（我個人不太覺得這個是神經網路演算法），並延伸介紹了其中會使用到的 K-means 分群演算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-01.png">
</p>

<h3 id="回顧一下-gaussian-svm">回顧一下 Gaussian SVM</h3>

<p>Gaussian Kernel 也稱為是 Radial Basis Function(RBF)，定義一種距離關係，Gaussiam SVM 也就是使用這些 RBF 經過線性組合的預測模型。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-02.png">
</p>

<h3 id="rbf-network-的定義">RBF Network 的定義</h3>

<p>RBF Network 定義是資料點與代表性中心點投票出來的結果作為預測，其中每個中心點具有代表性，資料點經過 RBF 距離公式的轉換之後（距離越小，越有影響力），再經過 beta 投票。</p>

<p>而 RBF Network 要學習的參數就是中心點 u 以及每個中心點距離公式的權重 beta。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-04.png">
</p>

<h3 id="full-rbf-network">Full RBF Network</h3>

<p>了解定義之後，我們就要求出 RBF Network 最佳化的 u 及 beta，一種情況我們是將所有的資料點都當成是重要的中心點，這種情況，然後 beta 直接用 y 當成加權，這樣就是所謂的 Full RBF Network，這樣的預測模型就完全沒有訓練過程，只要將所有的點記下來，然後用 RBF 距離公式計算投票來定義新的資料點應該是屬於什麼。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-06.png">
</p>

<h3 id="nearest-neighbor">Nearest Neighbor</h3>

<p>由於每次預測都要使用所有的訓練資料點來計算預測結果很費力，我們從距離公式可以了解，投票結果大部分會受最近距離的中心點影響，所以我們其實可以只看最近一點中心點的 RBF 投票結果就好，這就是所謂的 Nearest Neighbor，如果是看最近 k 點中心點的 RBF 投票結果就是所謂的 K Nearest Neighbor。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-07.png">
</p>

<h3 id="beta-不使用相同權重">Beta 不使用相同權重</h3>

<p>Full RBF Network 的 Beta 使用相同權重其實就是 Nearest Neighbor，現在我們不想要讓 Beta 使用相同權重，因此要透過訓練資料來計算出最佳的 Beta 權重，最佳的 Beta 有公式解，且這個結果會讓 RBF Network 得到 Ein=0，這樣其實會有 overfitting。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-08.png">
</p>

<h3 id="使用更少的中心點來正規劃">使用更少的中心點來正規劃</h3>

<p>我們從 SVM 中看到 SVM 其實只考慮了幾個點（SV）來決定胖胖的線，那 RBF Netwrok 應該也可以用同樣的概念來找出幾的重要的中心點，然後再來計算出最佳的 beta 就好，這樣就可以避免 overfitting。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-10.png">
</p>

<h3 id="中心點如何決定">中心點如何決定？</h3>

<p>中心點如何決定並不是一個簡單的問題，我們必須要找出讓訓練資料跟各中心點距離最小，然後又要找出訓練資料最佳的分組結果，這是無法找出最佳化公式的問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-11.png">
</p>

<h3 id="分別最佳化-1">分別最佳化（1）</h3>

<p>其中一個方法就是做最佳化，首先假設我們已經決定了最佳的中心點，那要算出資料點應該要分在哪個中心點的分組就非常容易，就看跟哪個中心點距離最小就可以了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-12.png">
</p>

<h3 id="分別最佳化-2">分別最佳化（2）</h3>

<p>假設現在分組已經決定了，找出最佳化的中心點也很容易，將距離公式進行微分資料就可以知道最佳的中心點就是分組所有資料點的平均。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-13.png">
</p>

<h3 id="k-means-算法">K-Means 算法</h3>

<p>經過以上的推導，我們可以知道 K-Means 演算法如下：</p>

<ol>
<li>先隨意從資料點找出 k 個資料點當成是初始的中心點</li>
<li>計算資料點的分組</li>
<li>資料分完組之後，計算各組資料點的平均當成是新的中心點</li>
<li>重複 2 跟 3 直到中心點不再改變</li>
</ol>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-14.png">
</p>

<h3 id="rbf-network-搭配-k-means-中心點">RBF Network 搭配 K-means 中心點</h3>

<p>現在我們將 RBF Netwrok 搭配 K-means 算出的中心點來做出先的 RBF Network 模型：</p>

<ol>
<li>使用 K-means 計算出 k 個最重要的中心點（k 要設幾個要自己試，可以用 cross-validation）</li>
<li>將訓練資料 x 經過 k 個 RBF 距離公式進行特徵轉換</li>
<li>將特徵轉換後的訓練資料跑 linear model 計算最佳化的 beta</li>
<li>如此就得到了訓練完後具有正規化性質 RBF Network 了</li>
</ol>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-15.png">
</p>

<h3 id="k-means-的問題">K-means 的問題</h3>

<p>K 很難決定、初始化的中心點也會影響分群的結果，這是目前無解的，我們只能透過 cross validation 來決定最好的 k 是幾個，而初始化的中心點只能多試只次來避面模型是落在局部最佳化。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-16.png">
</p>

<h3 id="總結">總結</h3>

<p>這一章介紹了 RBF Network，基本上就是透過距離公式及中心點來對資料點進行投票的一個算法，而中心點就是一種代表性，為了決定中心點，我們可以使用 K-means，了解 RBF Network 的核心概念之後，覺得 RBF 算是蠻簡單的，幾乎不太需要訓練，相對的，我也覺得 RBF Network 的用處比較沒那麼大。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-14-17.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-14-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-13-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 13 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2017-07-10T04:59:44&#43;08:00">
        
  July 10, 2017

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-12-jiang-xue-xi-bi-ji/">第 12 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講說明了什麼是類神經網路，以及類神經網路的核心演算法 Backpropagation，及如何使用 Gradient Decent 算法來計算最佳解，這一講將介紹類神經網路的延伸 - 深度學習。</p>

<p>不過林軒田的課程在深度學習的介紹上只有一講，其實並不是很深入，大家有興趣可以去找<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html">李宏毅老師的課程</a>來看看。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-01.png">
</p>

<h3 id="再看一次類神經網路">再看一次類神經網路</h3>

<p>讓我們再看一次類神經網路，我們知道類神經網路中每一層的神經元就是在做一些細微的 pattern 比對，那我們應該要怎麼設計類神經網路的結構呢？主觀上，你可以設計，客觀上，我們必須對神經網路進行驗證，神經網路的結構在類神經網路這樣的領域上也是個重要的議題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-02.png">
</p>

<h3 id="淺跟深">淺跟深</h3>

<p>既然結構是個重要的議題，那淺層跟深層的神經網路有何不同呢？深層的神經網路就是我們所謂的深度學習，一般的淺層神經網路，如果有足夠多的神經元其實已經就能夠解決蠻多問題了，深層神經網路理論上會更強，相對的運算量也會比較大、容易 overfitting，但深層的神經網路是有其物理意義的，也因此深度學習在近年資料量多、運算速度變快之後開始快速發展。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-03.png">
</p>

<h3 id="深度學習的物理意義">深度學習的物理意義</h3>

<p>我們知道類神經網路的每個神經元就是在做 raw data 的 pattern 比對，每個神經元只做非常細微的比對，如果我們再加上一層 layer，那這一層 layer 就是在做前一層 pattern 的 pattern 比對，比對的 pattern 會比前一層更具體，因此加上越多 layer 的話，就能夠將更具體的 pattern 找出來。</p>

<p>因為這樣的特性，我們會將深度學習的方法用在比較無法具體找出 feature 的訊號問題上，像是影像處理、語音處理等等，讓深度學習來幫我們找出具體 pattern。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-04.png">
</p>

<h3 id="深度學習的關鍵問題與技術">深度學習的關鍵問題與技術</h3>

<p>深度學習所面臨的關鍵問題與技術大概有以下幾項，第一，如何覺得結構是一個問題，有時我們會利用一些領域知識來決定神經網路的結構，比如在影像問題上，我們會使用 CNN 這種特殊結構的神經網路。</p>

<p>另外由於模型很複雜，我們需要更多的資料來做計算，且需要使用 regularization 方法來避免 overfitting，常用的方法有 dropout 及 denoising。</p>

<p>且深度學習是一個比較難最佳化的問題，可能會有很多 local minimum，因此初始的 weight 也會影響最佳化的結果，因此通常需要使用 pre-training 這樣的方法來初始一個比較好的 weight 再來進行最佳化。</p>

<p>林軒田老師認為深度學習近期會有最麽大的進展，regularization 跟 pre-training 的方法也是很重要的原因，在這一講也主要再講這兩個部分。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-05.png">
</p>

<h3 id="兩步最佳化深度學習">兩步最佳化深度學習</h3>

<p>深度學習演算的架構大概就是先使用 pre-train 把每一層之間的 weight 先算一遍，然後在使用這些 weight 進行 backprop 運算微調。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-06.png">
</p>

<h3 id="能保留資訊的-encoding">能保留資訊的 Encoding</h3>

<p>我們之後 weight 在神經網路中就是一種 featrue transform，其實就是一種 encoding，如果我們的 weights 能夠保留最多原本的資訊，那就是一種好的 encoding。</p>

<p>我們對每一層的 pre-train 就是要找出每一層的這種好的 encoding。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-07.png">
</p>

<h3 id="能保留資訊的神經網">能保留資訊的神經網</h3>

<p>我們先把神經網的每層獨立拿出來做 pre-traing，要讓這一層能夠保留最多原本的資訊，應該要怎麼做呢？其實直觀來想，只要能夠讓原本的 x 經過神經元的處理之後還是跟原來的 x 很接近，那就是一個能夠保留原本資訊的 weight 神經網了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-08.png">
</p>

<h3 id="autoencoder-的功效">Autoencoder 的功效</h3>

<p>這種 Autoencoder 對於機器學習來說有什麼作用呢？對於 Supervise Learning 來說，這種 information preserving 的神經網是一種對原始輸入合理的轉換，相當於在結構中學習了資料的表達方式，因此能夠重組成原本的資料。</p>

<p>對於 Unsupervise Learning 來說可以用來做 outlier detection，比如 decode 後的某一筆資料與原本很不相似，那這筆資料可能就是 outlier。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-09.png">
</p>

<h3 id="基本的-autoencoder">基本的 Autoencoder</h3>

<p>基本的 Autoencoder 可以看成是單層的神經網路，輸入為 X，輸出也是 X。有了 Autoencoder 我們就可以對 Deep Learning 的每一層神經網做 per-train 了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-10.png">
</p>

<h3 id="deep-learning-的正規化">Deep Learning 的正規化</h3>

<p>Deep Learning 的正規化方式可以用之前學過的概念來引用，像是加上一些限制或是做 early stopping，但這邊要介紹在 Deep Learning 這個問題上比較特別的正規化方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-12.png">
</p>

<h3 id="overfitting-的原因">Overfitting 的原因</h3>

<p>我們再來回想一下 Overfitting，其中一個原因就是 dataset 中有雜訊。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-13.png">
</p>

<h3 id="處理雜訊">處理雜訊</h3>

<p>我們要避免 Overfitting 一個方式就是去除 dataset 中的雜訊。這邊我們轉換一個想法，如果我們為原本的 dataset 加上一些雜訊，會如何呢？</p>

<p>會這樣做的原因，也是來自 Autoencoder 這樣的概念，如果我們將原本的 dataset 加上一些雜訊，在 Autoencoder 中也能 decode 出原本的沒有雜訊的 dataset，那這個 Autoencoder 就是比較能夠忍受雜訊的 Autoencoder，相對的也就是能夠避免 overfitting 了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-14.png">
</p>

<h3 id="linear-autoencoder">Linear Autoencoder</h3>

<p>上面介紹的 Autoencoder 本質上是非線性的 Autoencoder，如果我們要使用 Linear Autoencoder，那是否會跟其他的 linear 最佳化問題一樣有個公式解呢？去掉原本神經網的非線性轉換後，得到 Linear Autoencoder 的表達式為：h(x)=WW&rsquo;x</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-15.png">
</p>

<h3 id="linear-autoencoder-error-function">Linear Autoencoder Error Function</h3>

<p>如此 Error Function 就會變成如下所示，我們要讓 X 經過轉換之後差距越小越好，我們要找到最佳的 WW&rsquo;，在線性代數的特性上，WW&rsquo; 可以做 SVD 表示成 VgammaV&rsquo;，我們的問題就變成了最佳化 gamma 與 V。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-16.png">
</p>

<h3 id="先最佳化-gamma">先最佳化 gamma</h3>

<p>讓我們先最佳化 gamma，在這邊的特性上我們可以看出 I - gamma 越小越好，也就是 gamma 越多 1 越好，gamma 最多的 1 理論上可以到 d&rsquo; 個。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-17.png">
</p>

<h3 id="再最佳化-v">再最佳化 V</h3>

<p>這邊說明的不是很清楚，可能需要請大家自行去看看林軒田老師的說明，結論上最佳化的 V 就是特徵值最大的 XX&rsquo; 的特徵向量。結合以上就可以找出 Linear Autoencoder 的公式解了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-18.png">
</p>

<h3 id="pca">PCA</h3>

<p>PCA（主成份分析法）是另一種 Linear Autoencoder 方法，只是這邊的輸出變成是 X - Xbar 去做 Autoencoding，PCA 算是比較知名的方法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-19.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講說明了什麼是 Deep Learning，及在 Deep Learning 中常常會使用到的 pre-train 方法 Autoencoder，Autoencoder 基本上是一種 unsupervise learning，我們也可以使用 Autoencoder 來避免 overfitting，在線性的 Autoencoder 我們可以使用 PCA。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-13-20.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-13-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-12-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 12 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2017-06-01T07:24:52&#43;08:00">
        
  June 1, 2017

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/">第 11 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，大家不一定要記住所有演算法的細節，但大致上對 Aggregation 的方式有些概念就可以啦！</p>

<p>如果要記，就要記住這個核心概念：Aggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。</p>

<p>這一講我們將開始介紹現在很紅的類神經網路機器學習演算法。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-01.png">
</p>

<h3 id="perceptron-的線性組合">Perceptron 的線性組合</h3>

<p>我們先看一下最簡單的類神經網路，其實可以看成是多個 Perceptron 的線性組合，如果之前學過的 Aggregation，這樣組合多個 Perceptron 就能帶來更複雜的學習效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-02.png">
</p>

<h3 id="單層類神經網路的限制">單層類神經網路的限制</h3>

<p>單層類神經網路可以透過組合越多的神經元來模擬曲線的邊界，用以解決更複雜的問題，但還是有其限制，比如 XOR 及雙曲線這樣的邊界，無論如何都無法透過單層的線性組合來做到，因此我們需要想辦法再延伸單層類神經網路。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-04.png">
</p>

<h3 id="多層類神經網路">多層類神經網路</h3>

<p>所以就延伸出了多層類神經網路，就跟邏輯閘設計一樣，多層的架構就可以做出 XOR 這樣的邏輯，多層類神經網路就可以模擬各種各樣的邊界了，一般人家在說的類神經網路其實也就是說多層類神經網路。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-05.png">
</p>

<h3 id="在生物上的關係">在生物上的關係</h3>

<p>類神經網路與生物上的神經網路有什麼關係呢？其實類神經網路的架構就是有想要模擬生物上的神經網路，但不完全就跟生物上的神經網路一樣，就跟飛機是模擬鳥類，但跟鳥類飛行實際如何運作並不完全一致。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-06.png">
</p>

<h3 id="類神經網路的-output">類神經網路的 output</h3>

<p>我們從這個圖探討類神經網路的 output，在 output 之前我們可以把它看成是一個對 x 資料的轉換，x 資料經過各個神經元結合轉換之後，再透過 output 層的線性組合做 voting，如果要做分類就對最後的 output 加上 sign 函數，要做迴歸就直接輸出 output 結果，如果要輸出機率值，就對最後的 output 加上 theta 函數，如此就可以用來解各種常見的 Machine Learning 問題。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-07.png">
</p>

<h3 id="特徵轉換">特徵轉換</h3>

<p>我們再往前看一下 x 在 output 前經過的特徵轉換層，這些轉換層是由許多神經元組成，用來計算複雜的特徵轉換，每個神經元也會做組合與輸出，這邊的組合如果是用線性組合，那在數學意義上，所有的神經網路就是單純的在做線性組合，所以並無法做到複雜的特徵轉換。</p>

<p>所以這邊的組合要跟邏輯閘一樣使用類似 sign 函數來組合，才能組合出複雜的邊界，但 sign 函數組合並不是一個連續函數，因此比較難優化，所以我們會永 tanh 函數來逼近 sign 函數，如此就不僅可以模擬複雜邊界，然後計算也比較容易優化。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-08.png">
</p>

<h3 id="類神經網路假設">類神經網路假設</h3>

<p>如上述去界定類神經網路的架構後，我們可以把類神經網路畫成如下圖，我們會有 x 作為輸入，然後經過各層的 weight 計算後，再透過神經元的 tanh 組合，最後再輸出結果，如此我們只剩下如何去計算出各層的 weight 就可以訓練出類神經網路了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-09.png">
</p>

<h3 id="神經元的物理意義解釋">神經元的物理意義解釋</h3>

<p>這邊我們可以探討一下神經元的物理意義，我們可以看到前一層的輸出會作為後一層的輸入，如果達到一定程度神經元就會輸出結果，所以其實他就是在做兩層之間的匹配程度，越匹配就越能輸出結果，也就是每個神經元都是在學習某一種 pattern。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-10.png">
</p>

<h3 id="如何學習各層之間的-weight">如何學習各層之間的 weight？</h3>

<p>之前學過的 Gradient Boosting 可以用來解單層的類神經網路，但多層的類神經網路不容易用 Gradient Boosting 來解，這邊需要用 Stochastic Gradient Decent 來解會比較簡單一些。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-11.png">
</p>

<h3 id="backpropagation-演算法">Backpropagation 演算法</h3>

<p>計算類神經網路的 Gradient Decent 需要使用到 Backpropagation 演算法，這邊我跳過了數學推導過程，想要詳細了解我另外推薦<a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">李宏毅老師的講解</a>，演算法概觀如下：首先，需要先設定整個類神經網路的 weight value，然後 1. 隨機選取一個資料點 x，2. 計算 forward pass，3 計算 backward pass，4 調整 weight。</p>

<p>（forward 跟 backward pass 分別怎麼計算請看<a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">李宏毅老師的講解</a>）</p>

<p>通常實務上我們會重複 1 - 3 很多次之後做一個平均再去 update weight，這樣的做法叫做 mini batch。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-14.png">
</p>

<h3 id="nn-最佳化的問題">NN 最佳化的問題</h3>

<p>由於類神經網路非常複雜，有很多個凸點，所以很容易在學習過程中得到一個 local minimum 的結果。因此不同的起始 weight 可能會得到不同的最佳化結果，在訓練類神經網路時可以挑不同的起始 weight 來做訓練。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-15.png">
</p>

<h3 id="vc-dimension">VC Dimension</h3>

<p>類神將網路的 VC Dimension 為 V*D，V 是神經元數量，D 是權重的數量，所以如果神經網路的層數跟神經元多起來那 VC Dimension 就會很高，所以可能就會有 overfitting 的現象，這樣就需要做 regularization 來防止 overfitting。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-16.png">
</p>

<h3 id="early-stopping">Early Stopping</h3>

<p>除了使用一般的逞罰項來做 regularization 之外，類神經網路還是用了另一個方式來做到 regularization，這個方法叫 Early Stopping，至於哪時要 stop 呢？那就要做 validation。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-18.png">
</p>

<h3 id="總結">總結</h3>

<p>這一講說明了什麼是類神經網路，以及類神經網路的核心演算法 Backpropagation，下一講將介紹類神經網路的延伸 - 深度學習。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-19.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-12-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
              
  
    
      
        
      
    
  


  

<article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
  <div class="postShorten-wrap">
    
    <div class="postShorten-header">
      <h1 class="postShorten-title" itemprop="headline">
        <a class="link-unstyled" href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/">
          林軒田教授機器學習技法 Machine Learning Techniques 第 11 講學習筆記
        </a>
      </h1>
      
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2017-04-25T09:35:59&#43;08:00">
        
  April 25, 2017

      </time>
    
    
  </div>

    </div>
    <div class="postShorten-excerpt" itemprop="articleBody" style="margin-bottom: 50px;">
      <div class="main-content-wrap">
        

<h3 id="前言">前言</h3>

<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href="http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-10-jiang-xue-xi-bi-ji/">第 10 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>

<h3 id="範例原始碼-fukuml-簡單易用的機器學習套件-https-github-com-fukuball-fuku-ml">範例原始碼：<a href="https://github.com/fukuball/fuku-ml">FukuML - 簡單易用的機器學習套件</a></h3>

<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href="https://github.com/fukuball/fuku-ml">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>

<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href="https://github.com/fukuball/fuku-ml">FukuML</a> 了。不過我還是有寫 <a href="https://github.com/fukuball/FukuML-Tutorial">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>

<h3 id="熱身回顧一下">熱身回顧一下</h3>

<p>上一講的 Random Forest 演算核心主要就是利用 bootstrap data 的方式訓練出許多不同的 Decision Trees 再 uniform 結合起來。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-01.png">
</p>

<h3 id="adaboost-decision-tree">AdaBoost Decision Tree</h3>

<p>這一講接下來要介紹的 AdaBoost Decision Tree 其實乍看有些類似，但它的訓練資料集並不是透過 bootstrap 來打亂，而是使用之前 AdaBoost 的方式再每一輪資料計算加權 u(t) 去訓練出許多不同的 Decision Tree，最後再以 alpha(t) 的權重將所有的 Decision Tree 結合起來。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-02.png">
</p>

<h3 id="權重會影響演算法">權重會影響演算法</h3>

<p>由於 AdaBoost Decision Tree 會考慮到權重，因此應該要像之前介紹過的 AdaBoost 會將權重傳進 Decision Stump 一樣，AdaBoost Decision Tree 應該也要將權重傳進 Decision Tree 裡做訓練，但這樣就需要調整 Decision Tree 原本的演算法，我們不喜歡這樣。</p>

<p>轉換一個方式，也許我們可以一樣使用抽樣的方式來將訓練資料依造 u(t) 的權重做抽樣，這樣就可以直接將用權重抽樣玩的訓練資料集傳進 Decision Tree 做訓練，達到相同的效果，如此就不用改原本 Decision Tree 的演算法了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-03.png">
</p>

<h3 id="要使用-weak-decision-tree">要使用 Weak Decision Tree</h3>

<p>另外要注意的是，如果 AdaBoost Decision Tree 使用了 fully grown 的 Decision Tree，這樣 alpha(t) 就會變得無限大，如此訓練完的 AdaBoost Decision Tree 做預測時就只會參考這個權重無限大的 Decision Tree，這樣就沒有 Aggregation Model 的效果了，我們應該要避免這個問題，所以要使用弱一點的 Decision Tree，比如透過 pruned 來避免 Decision Tree fully grown。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-04.png">
</p>

<h3 id="特例-使用-extremely-pruned-tree">特例：使用 Extremely Pruned Tree</h3>

<p>如果 AdaBoost Decision Tree 使用了 Extremely Pruned Tree，比如限制樹的高度只有 1，那這樣其實就是之前學過的 AdaBoost，這是 AdaBoost Decision Tree 中的一個特例。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-05.png">
</p>

<h3 id="gradien-boost">Gradien Boost</h3>

<p>這邊的數學演算太過複雜，大家可以直接觀看影片學習，我這邊直接說數學推導最後得出來的結論。</p>

<p>AdaBoost 透過一些數學特性的推導之後，可得出圖中的式子，代表要最佳化 binary-output Error，這個式子可以換成是要算 real-output Error，這樣就是所謂的 Gradien Boost。</p>

<p>由於 real-output Error 的最佳化是一個連續函數，我們可以使用跟之前的 logistic regression 一樣的方式使用 gradient decent 找出最佳的 ita 及 h(x)。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-06.png">
</p>

<h3 id="gradien-boost-演算法">Gradien Boost 演算法</h3>

<p>Gradien Boost 演算法的數學推導這邊也請大家去看影片，我直接講數學推導完之後得到的結論，整個演算法看起來很簡單，第一步先使用 xn 與餘數 (yn - sn) 做訓練，得出 gt，第二步再使用 gt 對 xn 做資料轉換，再使用 gt(xn) 與餘數 (yn - sn) 做 linear regression 算出 alphat，最後使用 sn + alphat * gt(xn) 得出新的 sn，重複這個過程，將各個 Decision Tree 結合起來就是 Gradien Boost Decision Tree 了！</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-07.png">
</p>

<h3 id="blending-models">Blending Models</h3>

<p>課程到這邊已經介紹完了 Blending Models 的各種形式，有 uniform、non-uniform、conditional 的形式，uniform 可以帶來穩定性，non-uniform 及 conditional 可以帶來模型複雜度，但要小心 overfiting。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-08.png">
</p>

<h3 id="aggregation-learning-model">Aggregation Learning Model</h3>

<p>從上述的 blending 方式，我們可以發展出不同的 Aggregation Model，如 Badding 使用 uniform vote、AdaBoost 使用 linear vote by reweighting、Decision Tree 使用 conditional vote、GradientBoost 使用 linear vote by residual（餘數） fitting。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-09.png">
</p>

<h3 id="aggregation-model-的好處">Aggregation Model 的好處</h3>

<p>Aggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-11.png">
</p>

<h3 id="總結">總結</h3>

<p>在這一講，我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，基本上對大部分的 Aggregation Model 都有一些認識了。</p>

<p style="text-align:center">
    <img src="http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-12.png">
</p>

      </div>
      <p>
        <a href="https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/" class="postShorten-excerpt_link link"></a>
        
      </p>
    </div>
  </div>
  
</article>

            
            
  <div class="pagination-bar">
    <ul class="pagination">
      
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="https://blog.fukuball.com/tags/machine-learning/page/2/">
              <span></span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
      
      <li class="pagination-number"> </li>
    </ul>
  </div>


          </section>
        
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Fukuball. 
  </span>
</footer>

      </div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//www.gravatar.com/avatar/6c910ba730e0acfda9ee450eec9776e6?s=110" alt="" />
    
    <h4 id="about-card-name">Fukuball</h4>
    
      <div id="about-card-bio">我是林志傑，網路上常用的名字是 Fukuball。我使用 PHP 及 Python，對機器學習及區塊鏈技術感到興趣。 <a href="https://www.fukuball.com">https://www.fukuball.com</a></div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Co-Founder / Head of Engineering at OurSong
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Taipei, Taiwan
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center"></div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-23smart-contract-%E5%88%9D%E6%8E%A2%E5%BE%9E-bytecode-%E5%88%B0-solidity/">
                <h3 class="media-heading">Ethereum 開發筆記 2–3：Smart Contract 初探，從 Bytecode 到 Solidity</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Ethereum 上的 EVM（Ethereum Virtual Machine）可以執行程式，而 EVM 上的可執行程式基本上是 Bytecode 的形式，所以所謂的 Smart Contract 就是存放在 Ethereum 上的 Bytecode，然後可由 EVM 來執行。
Bytecode Smart Contract 直接用 Bytecode 寫 Smart Contract 我們來嘗試一下直接用 Bytecode 來寫 Smart Contract，以下這段程式碼主要內容是執行運算後，將運算結果存放在 0 這個位置：
PUSH1 0x03 PUSH1 0x05 ADD // 3 + 5 -&gt; 8 PUSH1 0x02 MUL // 8 * 2 -&gt; 16 PUSH1 0x00 SSTORE // 將 16 存到 0 這個位置  這段程式轉成 Bytecode 就是：
0x60 0x03 0x60 0x05 0x01 0x60 0x02 0x02 0x60 0x00 0x55  也就是：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-22geth-%E5%9F%BA%E7%A4%8E%E7%94%A8%E6%B3%95%E5%8F%8A%E6%9E%B6%E8%A8%AD-muti-nodes-%E7%A7%81%E6%9C%89%E9%8F%88/">
                <h3 class="media-heading">Ethereum 開發筆記 2–2：Geth 基礎用法及架設 Muti-Nodes 私有鏈</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">要連上 Ethereum 就需要安裝 Ethereum Node，在這邊我們選擇使用 Geth 來安裝 Ethereum Node，接下來就來一步一步的學學怎麼使用 Geth，甚至如何使用 Geth 來架設自己的 Ethereum 私有鏈。
安裝環境 首先我們在 AWS 上開啟兩台 Ubuntu 虛擬機器，記得開 t2.medium（2 vCPU, 4 GB RAM）這個規格以上才跑得動，硬碟可以開 100 G，Security Group 將 TCP 30303 打開，Ethereum Node 之間是用 30303 這個 port 來溝通的。
接下來使用以下指令安裝 Geth：
$ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:ethereum/ethereum $ sudo apt-get update $ sudo apt-get install -y ethereum  兩台虛擬機器都要安裝，應該幾分鐘就可以裝好了。
使用 Main Net 安裝完 Geth 之後，我們就可以透過 Geth 連上 Ethereum Network 了，我們就來連上 Main Net 看看：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-21ethereum-%E9%96%8B%E7%99%BC%E6%95%B4%E9%AB%94%E8%84%88%E7%B5%A1/">
                <h3 class="media-heading">Ethereum 開發筆記 2–1：Ethereum 開發整體脈絡</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在第一次接觸 Ethereum 應用程式開發時，會發現有各式各樣工具，不知要從何下手，我們用一個圖來說明一下與 Ethereum 互動時的整體脈絡及這之間的工具主要做了什麼事，了解之後自己就可以挑選開發時、甚至使用在產品上時要用什麼適合的工具了。
要在自己的機器接上 Ethereum 首先需要安裝 Ethereum Node，我們之前安裝的 Mist 其實就會在我們的機器上安裝 Ethereum Node 並同步帳本，而像這樣安裝 Node 並同步帳本甚至進行挖礦的軟體有很多，大家可以去選擇適合自己使用的。Mist 其實是將一個叫 geth 的軟體用 GUI 包裝起來，如果是開發者的話，可以選擇直接安裝 geth。
geth 提供了許多 API 指令可以讓我們跟 Ethereum 做互動，但有時下指令並不是那麼親和，所以 geth 提供了 RPC(Remote Procedure Calls) 與 IPC(Inter-process Communications) 兩種方式來與 geth 互動，如果你要在 local 機器連上 geth，那就可以使用 IPC；如果要讓遠端連上 geth，那就使用 RPC，可以開 HTTP 或 Web Socket 兩種方式來讓遠端使用。
以上就是 Ethereum 應用程式開發的基礎環境，接下來跟開發網頁應用程式一樣，Ethereum 應用程式也分成後端與前端，後端程式就是 Smart Contract，前端程式就是 Dapp。Smart Contract 可使用 Solidity 撰寫，目前也有許多其他語言可以撰寫 Smart Contract。Smart Contract 要在 Ethereum 上的 EVM 執行要先 Compile 成 Byte Code 之後，再透過 IPC 或 RPC 發佈到 Ethereum 上。前端程式的 Dapp 可用 Web3 JavaScript 透過 RPC 接上 Ethereum，以及使用網頁應用常用到的 HTML、CSS、JavaScript 製作成使用者互動介面，如此就能執行發佈在 Ethereum 上 Smart Contract 所提供的一些程式功能了。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-18ethereum-%E7%9A%84%E7%8D%8E%E5%8B%B5%E6%A9%9F%E5%88%B6/">
                <h3 class="media-heading">Ethereum 開發筆記 1–8：Ethereum 的獎勵機制</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Bitcoin 的獎勵機制基本上是挖到新區塊的節點獲得記帳權及獎勵，Ethereum 大體也是遵循這樣的概念，但做了一些調整與變化，讓我們整個脈絡了解一下。
由於 Blockchain 是一種去中心化的系統，所有的礦工（節點）可以同時挖礦（計算合法 hash），彼此獨立運作，所以極有可能出現兩的礦工同時發現不同的滿足條件的區塊，如此就會產生我們之前有提過的分叉（Fork）。
那我們該採用誰的區塊當主鏈呢？我們會先依工作量最大的區塊為主鏈，如果工作量一樣，就看誰先接了子區塊，一般來說只有成了主鏈的區塊才能獲得獎勵。但這樣沒有變成主鏈的區塊之前的算力就都白費了，所以 Ethereum 創造了 Uncle Block（叔塊）這樣的概念，不能成為主鏈的區塊如果後來被收留成為 Uncle Block，那這些沒有成為主鏈的區塊也有機會可以做為 Uncle Block 而獲得獎勵。
這就是 Ethereum 共識機制中的 GHOST（Greedy Heaviest Observed Subtree）協議，Ethereum 會這樣設計的原因，是由於 Ethereum 產生區塊的速度較快，也因此較容易產生分叉，也會使得新區塊較難以在整個網絡傳播，這對於傳播速度較慢的區塊並不公平。且分叉後的區塊可能在幾個區塊之後整併起來，我們會發現裡面的交易可能會與主鏈一致（雖然單獨查看分塊交易內容不同，不過數個區塊整體一起看交易內容就一致了），符合這種條件的分叉區塊我們就會納入主鏈參考，這些區塊就成了所謂的 Uncle Block，這某種角度也是更確認了 Blockchain 上的交易內容一致，因此 Uncle Block 也有貢獻，應該給予獎勵。
以上我們已經了解了 Ethereum 上的區塊大致分成兩種，普通區塊和 Uncle Block，Ethereum 對這兩種區塊的獎勵方式是不同的。我們分別來看一下。
普通區塊獎勵  固定獎勵 5 ETH 區塊內所有的 Gas Fee 如果區塊納入了 Uncle Block，那每包含一個 Uncle Block 可以得到固定獎勵 5 ETH * 1/32，也就是 0.15625 ETH，一個區塊最多隻能包含 2 個 Uncle Block，也因此不會無限延伸，同時又可鼓勵區塊納入 Uncle Block，增加交易內容的一致性。  Uncle Block 獎勵  用公式計算：（Uncle Block 高度 + 8 - 包含此 Uncle Block 的區塊的高度）* 普通區塊固定獎勵 / 8  我們用個實例來看一下獎勵怎麼算。首先我們來看一個普通區塊：https://etherscan.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-17blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E6%80%A7%E8%B3%AA/">
                <h3 class="media-heading">Ethereum 開發筆記 1–7：Blockchain 的一些重要性質</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">我們這邊再次總結一下 Blockchain 中幾點較重要的性質，包含共識機制、不可竄改、經濟激勵三項。
共識機制（Consensus） 在分散式系統中，我們需要有一套用於協同合作的共識機制來組織行動，但有時候系統中的成員可能會出錯或是故意傳送出錯誤的資訊，而使得網路中不同成員對於全體協作的策略得出不同的結論，進而破壞系統的一致性，這就是所謂的拜占庭將軍問題。
拜占庭將軍問題（Byzantine Generals Problem） 拜占庭將軍問題這個故事是這樣的：
 一組拜占庭將軍分別各率領一支軍隊共同圍困一座城市，這個敵人雖不比拜占庭帝國，但也足以抵禦 5 支拜占庭軍隊的同時襲擊。這 10 支軍隊在分開的包圍狀態下，他們任 1 支軍隊單獨進攻都毫無勝算，除非有至少 6 支軍隊（一半以上）同時襲擊才能攻下敵國。他們分散在敵國的四周，依靠通信兵騎馬相互通信來協商進攻意向及進攻時間。困擾這些將軍的問題是，他們不確定他們中是否有叛徒，叛徒可能擅自變更進攻意向或者進攻時間。在這種狀態下，拜占庭將軍們才能保證有多於 6 支軍隊在同一時間一起發起進攻，從而贏取戰鬥？
 上述的故事對映到電腦系統裡，將軍便成了電腦，而通信兵就是通訊系統。叛徒發送前後不一致的進攻提議，被稱為「拜占庭錯誤」，而能夠處理拜占庭錯誤的這種容錯性稱為「Byzantine Fault Tolerance」。Blockchain 上的共識機制通常具有容錯的設計來達成一致性，主要比較常見的共識機制方法有兩個，「工作量證明」以及「股權證明」兩種方法。
工作量證明演算法（Proof of Work, PoW） 中本聰在 Bitcoin 中創造性的引入了「工作量證明」（俗稱挖礦）來解決拜占庭將軍問題，顧名思義，工作量證明就是用來證明你做了一定量的工作，可用工作成果來證明完成相應的工作量。其中的工作技術原理可以看之前這篇文章：Ethereum 開發筆記 1–4：Blockchain 技術原理簡介
由於工作量證明具相當高的計算成本，因此無誘因去偽造，只有遵守協議約定，才能夠回收成本並獲得收益，也因此減少了叛徒的產生，減少拜占庭錯誤。
股權證明演算法（Proof of Stake, PoS） 股權證明的出現，主要是希望取代工作量證明，進而減少「挖礦」的大量運算。它與工作量證明不同地方在於：工作量證明中，大家比的是「算力」（運算能力），透過大量運算得出符合難度的 Hash 值，進而得到獎勵；而在股權證明，大家比拼的是「股權」，「股權」越大的人（節點）越大機會負責產生新區塊，進而得到獎勵。
舉例來說，在股權證明系統中所有擁有股權（此 Blockchain 的數位貨幣）的人都有機會被挑選為產生新區塊（也就是記帳）的人，擁有更多股權的人被選中的機率越大。假這這個系統中共有三個人：Alice 持有 50 股、Bob 持有 30 股、Cathy 持有 20 股，那每次 Alice 被選為記帳人的機率會是 Cathy 的兩倍。所以股權證明會驅使人們購買更多的股權，進而增加獲選為記帳人的機率，以買股權來代替挖礦，同樣需要付出高成本，也因此可以減少叛徒的產生，減少拜占庭錯誤。
不可竄改（Immutability） Blockchain 不可竄改的性質主要來自資料結構及 hash 方式的設計，讓資料的順序緊密鏈結，若從中竄改了某些資料，那之後的鏈結 hash 都會發生錯誤，形成了 Blockchain 不可竄改的特性。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-16blockchain-%E7%9B%B8%E9%97%9C%E7%9A%84%E5%8A%A0%E5%AF%86%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98/">
                <h3 class="media-heading">Ethereum 開發筆記 1–6：Blockchain 相關的加密基礎知識</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Blockchain 裡應用了一些加密技術來保證及驗證交易訊息的正確性，這也更加強了 Blockchain 資料不可竄改的特性。我們來介紹其中比較重要的「公私鑰加密」以及「Merkle Tree」加密樹。
公私鑰加密 公私鑰加密算法是目前資訊通訊安全的基石，它保證了加密訊息不可被破解，相關的加解密原理大家可以參考這兩篇文章：
 RSA算法原理（一）http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html RSA算法原理（二）http://www.ruanyifeng.com/blog/2013/07/rsa_algorithm_part_two.html  加密與解密 公私鑰加密方法是一種非對稱式加密，透過公鑰加密過後的訊息只有私鑰可以解密，也因此只要保護好私鑰就能保證資訊的安全。
現在假設 Alice 要傳一個訊息給 Bob，希望訊息加密過後只有 Bob 可以解密，大概會經過如下步驟：
 Bob 傳他的公鑰給 Alice Alice 使用 Bob 的公鑰加密訊息 Alice 將加密過後的訊息傳給 Bob Bob 用他的私鑰解密訊息  我們這邊使用 openssl 來練習一下加密與解密，首先我們來產生一對公私鑰：
// Create RSA private key $ openssl genrsa -des3 -out rsa-key.pem 2048 // Create public key $ openssl rsa -in rsa-key.pem -outform PEM -pubout -out rsa-key-pub.pem  其中 rsa-key.pem 就是私鑰，rsa-key-pub.pem 為公鑰，私鑰會要求設置密碼，請妥善記下密碼。
我們先用 rsa-key-pub.pem 加密資料：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-15blockchain-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%BE%A9%E8%88%87%E5%90%8D%E8%A9%9E/">
                <h3 class="media-heading">Ethereum 開發筆記 1–5：Blockchain 的一些定義與名詞</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">在 Ethereum 開發筆記 1–4 應該已經將 Blockchain 的技術原理說明得很清楚了，不過如果要向一般大眾簡單說明 Blockchain 是什麼，要怎麼說呢？我會說：Blockchain 就是一個分散式帳本，大家都有一樣的帳本，大家都可一起參與記帳，且記完帳大家的帳本就會自動更新到最新版本，而帳裡的紀錄都會分塊並用密碼按順序鏈結起來，用以驗證帳的正確性，如果中間有人改了資料，那後面的鏈結密碼都會發生錯誤，因此沒有人可以亂改帳，這就是 Blockchain。
但 Blockchain 這個名詞還包含了許多概念與內涵，我們之前說過，Blockchain 是因為分散式去中心化帳本的發展而慢慢產生出來的，這樣慢慢被統稱出來的名詞裡底下也就會包含了許多內涵，很難用三言兩語來說明，所以有一些 Blockchain 相關的定義與名詞我們都可以了解一下，這樣就能更了解 Blockchain。
交易（Transaction） 交易是 Blockchain 帳本中的原子單位，如果將交易再往下拆分就會變得沒有意義，比如下列就是一個交易：
 A 減少了 $10 B 增加了 $9 C 增加了 $1  如果只看 1，我們就會想那減少的 $10 到哪裡去了？所以 1、2、3 一起看才算是一個交易。
Blockchain 是一個分散式帳本（Distributed Ledger） 不像銀行依靠自己的帳本來記帳，Blockchain 提供了可靠的分散式帳本，當銀行之間要進行交易時，會需要一個受信任的第三方來進行銀行之間的交易，這也是為何你在做跨國轉帳時，需要付出高昂的手續費以及等待數天處理交易，Blockchain 可靠的分散式帳本讓跨國交易可以在幾分鐘甚至幾秒之內完成，這也是為何銀行想要應用 Blockchain 在金融交易上以降低交易成本。
Blockchain 是一個資料結構（Data Structure） 通常 Blockchain 的資料結構如下組成：
 交易是原子單位 區塊是由一系列的交易組成 區塊鏈由排序良好的區塊所組成  Blockchain 會有分叉（Fork） 當有兩名礦工 A 及 B 幾乎在相同時間內算出了合法的 hash，這兩個區塊傳播到鄰近節點時，有些節點收到了 A 的區塊，有些節點收到了 B 的區塊，這兩個區塊都可以是主鏈的延伸，這時就會產生區塊鏈分叉。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-14blockchain-%E6%8A%80%E8%A1%93%E5%8E%9F%E7%90%86%E7%B0%A1%E4%BB%8B/">
                <h3 class="media-heading">Ethereum 開發筆記 1–4：Blockchain 技術原理簡介</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前我們簡單地介紹過 Blockchain 了，但我們還是對 Blockchain 背後的技術原理不是那麼了解，我們知道 Blockchain 是因為一個數位貨幣帳本這樣的概念被創造出來的，而數位貨幣最擔心的是什麼問題呢？其實就是雙重支付（Double-Spending）這樣的問題。
數位貨幣不像實體貨幣，數位資產比起實體資產容易複製，也因此如果花用數位貨幣的行為如果沒有處理好，就會產生憑空多出其他交易，這就像是偽鈔一樣，會造成通貨膨脹而導致貨幣貶值，讓人不再信任並願意持與流通。因此數位貨幣的支付通常需要一個受信任的第三方來做驗證，這樣的做法雖然簡單，卻存在單點脆弱性，只要這第三方受到攻擊或是監守自盜也一樣會讓這個數位貨幣變成一個失敗的貨幣。
分散式去中心化帳本能解決單點脆弱性的問題，但在驗證正確性這點難度卻很高，所有的節點都有記帳的權利，要如何確定由誰來記帳、記的帳對不對？如果無法確定帳是對的，那就存在雙重支付的風險。
為了改善單點脆弱性及雙重支付這樣的問題，許多分散式的雙重支付防範方法慢慢被提出來，中本聰提出了去中心化（以受信任第三方為中心）的方法來展示解決雙重支付問題，並實作出了 Bitcoin，使用共識機制來解決記帳及驗證的問題，這帶來去中心化數位貨幣帳本的成功。
Bitcoin 的共識協議主要由「工作量證明」（Proof-of-Work, PoW）和「最長鏈機制」兩部分組成，Bitcoin 上的各個節點就是透過共識機制中的工作量證明來決定誰有記帳權，然後取得記帳權的節點就能將新的區塊記帳加到最長鏈上並給予該節點獎勵（新區塊獎勵及交易費收益）。
Bitcoin 的 工作量證明大概會做以下的事情：
 收集還未記到帳上的交易 檢查每個交易中付款地址有沒有足夠的餘額 驗證交易是否有正確的簽名 把驗證通過的交易信息進行打包（組成 Merkle Tree） 為自己增加一個交易紀錄獲得 Bitcoin 獎勵金 計算合法的 hash 爭奪記帳權  計算合法 hash 的方式請見下方影片說明，個人覺得這個影片是目前將 Blockchain 加密機制說明得最清楚的影片。我這邊簡略說明一下，合法的 hash 公式大致看起來像這樣：hash(交易內容+交易簽名+nonce+上一個區塊的 hash)，我們要取得記帳權，就需要找出前面開頭有 N 個 0 的 hash，由於交易內容、交易簽名及上一個區塊的 hash 都是不可變的，所以每個節點就是不斷的調整 nonce 來計算得出不同的 hash，直到找到開頭 N 個 0 的 hash 為止，第一個找的節點就能獲得記帳權，而其他的節點只要計算 hash 對不對就能驗證這個帳對不對。其中 N 個 0 開頭的 hash 就代表了計算的難度，越多 0 代表越難找到這樣的 hash，也因此可以調整計算難度。就是這樣的設計解決了去中心化分散式系統驗證資料及決定記帳順序的難題，也就改善了數位貨幣單點脆弱性及雙重支付的問題。
  以上的內容看完應該就能大體了解 Blockchain 的原理了，甚至要自己做一個 Blockchain 都沒問題！了解了 Blockchain 的技術原理之後，應該能更信任去中心化的數位貨幣的安全性，或許有天大家都信任了去中心化的數位貨幣我們就真的能廣泛使用數位貨幣，為經濟活動帶來更有效率的流通。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98%E7%B7%B4%E7%BF%92-1%E4%BD%BF%E7%94%A8-mist-%E7%99%BC%E8%A1%8C%E8%87%AA%E5%B7%B1%E7%9A%84-token/">
                <h3 class="media-heading">Ethereum 開發筆記練習 1：使用 Mist 發行自己的 Token</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">之前說過，Blockchain 基本上是因為金流帳本這樣的問題而被創造出來的，也就是說區塊鏈非常適合運用在金流的應用上，我們也可以建立自己的 Blockchain 來搭建自己的金流系統，不過在 Ethereum 上 Smart Contract 這種設計讓我們擁有可以在 Ethereum 區塊鏈上創造自己金流系統的能力，如此我們就不需要自己建一條鏈了。
我們使用 Smart Contract 仿造貨幣性質創造了數位資產（說穿了其實就是在 Smart Contract 上紀錄的變數而已），而這種具貨幣性質的數位資產又被稱作 Token，如此我們就可以在應用程式中使用這個去中心化的金流系統，由於 Token 的應用很普遍，大部分的功能都已經標準化了，我們只要仿造標準來實作就可以發行自己的數位貨幣了。
在這邊我們就練習一下怎麼使用 Mist 發佈 Token Smart Contract 來發行自己的數位貨幣。（目前我們還沒有學習過如何撰寫 Smart Contract，因此這邊會先直接提供範例程式碼，實作的部分我們之後再慢慢學習）
以下是我們的範例程式碼：
 請打開 Mist，如下圖點擊 Contract，然後點擊 Deploy New Contract。
你會看到如下圖的頁面，請在 Solidity Contract Source Code 中貼上我們上面提供的範例程式碼。
貼上範例程式碼之後，Mist 會自動編譯程式，檢查是否有語法上的錯誤，如果沒問題，右方的 Select Contract to Deploy 就會出現選項，在這邊我們選擇 Token ERC 20。
選擇 Token ERC 20 之後，右方會出現要初始化 Contract 的參數表單，有 Initial supply、Token name、Token symbol 需要填寫。Initial supply 代表 Token 的總發行量是多少，我這邊設定成 7777777777，你可以設成你想要的數字。Token name 就是這個 Token 要叫什麼名字，這邊我設定成 7 Token，你想要取 Dog Coin 或是 Cat Coin 也都可以。Token symbol 就是這個 Token 要用什麼代號，像是美金就是用 $、Ether 是用 ETH，這邊我設定成 7token，你可以取自己覺得帥的代號。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://blog.fukuball.com/ethereum-%E9%96%8B%E7%99%BC%E7%AD%86%E8%A8%98-13%E4%BD%BF%E7%94%A8-mist/">
                <h3 class="media-heading">Ethereum 開發筆記 1–3：使用 Mist</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Mist 跟前回介紹的 MetaMask 一樣是可以與 Ethereum 進行互動的工具，除了可以管理 Ethereum 相關密鑰之外，Mist 還包含了 Ethereum 節點以及網頁瀏覽器，方便大家瀏覽 Dapp 網頁。
首先請到這邊安裝 Mist，請選擇適於自己的作業系統安裝。
由於 Mist 會安裝節點在你的電腦裡，也因此會同步整個帳本下來，所以會花上不少時間同時也會佔用許多硬碟空間。我們目前僅是要使用測試鏈，所以請切換到 Ropsten 測試鏈（如下圖），這樣就不用花這麼多時間與空間了。
在 Mist 的左下角可以觀察目前已同步到你的電腦的區塊數（如下圖），如果這個數字跟 Etherscan（Etherscan 是一個可以查看 Ethereum 區塊鏈所有交易的網站） 上的最新區塊數一致的話，那就代表已經同步完成了。
接下來讓我們用 Mist 開一個 Ethereum 帳戶，請點擊 Add Account，並依指示輸入密碼後創建帳號，密碼請務必要記下來，將來交易時都會需要輸入你的密碼。
學會創建 Ethereum 帳戶之後，我們要來看一下 Mist 要怎麼備份帳號，請點擊 Mist 上方選單的 File -&gt; Backup -&gt;Accounts（如下圖），這樣就會打開帳號存放的資料夾，所有的帳號都會加密存在這邊，所以只要備份這些檔案及當時設定的密碼，你就可以在別台電腦復原你的帳號。
現在你這個 Ethereum 帳戶還沒有任何 Ether，我們仿造之前用 MetaMask 來跟水龍頭要 Ether 的步驟來取得 Ether 看看。
我個人提供了一個水龍頭 Dapp，請前往這個網址來取得 Ether：https://blog.fukuball.com/dapp/faucet/
由於 Mist 也是一個 Dapp 網頁瀏覽器，請在 Mist 上方的網址列輸入：https://blog.fukuball.com/dapp/faucet/
Mist 在揭露你的 Ethereum 帳戶資訊給 Dapp 網頁時都會詢問你的同意，請先選擇要瀏覽這個 Dapp 網頁的帳號（你可能在 Mist 有多個帳號，所以就需要選擇目前要用哪個帳號瀏覽這個網頁）。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero=""
         data-message-one=""
         data-message-other="">
         76 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://blog.fukuball.com/images/ok.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://blog.fukuball.com/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>






<script async src="https://www.googletagmanager.com/gtag/js?id=UA-41911929-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-41911929-4');
</script>

    
  </body>
</html>

